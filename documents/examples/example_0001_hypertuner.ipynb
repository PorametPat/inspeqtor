{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988380cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.typing import VariableDict\n",
    "import flax.traverse_util as traverse_util\n",
    "from functools import partial\n",
    "import optax  # type: ignore\n",
    "\n",
    "import tempfile\n",
    "from enum import StrEnum\n",
    "\n",
    "import inspeqtor.experimental as sq\n",
    "from ray.tune.search.sample import Domain\n",
    "from helper import get_data_model, custom_feature_map\n",
    "\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246bc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_key(data):\n",
    "    return {\n",
    "        # Concanate the key by '/'\n",
    "        \"/\".join(key): value\n",
    "        for key, value in data.items()\n",
    "    }\n",
    "\n",
    "\n",
    "def clean_history_entries(\n",
    "    histories: list[sq.optimize.HistoryEntryV3],\n",
    "):\n",
    "    clean_histories = [\n",
    "        {\n",
    "            \"step\": history.step,\n",
    "            \"loss\": history.loss,\n",
    "            \"loop\": history.loop,\n",
    "            **history.aux,\n",
    "        }\n",
    "        for history in histories\n",
    "    ]\n",
    "    # Move from device to host, i.e. from jax.Array to numpy.ndarray\n",
    "    clean_histories = jax.tree.map(\n",
    "        lambda x: x.item() if isinstance(x, jnp.ndarray) else x, clean_histories\n",
    "    )\n",
    "    # Flatten the nested dictionaries\n",
    "    clean_histories = list(map(traverse_util.flatten_dict, clean_histories))\n",
    "    # Transform the keys of the dictionary\n",
    "    clean_histories = list(map(transform_key, clean_histories))\n",
    "    return clean_histories\n",
    "\n",
    "\n",
    "def default_trainable_v4(\n",
    "    control_sequence: sq.control.ControlSequence,\n",
    "    metric: sq.model.LossMetric,\n",
    "    experiment_identifier: str,\n",
    "    hamiltonian: typing.Callable | str,\n",
    "    construct_model_fn: typing.Callable[\n",
    "        [dict[str, int]], tuple[nn.Module, dict[str, typing.Any]]\n",
    "    ],\n",
    "    calculate_metrics_fn: typing.Callable,\n",
    "    NUM_EPOCH: int = 1000,\n",
    "    CHECKPOINT_EVERY: int = 100,\n",
    "):\n",
    "    \"\"\"Create trainable function for `ray.tune` for hyperparameter tuning\n",
    "\n",
    "    Args:\n",
    "        control_sequence (PulseSequence): Pulse sequence of dataset\n",
    "        metric (LossMetric): Metric to be minimized for.\n",
    "        experiment_identifier (str): The experiment identifier\n",
    "        hamiltonian (typing.Callable | str): Ideal Hamiltonian function or name.\n",
    "        model_choice (type[nn.Module], optional): Choice of the Blackbox model. Defaults to BasicBlackBoxV2.\n",
    "        NUM_EPOCH (int, optional): Number of training epoch. Defaults to 1000.\n",
    "        CHECKPOINT_EVERY (int, optional): Checkpointing every given number. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        typing.Callable: Trainable function that recieve hyperparameter configutation, dataset and random key.\n",
    "    \"\"\"\n",
    "    from ray import train\n",
    "\n",
    "    def trainable(\n",
    "        config: dict[str, int],\n",
    "        train_data: sq.optimize.DataBundled,\n",
    "        val_data: sq.optimize.DataBundled,\n",
    "        test_data: sq.optimize.DataBundled,\n",
    "        train_key: jnp.ndarray,\n",
    "    ):\n",
    "        optimizer = sq.optimize.get_default_optimizer(8 * NUM_EPOCH)\n",
    "\n",
    "        model, model_config = construct_model_fn(config)\n",
    "\n",
    "        partial_loss_fn = partial(\n",
    "            sq.model.loss_fn,\n",
    "            model=model,\n",
    "            loss_metric=metric,\n",
    "            calculate_metrics_fn=calculate_metrics_fn,\n",
    "        )\n",
    "\n",
    "        def prepare_report(history: list[sq.optimize.HistoryEntryV3]):\n",
    "            metric_types = [\n",
    "                sq.model.LossMetric.MSEE,\n",
    "                sq.model.LossMetric.AEF,\n",
    "                sq.model.LossMetric.WAEE,\n",
    "            ]\n",
    "            metrics = {}\n",
    "            for entry in history:\n",
    "                for metric_type in metric_types:\n",
    "                    metrics[f\"{entry.loop}/{metric_type}\"] = entry.aux[\n",
    "                        metric_type\n",
    "                    ].item()\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        def callback(\n",
    "            model_params: VariableDict,\n",
    "            opt_state: optax.OptState,\n",
    "            history: list[sq.optimize.HistoryEntryV3],\n",
    "        ) -> None:\n",
    "            # Get the lasted 3 entries\n",
    "            last_entries = history[-3:]\n",
    "\n",
    "            loops = [\"train\", \"val\", \"test\"]\n",
    "            # assert that the last 3 entries are from train, val, and test\n",
    "            assert all(entry.loop in loops for entry in last_entries)\n",
    "\n",
    "            # Prepare the report\n",
    "            metrics = prepare_report(history)\n",
    "\n",
    "            # Check if last_entry.step is divisible by 100\n",
    "            if (last_entries[-1].step + 1) % CHECKPOINT_EVERY == 0:\n",
    "                # Checkpoint the model\n",
    "\n",
    "                # Clean the history entries\n",
    "                clean_histories = clean_history_entries(history)\n",
    "\n",
    "                with tempfile.TemporaryDirectory() as tmpdir:\n",
    "                    _ = sq.model.save_model(\n",
    "                        path=tmpdir,\n",
    "                        experiment_identifier=experiment_identifier,\n",
    "                        control_sequence=control_sequence,\n",
    "                        hamiltonian=hamiltonian,\n",
    "                        model_config=model_config,\n",
    "                        model_params=model_params,\n",
    "                        history=clean_histories,\n",
    "                        with_auto_datetime=False,\n",
    "                    )\n",
    "\n",
    "                    # Report the loss and val_loss to tune\n",
    "                    train.report(\n",
    "                        metrics=metrics,\n",
    "                        checkpoint=train.Checkpoint.from_directory(tmpdir),\n",
    "                    )\n",
    "            else:\n",
    "                # Report the loss and val_loss to tune\n",
    "                train.report(\n",
    "                    metrics=metrics,\n",
    "                )\n",
    "\n",
    "            return None\n",
    "\n",
    "        _, _, history = sq.optimize.train_model(\n",
    "            key=train_key,\n",
    "            train_data=train_data,\n",
    "            val_data=val_data,\n",
    "            test_data=test_data,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=partial_loss_fn,\n",
    "            NUM_EPOCH=NUM_EPOCH,\n",
    "            callbacks=[callback],\n",
    "        )\n",
    "\n",
    "        # Prepare the report\n",
    "        metrics = prepare_report(history[-3:])\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    return trainable\n",
    "\n",
    "\n",
    "def sample_from_search_space(search_space: typing.Mapping[str, Domain]):\n",
    "    return {key: value.sample() for key, value in search_space.items()}\n",
    "\n",
    "\n",
    "class SearchAlgo(StrEnum):\n",
    "    HYPEROPT = \"hyperopt\"\n",
    "    OPTUNA = \"optuna\"\n",
    "\n",
    "\n",
    "def hypertuner(\n",
    "    trainable: typing.Callable,\n",
    "    train_data: sq.optimize.DataBundled,\n",
    "    test_data: sq.optimize.DataBundled,\n",
    "    val_data: sq.optimize.DataBundled,\n",
    "    train_key: jnp.ndarray,\n",
    "    metric: sq.model.LossMetric,\n",
    "    search_space: typing.Mapping[str, Domain],\n",
    "    num_samples: int = 100,\n",
    "    search_algo: SearchAlgo = SearchAlgo.HYPEROPT,\n",
    "):\n",
    "    \"\"\"Perform hyperparameter tuning\n",
    "\n",
    "    Args:\n",
    "        trainable (typing.Callable): Trainable function\n",
    "        train_pulse_parameters (jnp.ndarray): Training pulse parameters\n",
    "        train_unitaries (jnp.ndarray): Training ideal unitary matrix\n",
    "        train_expectation_values (jnp.ndarray): Training experiment expectation value\n",
    "        test_pulse_parameters (jnp.ndarray): Testing pulse parameters\n",
    "        test_unitaries (jnp.ndarray): Testing ideal unitary matrix\n",
    "        test_expectation_values (jnp.ndarray): Testing experiment expectation value\n",
    "        val_pulse_parameters (jnp.ndarray): Validating pulse parameters\n",
    "        val_unitaries (jnp.ndarray): Validating ideal unitary matrix\n",
    "        val_expectation_values (jnp.ndarray): Validating experiment expectation value\n",
    "        train_key (jnp.ndarray): Random key\n",
    "        metric (LossMetric): Metric to optimized for.\n",
    "        num_samples (int, optional): The number of random configuration of hyperparameter. Defaults to 100.\n",
    "        search_algo (SearchAlgo, optional): The search algorithm to be used for optimization. Defaults to SearchAlgo.HYPEROPT.\n",
    "        search_spaces (_type_, optional): Search space of hyperparameters. Defaults to { \"hidden_layer_1_1\": (5, 50), \"hidden_layer_1_2\": (5, 50), \"hidden_layer_2_1\": (5, 50), \"hidden_layer_2_2\": (5, 50), }.\n",
    "        initial_config (_type_, optional): Initial hyperparameters. Defaults to { \"hidden_layer_1_1\": 10, \"hidden_layer_1_2\": 20, \"hidden_layer_2_1\": 10, \"hidden_layer_2_2\": 20, }.\n",
    "\n",
    "    Returns:\n",
    "        _type_: Optimization result.\n",
    "    \"\"\"\n",
    "    from ray import tune\n",
    "    from ray.tune.search.hyperopt import HyperOptSearch\n",
    "    from ray.tune.search.optuna import OptunaSearch\n",
    "    from ray.tune.search import Searcher\n",
    "\n",
    "    current_best_params = [{key: value.sample() for key, value in search_space.items()}]\n",
    "\n",
    "    # Prepend 'val/' to the metric\n",
    "    prepended_metric = f\"val/{metric}\"\n",
    "\n",
    "    if search_algo == SearchAlgo.HYPEROPT:\n",
    "        search_algo_instance: Searcher = HyperOptSearch(\n",
    "            metric=prepended_metric,\n",
    "            mode=\"min\",\n",
    "            points_to_evaluate=current_best_params,\n",
    "        )\n",
    "    elif search_algo == SearchAlgo.OPTUNA:\n",
    "        search_algo_instance = OptunaSearch(\n",
    "            metric=prepended_metric,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "    run_config = tune.RunConfig(\n",
    "        name=\"tune_experiment\",\n",
    "        checkpoint_config=tune.CheckpointConfig(\n",
    "            num_to_keep=10,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_parameters(\n",
    "            trainable,\n",
    "            train_data=train_data,\n",
    "            val_data=val_data,\n",
    "            test_data=test_data,\n",
    "            train_key=train_key,\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=search_algo_instance,\n",
    "            metric=prepended_metric,\n",
    "            mode=\"min\",\n",
    "            num_samples=num_samples,\n",
    "        ),\n",
    "        param_space=search_space,  # type: ignore\n",
    "        run_config=run_config,\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_best_hypertuner_results(\n",
    "    results, metric: sq.model.LossMetric, loop: str = \"val\"\n",
    "):\n",
    "    prepended_metric = f\"{loop}/{metric}\"\n",
    "\n",
    "    with results.get_best_result(\n",
    "        metric=prepended_metric, mode=\"min\"\n",
    "    ).checkpoint.as_directory() as checkpoint_dir:\n",
    "        model_state, hist, data_config = sq.model.load_model(\n",
    "            checkpoint_dir, skip_history=False\n",
    "        )\n",
    "    return model_state, hist, data_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99ef48",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)\n",
    "key, data_key, model_key, train_key, gate_optim_key = jax.random.split(key, 5)\n",
    "sample_size = 1000\n",
    "shots = 3000\n",
    "data_model = get_data_model()\n",
    "qubit_info = sq.predefined.get_mock_qubit_information()\n",
    "whitebox = sq.predefined.get_single_qubit_whitebox(\n",
    "    hamiltonian=data_model.ideal_hamiltonian,\n",
    "    control_sequence=data_model.control_sequence,\n",
    "    qubit_info=qubit_info,\n",
    "    dt=data_model.dt,\n",
    ")\n",
    "\n",
    "# NOTE pick the pulse sequence you want to use\n",
    "# def get_control_sequence_fn():\n",
    "#         return sq.predefined.get_drag_control_sequence(qubit_info)\n",
    "\n",
    "get_control_sequence_fn = sq.predefined.get_multi_drag_control_sequence_v3\n",
    "\n",
    "# NOTE: Simulate the experiment with some detuning noise\n",
    "exp_data, control_sequence, unitaries, noisy_simulator = (\n",
    "    sq.predefined.generate_experimental_data(\n",
    "        key=data_key,\n",
    "        hamiltonian=data_model.total_hamiltonian,\n",
    "        sample_size=sample_size,\n",
    "        shots=shots,\n",
    "        strategy=sq.predefined.SimulationStrategy.SHOT,\n",
    "        get_qubit_information_fn=lambda: data_model.qubit_information,\n",
    "        get_control_sequence_fn=lambda: data_model.control_sequence,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare the data for training\n",
    "loaded_data = sq.utils.prepare_data(\n",
    "    exp_data=exp_data, control_sequence=control_sequence, whitebox=whitebox\n",
    ")\n",
    "\n",
    "model_constructor = sq.model.make_basic_blackbox_model(\n",
    "    # unitary_activation_fn=lambda x: 2 * jnp.pi * (jnp.cos(x) + 1) / 2,\n",
    "    # diagonal_activation_fn=lambda x: jnp.cos(x),\n",
    "    # unitary_activation_fn = lambda x: (2 * jnp.pi * nn.hard_sigmoid(x)) + 1e-3,\n",
    "    # diagonal_activation_fn = lambda x: ((2 * nn.hard_sigmoid(x)) - 1) + 1e-3,\n",
    ")\n",
    "\n",
    "# Choose the loss metric\n",
    "metric = sq.model.LossMetric.WAEE\n",
    "# Define trainanle function for hyperparameter tuning\n",
    "trainable = sq.optimize.default_trainable_v4(\n",
    "    control_sequence=loaded_data.control_sequence,\n",
    "    metric=metric,\n",
    "    experiment_identifier=\"test\",\n",
    "    hamiltonian=sq.predefined.rotating_transmon_hamiltonian,\n",
    "    construct_model_fn=lambda x: sq.model.construct_wo_model_from_config(\n",
    "        x, model_constructor\n",
    "    ),\n",
    "    calculate_metrics_fn=sq.model.calculate_metrics,\n",
    ")\n",
    "\n",
    "key = jax.random.key(0)\n",
    "key, random_split_key_1, random_split_key_2, train_key, prediction_key = (\n",
    "    jax.random.split(key, 5)\n",
    ")\n",
    "(\n",
    "    train_p,\n",
    "    train_u,\n",
    "    train_e,\n",
    "    eval_p,\n",
    "    eval_u,\n",
    "    eval_ex,\n",
    ") = sq.utils.random_split(\n",
    "    random_split_key_1,\n",
    "    20,  # Test size\n",
    "    loaded_data.control_parameters,\n",
    "    loaded_data.unitaries,\n",
    "    loaded_data.expectation_values,\n",
    ")\n",
    "\n",
    "(val_p, val_u, val_ex, test_p, test_u, test_ex) = sq.utils.random_split(\n",
    "    random_split_key_2, 10, eval_p, eval_u, eval_ex\n",
    ")\n",
    "\n",
    "train_data = sq.optimize.DataBundled(custom_feature_map(train_p), train_u, train_e)\n",
    "val_data = sq.optimize.DataBundled(custom_feature_map(val_p), val_u, val_ex)\n",
    "test_data = sq.optimize.DataBundled(custom_feature_map(test_p), test_u, test_ex)\n",
    "\n",
    "# Hyperparameter tuning\n",
    "results = sq.optimize.hypertuner(\n",
    "    trainable=trainable,\n",
    "    train_data=train_data,\n",
    "    test_data=test_data,\n",
    "    val_data=val_data,\n",
    "    train_key=train_key,\n",
    "    num_samples=10,\n",
    "    search_algo=sq.optimize.SearchAlgo.OPTUNA,\n",
    "    metric=metric,\n",
    "    search_space={\n",
    "        \"hidden_layer_1_1\": tune.randint(0, 1),  # (0, 1) means no hidden layer\n",
    "        \"hidden_layer_1_2\": tune.randint(0, 1),  # (0, 1) means no hidden layer\n",
    "        \"hidden_layer_2_1\": tune.randint(0, 1),  # (0, 1) means no hidden layer\n",
    "        \"hidden_layer_2_2\": tune.randint(4, 5),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "model_state, train_hist, data_config = sq.optimize.get_best_hypertuner_results(\n",
    "    results, metric=metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1028df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and load model\n",
    "save_path = sq.model.save_model(\n",
    "    path=\"ckpt\",\n",
    "    experiment_identifier=\"test\",\n",
    "    control_sequence=loaded_data.control_sequence,\n",
    "    hamiltonian=data_config.hamiltonian,\n",
    "    model_config=model_state.model_config,\n",
    "    model_params=model_state.model_params,\n",
    "    history=train_hist,\n",
    ")\n",
    "\n",
    "loaded_model = sq.model.load_model(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inspeqtor (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
