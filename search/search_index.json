{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting start","text":"<p>         inspeqtor     </p>"},{"location":"#greeting","title":"Greeting \ud83d\udd96","text":"<p>We aim to be a Data-efficient Framework for Characterization and Calibration of Quantum Device. As performing experiment and take data from the quantum device could be expensive. We would like to squeeze every drop of information from the data.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install <code>inspeqtor</code>, you have to pull the repository locally via</p> <pre><code>    git clone https://github.com/PorametPat/inspeqtor.git\n</code></pre> <p>Then install it using</p> uvpip <pre><code>uv add ./&lt;PATH&gt;\n</code></pre> <pre><code>pip install ./&lt;PATH&gt;\n</code></pre> <p>where the <code>&lt;PATH&gt;</code> is the path to the local <code>inspeqtor</code> repository that you cloned.</p>"},{"location":"#what-you-can-do-with-inspeqtor","title":"What you can do with <code>inspeqtor</code>","text":""},{"location":"#next-step","title":"Next step","text":"<p>Overviews is a good place to start.</p>"},{"location":"#citation","title":"Citation","text":""},{"location":"api-reference/","title":"API Reference","text":""},{"location":"dev/","title":"Developer Guide for Inspeqtor","text":""},{"location":"dev/#getting-started","title":"Getting Started","text":"<p>This guide provides the necessary steps and commands for developing the Inspeqtor package. We use the <code>uv</code> package manager for dependency management and various tools for testing, linting, and documentation.</p>"},{"location":"dev/#project-setup","title":"Project Setup","text":""},{"location":"dev/#creating-a-new-project","title":"Creating a New Project","text":"Create a library project<pre><code>uv init inspeqtor --lib\n</code></pre> Create an application project<pre><code>uv init inspeqtor\n</code></pre>"},{"location":"dev/#setting-up-a-virtual-environment","title":"Setting Up a Virtual Environment","text":"Create a virtual environment with Python 3.12<pre><code>uv venv [name] --python 3.12\n</code></pre> <p>If using a custom name for your virtual environment, set these environment variables:</p> Set environment variable for custom venv name<pre><code>export UV_PROJECT_ENVIRONMENT=[name]\n</code></pre> Set VIRTUAL_ENV environment variable<pre><code>export VIRTUAL_ENV=[name]\n</code></pre>"},{"location":"dev/#installing-the-package","title":"Installing the Package","text":"Install the package in development mode<pre><code>uv add . --editable --dev\n</code></pre> Install Jupyter integration<pre><code>uv add ipykernel --dev\n</code></pre> Intall dependency in optional docs group<pre><code>uv add mkdocs-marimo --optional docs\n</code></pre>"},{"location":"dev/#syncing-dependencies","title":"Syncing dependencies","text":"Install all optional dependencies<pre><code>uv sync --all-extras\n</code></pre>"},{"location":"dev/#development-workflow","title":"Development Workflow","text":"<p>The project uses several tools for development:</p> <ul> <li>pytest for testing</li> <li>ruff for linting and formatting</li> <li>pyright for type checking</li> <li>pre-commit for git hooks</li> <li>tuna for import profiling</li> </ul>"},{"location":"dev/#testing","title":"Testing","text":"Basic test run<pre><code>uv run pytest tests/ -v\n</code></pre> Test experimental module with detailed output<pre><code>uv run pytest tests/experimental/. -vv --durations=0\n</code></pre> Test with specific Python version<pre><code>uv run --python 3.12 --with '.[test]' pytest tests/experimental/.\n</code></pre> Test docstrings<pre><code>uv run -m doctest src/inspeqtor/experimental/utils.py\n</code></pre> Test with live logging<pre><code>uv run -m doctest src/inspeqtor/experimental/utils.py --log-cli-level=INFO\n</code></pre>"},{"location":"dev/#code-quality","title":"Code Quality","text":"Run linting with Ruff<pre><code>uvx ruff check .\n</code></pre> Check code formatting<pre><code>uvx ruff format --check .\n</code></pre> Run type checking with Pyright<pre><code>uv run pyright .\n</code></pre> Run pre-commit hooks<pre><code>uv run pre-commit run --all-files\n</code></pre>"},{"location":"dev/#documentation","title":"Documentation","text":""},{"location":"dev/#mkdocs-setup","title":"MkDocs Setup","text":"<p>We use MkDocs Material and pymdown-extensions for documentation generation. Follow the Real Python tutorial for detailed setup instructions.</p> Serve documentation locally<pre><code>uv run mkdocs serve\n</code></pre> Deploy to GitHub Pages<pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"dev/#performance-profiling","title":"Performance Profiling","text":"Generate import profile<pre><code>uv run python -X importtime profile.py 2&gt; import.log\n</code></pre> Visualize import profile<pre><code>uvx tuna import.log\n</code></pre>"},{"location":"dev/#cicd","title":"CI/CD","text":"<p>For setting up GitHub Actions for CI/CD, refer to this tutorial.</p> <p>This guide covers the essential commands and workflows for developing the Inspeqtor package. For more detailed information about specific components, refer to the respective documentation.</p>"},{"location":"explanation/","title":"Design Decisions","text":""},{"location":"explanation/#the-import","title":"The <code>import</code> \ud83e\uddd0","text":"<p>We currently recommend user to import the package using the following code snippet.</p> <pre><code>import inspeqtor.experimental as sq\n</code></pre> <p>In the future, the <code>experimental</code> is likely to be removed once the <code>legacy</code> module is fully removed. We expect the migration to be as easy as removing <code>experimental</code> part.</p>"},{"location":"api/boed/","title":"Boed","text":""},{"location":"api/boed/#src.inspeqtor.experimental.boed","title":"src.inspeqtor.experimental.boed","text":""},{"location":"api/boed/#src.inspeqtor.experimental.boed.AuxEntry","title":"AuxEntry","text":"<p>The auxillary entry returned by loss function</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>class AuxEntry(typing.NamedTuple):\n    \"\"\"The auxillary entry returned by loss function\"\"\"\n\n    terms: jnp.ndarray | None\n    eig: jnp.ndarray\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.safe_shape","title":"safe_shape","text":"<pre><code>safe_shape(a: Any) -&gt; tuple[int, ...] | str\n</code></pre> <p>Safely get the shape of the object</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>Any</code> <p>Expect the object to be jnp.ndarray</p> required <p>Returns:</p> Type Description <code>tuple[int, ...] | str</code> <p>tuple[int, ...] | str: Either return the shape of <code>a</code></p> <code>tuple[int, ...] | str</code> <p>or string representation of the type</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def safe_shape(a: typing.Any) -&gt; tuple[int, ...] | str:\n    \"\"\"Safely get the shape of the object\n\n    Args:\n        a (typing.Any): Expect the object to be jnp.ndarray\n\n    Returns:\n        tuple[int, ...] | str: Either return the shape of `a`\n        or string representation of the type\n    \"\"\"\n    try:\n        assert isinstance(a, jnp.ndarray)\n        return a.shape\n    except AttributeError:\n        return str(type(a))\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.report_shape","title":"report_shape","text":"<pre><code>report_shape(a: PyTree) -&gt; PyTree\n</code></pre> <p>Report the shape of pytree</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>PyTree</code> <p>The pytree to be report.</p> required <p>Returns:</p> Type Description <code>PyTree</code> <p>jaxtyping.PyTree: The shape of pytree.</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def report_shape(a: jaxtyping.PyTree) -&gt; jaxtyping.PyTree:\n    \"\"\"Report the shape of pytree\n\n    Args:\n        a (jaxtyping.PyTree): The pytree to be report.\n\n    Returns:\n        jaxtyping.PyTree: The shape of pytree.\n    \"\"\"\n    return jax.tree.map(safe_shape, a)\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.lexpand","title":"lexpand","text":"<pre><code>lexpand(a: ndarray, *dimensions: int) -&gt; ndarray\n</code></pre> <p>Expand tensor, adding new dimensions on left.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>expand the dimension on the left with given dimension arguments.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: New array with shape (*dimension + a.shape)</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def lexpand(a: jnp.ndarray, *dimensions: int) -&gt; jnp.ndarray:\n    \"\"\"Expand tensor, adding new dimensions on left.\n\n    Args:\n        a (jnp.ndarray): expand the dimension on the left with given dimension arguments.\n\n    Returns:\n        jnp.ndarray: New array with shape (*dimension + a.shape)\n    \"\"\"\n    return jnp.broadcast_to(a, dimensions + a.shape)\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.random_split_index","title":"random_split_index","text":"<pre><code>random_split_index(rng_key: ndarray, num_samples: int, test_size: int) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Create the randomly spilt of indice to two set, with one of test_size and another as the rest.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>ndarray</code> <p>The random key</p> required <code>num_samples</code> <code>int</code> <p>The size of total sample size</p> required <code>test_size</code> <code>int</code> <p>The size of test set</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[jnp.ndarray, jnp.ndarray]: Array of train indice and array of test indice.</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def random_split_index(\n    rng_key: jnp.ndarray, num_samples: int, test_size: int\n) -&gt; tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"Create the randomly spilt of indice to two set, with one of test_size and another as the rest.\n\n    Args:\n        rng_key (jnp.ndarray): The random key\n        num_samples (int): The size of total sample size\n        test_size (int): The size of test set\n\n    Returns:\n        tuple[jnp.ndarray, jnp.ndarray]: Array of train indice and array of test indice.\n    \"\"\"\n    idx = jax.random.permutation(rng_key, jnp.arange(num_samples))\n    return idx[test_size:], idx[:test_size]\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.marginal_loss","title":"marginal_loss","text":"<pre><code>marginal_loss(model: Callable, marginal_guide: Callable, design: ndarray, *args, observation_labels: list[str], target_labels: list[str], num_particles: int, evaluation: bool = False) -&gt; Callable[[ArrayTree, ndarray], tuple[ndarray, AuxEntry]]\n</code></pre> <p>The marginal loss implemented following https://docs.pyro.ai/en/dev/contrib.oed.html#pyro.contrib.oed.eig.marginal_eig</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Callable</code> <p>The probabilistic model</p> required <code>marginal_guide</code> <code>Callable</code> <p>The custom guide</p> required <code>design</code> <code>ndarray</code> <p>Possible designs of the experiment</p> required <code>observation_labels</code> <code>list[str]</code> <p>The list of string of observations</p> required <code>target_labels</code> <code>list[str]</code> <p>The target latent parameters to be optimized for</p> required <code>num_particles</code> <code>int</code> <p>The number of independent trials</p> required <code>evaluation</code> <code>bool</code> <p>True for actual evalution of the EIG. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[ArrayTree, ndarray], tuple[ndarray, AuxEntry]]</code> <p>typing.Callable[ [chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry] ]: Loss function that return tuple of (1) Total loss, (2.1) Each terms without the average, (2.2) The EIG</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def marginal_loss(\n    model: typing.Callable,\n    marginal_guide: typing.Callable,\n    design: jnp.ndarray,\n    *args,\n    observation_labels: list[str],\n    target_labels: list[str],\n    num_particles: int,\n    evaluation: bool = False,\n) -&gt; typing.Callable[[chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry]]:\n    \"\"\"The marginal loss implemented following\n    https://docs.pyro.ai/en/dev/contrib.oed.html#pyro.contrib.oed.eig.marginal_eig\n\n    Args:\n        model (typing.Callable): The probabilistic model\n        marginal_guide (typing.Callable): The custom guide\n        design (jnp.ndarray): Possible designs of the experiment\n        observation_labels (list[str]): The list of string of observations\n        target_labels (list[str]): The target latent parameters to be optimized for\n        num_particles (int): The number of independent trials\n        evaluation (bool, optional): True for actual evalution of the EIG. Defaults to False.\n\n    Returns:\n        typing.Callable[ [chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry] ]: Loss function that return tuple of (1) Total loss, (2.1) Each terms without the average, (2.2) The EIG\n    \"\"\"\n\n    # Marginal loss\n    def loss_fn(param, key: jnp.ndarray) -&gt; tuple[jnp.ndarray, AuxEntry]:\n        expanded_design = lexpand(design, num_particles)\n        # vectorized(model, num_particles)\n        # Sample from p(y | d)\n        key, subkey = jax.random.split(key)\n        trace = handlers.trace(handlers.seed(model, subkey)).get_trace(\n            expanded_design,\n            *args,\n        )\n        y_dict = {\n            observation_label: trace[observation_label][\"value\"]\n            for observation_label in observation_labels\n        }\n\n        # Run through q(y | d)\n        key, subkey = jax.random.split(key)\n        conditioned_marginal_guide = handlers.condition(marginal_guide, data=y_dict)\n        cond_trace = handlers.trace(\n            handlers.substitute(\n                handlers.seed(conditioned_marginal_guide, subkey), data=param\n            )\n        ).get_trace(\n            expanded_design,\n            *args,\n            observation_labels=observation_labels,\n            target_labels=target_labels,\n        )\n        # Compute the log prob of observing the data\n        terms = -1 * jnp.array(\n            [\n                cond_trace[observation_label][\"fn\"].log_prob(\n                    cond_trace[observation_label][\"value\"]\n                )\n                for observation_label in observation_labels\n            ]\n        ).sum(axis=0)\n\n        if evaluation:\n            terms += jnp.array(\n                [\n                    trace[observation_label][\"fn\"].log_prob(\n                        trace[observation_label][\"value\"]\n                    )\n                    for observation_label in observation_labels\n                ]\n            ).sum(axis=0)\n\n        agg_loss, loss = _safe_mean_terms_v2(terms)\n        return agg_loss, AuxEntry(terms=terms, eig=loss)\n\n    return loss_fn\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.vnmc_eig_loss","title":"vnmc_eig_loss","text":"<pre><code>vnmc_eig_loss(model: Callable, marginal_guide: Callable, design: ndarray, *args, observation_labels: list[str], target_labels: list[str], num_particles: tuple[int, int], evaluation: bool = False) -&gt; Callable[[ArrayTree, ndarray], tuple[ndarray, AuxEntry]]\n</code></pre> <p>The VNMC loss implemented following https://docs.pyro.ai/en/dev/_modules/pyro/contrib/oed/eig.html#vnmc_eig</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Callable</code> <p>The probabilistic model</p> required <code>marginal_guide</code> <code>Callable</code> <p>The custom guide</p> required <code>design</code> <code>ndarray</code> <p>Possible designs of the experiment</p> required <code>observation_labels</code> <code>list[str]</code> <p>The list of string of observations</p> required <code>target_labels</code> <code>list[str]</code> <p>The target latent parameters to be optimized for</p> required <code>num_particles</code> <code>int</code> <p>The number of independent trials</p> required <code>evaluation</code> <code>bool</code> <p>True for actual evalution of the EIG. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Callable[[ArrayTree, ndarray], tuple[ndarray, AuxEntry]]</code> <p>typing.Callable[ [chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry] ]: Loss function that return tuple of (1) Total loss, (2.1) Each terms without the average, (2.2) The EIG</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>@warn_not_tested_function\ndef vnmc_eig_loss(\n    model: typing.Callable,\n    marginal_guide: typing.Callable,\n    design: jnp.ndarray,\n    *args,\n    observation_labels: list[str],\n    target_labels: list[str],\n    num_particles: tuple[int, int],\n    evaluation: bool = False,\n) -&gt; typing.Callable[[chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry]]:\n    \"\"\"The VNMC loss implemented following\n    https://docs.pyro.ai/en/dev/_modules/pyro/contrib/oed/eig.html#vnmc_eig\n\n    Args:\n        model (typing.Callable): The probabilistic model\n        marginal_guide (typing.Callable): The custom guide\n        design (jnp.ndarray): Possible designs of the experiment\n        observation_labels (list[str]): The list of string of observations\n        target_labels (list[str]): The target latent parameters to be optimized for\n        num_particles (int): The number of independent trials\n        evaluation (bool, optional): True for actual evalution of the EIG. Defaults to False.\n\n    Returns:\n        typing.Callable[ [chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry] ]: Loss function that return tuple of (1) Total loss, (2.1) Each terms without the average, (2.2) The EIG\n    \"\"\"\n\n    # Marginal loss\n    def loss_fn(param, key: jnp.ndarray) -&gt; tuple[jnp.ndarray, AuxEntry]:\n        N, M = num_particles\n\n        expanded_design = lexpand(design, N)\n\n        # Sample from p(y, theta | d)\n        key, subkey = jax.random.split(key)\n        trace = handlers.trace(handlers.seed(model, subkey)).get_trace(\n            expanded_design,\n            *args,\n        )\n        y_dict = {\n            observation_label: trace[observation_label][\"value\"]\n            for observation_label in observation_labels\n        }\n\n        # Sample M times from q(theta | y, d) for each y\n        key, subkey = jax.random.split(key)\n        reexpanded_design = lexpand(expanded_design, M)\n        conditioned_marginal_guide = handlers.condition(marginal_guide, data=y_dict)\n        cond_trace = handlers.trace(\n            handlers.substitute(\n                handlers.seed(conditioned_marginal_guide, subkey), data=param\n            )\n        ).get_trace(\n            reexpanded_design,\n            *args,\n            observation_labels=observation_labels,\n            target_labels=target_labels,\n        )\n\n        theta_y_dict = {\n            target_label: cond_trace[target_label][\"value\"]\n            for target_label in target_labels\n        }\n        theta_y_dict.update(y_dict)\n\n        # Re-run that through the model to compute the joint\n        key, subkey = jax.random.split(key)\n        conditioned_model = handlers.condition(model, data=theta_y_dict)\n        conditioned_model_trace = handlers.trace(\n            handlers.seed(conditioned_model, subkey)\n        ).get_trace(\n            reexpanded_design,\n            *args,\n        )\n\n        # Compute the log prob of observing the data\n        terms = -1 * jnp.array(\n            [\n                cond_trace[target_label][\"fn\"].log_prob(\n                    cond_trace[target_label][\"value\"]\n                )\n                for target_label in target_labels\n            ]\n        ).sum(axis=0)\n\n        terms += jnp.array(\n            [\n                conditioned_model_trace[target_label][\"fn\"].log_prob(\n                    conditioned_model_trace[target_label][\"value\"]\n                )\n                for target_label in target_labels\n            ]\n        ).sum(axis=0)\n\n        terms += jnp.array(\n            [\n                conditioned_model_trace[observation_label][\"fn\"].log_prob(\n                    conditioned_model_trace[observation_label][\"value\"]\n                )\n                for observation_label in observation_labels\n            ]\n        ).sum(axis=0)\n\n        terms = -jax.scipy.special.logsumexp(terms, axis=0) + jnp.log(M)\n\n        if evaluation:\n            terms += jnp.array(\n                [\n                    trace[observation_label][\"fn\"].log_prob(\n                        trace[observation_label][\"value\"]\n                    )\n                    for observation_label in observation_labels\n                ]\n            ).sum(axis=0)\n\n        agg_loss, loss = _safe_mean_terms_v2(terms)\n        return agg_loss, AuxEntry(terms=terms, eig=loss)\n\n    return loss_fn\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.init_params_from_guide","title":"init_params_from_guide","text":"<pre><code>init_params_from_guide(marginal_guide: Callable, *args, key: ndarray, design: ndarray) -&gt; ArrayTree\n</code></pre> <p>Initlalize parameters of marginal guide.</p> <p>Parameters:</p> Name Type Description Default <code>marginal_guide</code> <code>Callable</code> <p>Marginal guide to be used with marginal eig</p> required <code>key</code> <code>ndarray</code> <p>Random Key</p> required <code>design</code> <code>ndarray</code> <p>Example of the designs of the experiment</p> required <p>Returns:</p> Type Description <code>ArrayTree</code> <p>chex.ArrayTree: Random parameters for marginal guide to be optimized.</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def init_params_from_guide(\n    marginal_guide: typing.Callable,\n    *args,\n    key: jnp.ndarray,\n    design: jnp.ndarray,\n) -&gt; chex.ArrayTree:\n    \"\"\"Initlalize parameters of marginal guide.\n\n    Args:\n        marginal_guide (typing.Callable): Marginal guide to be used with marginal eig\n        key (jnp.ndarray): Random Key\n        design (jnp.ndarray): Example of the designs of the experiment\n\n    Returns:\n        chex.ArrayTree: Random parameters for marginal guide to be optimized.\n    \"\"\"\n    key, subkey = jax.random.split(key)\n    # expanded_design = lexpand(design, num_particles)\n    marginal_guide_trace = handlers.trace(\n        handlers.seed(marginal_guide, subkey)\n    ).get_trace(design, *args, observation_labels=[], target_labels=[])\n\n    # Get only nodes that are parameters\n    params = {\n        name: node[\"value\"]\n        for name, node in marginal_guide_trace.items()\n        if node[\"type\"] == \"param\"\n    }\n\n    return params\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.opt_eig_ape_loss","title":"opt_eig_ape_loss","text":"<pre><code>opt_eig_ape_loss(loss_fn: Callable[[ArrayTree, ndarray], tuple[ndarray, AuxEntry]], params: ArrayTree, num_steps: int, optim: GradientTransformation, key: ndarray, callbacks: list = []) -&gt; ArrayTree\n</code></pre> <p>Optimize the EIG loss function.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable[[ArrayTree, ndarray], tuple[ndarray, AuxEntry]]</code> <p>Loss function</p> required <code>params</code> <code>ArrayTree</code> <p>Initial parameter</p> required <code>num_steps</code> <code>int</code> <p>Number of optimization step</p> required <code>optim</code> <code>GradientTransformation</code> <p>Optax Optimizer</p> required <code>key</code> <code>ndarray</code> <p>Random key</p> required <p>Returns:</p> Type Description <code>ArrayTree</code> <p>tuple[chex.ArrayTree, list[typing.Any]]: Optimized parameters, and optimization history.</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def opt_eig_ape_loss(\n    loss_fn: typing.Callable[\n        [chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry]\n    ],\n    params: chex.ArrayTree,\n    num_steps: int,\n    optim: optax.GradientTransformation,\n    key: jnp.ndarray,\n    callbacks: list = [],\n) -&gt; chex.ArrayTree:\n    \"\"\"Optimize the EIG loss function.\n\n    Args:\n        loss_fn (typing.Callable[[chex.ArrayTree, jnp.ndarray], tuple[jnp.ndarray, AuxEntry]]): Loss function\n        params (chex.ArrayTree): Initial parameter\n        num_steps (int): Number of optimization step\n        optim (optax.GradientTransformation): Optax Optimizer\n        key (jnp.ndarray): Random key\n\n    Returns:\n        tuple[chex.ArrayTree, list[typing.Any]]: Optimized parameters, and optimization history.\n    \"\"\"\n    # Initialize the optimizer\n    opt_state = optim.init(params)\n    # jit the loss function\n    loss_fn = jax.jit(loss_fn)\n\n    for step in range(num_steps):\n        key, subkey = jax.random.split(key)\n        # Compute the loss and its gradient\n        (loss, aux), grad = jax.value_and_grad(loss_fn, has_aux=True)(params, subkey)\n        # Update the optimizer and params\n        updates, opt_state = optim.update(grad, opt_state, params)\n        params = optax.apply_updates(params, updates)\n\n        # entry = (step, loss, aux)\n        entry = HistoryEntry(step=step, loss=loss, aux=aux)\n\n        for callback in callbacks:\n            callback(entry)\n\n    return params\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.estimate_eig","title":"estimate_eig","text":"<pre><code>estimate_eig(key: ndarray, model: Callable, marginal_guide: Callable, design: ndarray, *args, optimizer: GradientTransformation, num_optimization_steps: int, observation_labels: list[str], target_labels: list[str], num_particles: tuple[int, int] | int, final_num_particles: tuple[int, int] | int | None = None, loss_fn: Callable = marginal_loss, callbacks: list = []) -&gt; tuple[ndarray, dict[str, Any]]\n</code></pre> <p>Optimize for marginal EIG</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>Random key</p> required <code>model</code> <code>Callable</code> <p>Probabilistic model of the experiment</p> required <code>marginal_guide</code> <code>Callable</code> <p>The marginal guide of the experiment</p> required <code>design</code> <code>ndarray</code> <p>Possible designs of the experiment</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer</p> required <code>num_optimization_steps</code> <code>int</code> <p>Number of the optimization step</p> required <code>observation_labels</code> <code>list[str]</code> <p>The list of string of observations</p> required <code>target_labels</code> <code>list[str]</code> <p>The target latent parameters to be optimized for</p> required <code>num_particles</code> <code>int</code> <p>The number of independent trials</p> required <code>final_num_particles</code> <code>int | None</code> <p>Final independent trials to calculate marginal EIG. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict[str, Any]]</code> <p>tuple[jnp.ndarray, dict[str, typing.Any]]: EIG, and tuple of optimized parameters and optimization history.</p> Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def estimate_eig(\n    key: jnp.ndarray,\n    model: typing.Callable,\n    marginal_guide: typing.Callable,\n    design: jnp.ndarray,\n    *args,\n    optimizer: optax.GradientTransformation,\n    num_optimization_steps: int,\n    observation_labels: list[str],\n    target_labels: list[str],\n    num_particles: tuple[int, int] | int,\n    final_num_particles: tuple[int, int] | int | None = None,\n    loss_fn: typing.Callable = marginal_loss,\n    callbacks: list = [],\n) -&gt; tuple[jnp.ndarray, dict[str, typing.Any]]:\n    \"\"\"Optimize for marginal EIG\n\n    Args:\n        key (jnp.ndarray): Random key\n        model (typing.Callable): Probabilistic model of the experiment\n        marginal_guide (typing.Callable): The marginal guide of the experiment\n        design (jnp.ndarray): Possible designs of the experiment\n        optimizer (optax.GradientTransformation): Optax optimizer\n        num_optimization_steps (int): Number of the optimization step\n        observation_labels (list[str]): The list of string of observations\n        target_labels (list[str]): The target latent parameters to be optimized for\n        num_particles (int): The number of independent trials\n        final_num_particles (int | None, optional): Final independent trials to calculate marginal EIG. Defaults to None.\n\n    Returns:\n        tuple[jnp.ndarray, dict[str, typing.Any]]: EIG, and tuple of optimized parameters and optimization history.\n    \"\"\"\n    # NOTE: In final evalution, if final_num_particles != num_particles,\n    # the code will error because we train params with num_particles\n    # the shape will mismatch\n    # final_num_particles = final_num_particles or num_particles\n\n    # Initialize the parameters by using trace from the marginal_guide\n    key, subkey = jax.random.split(key)\n    params = init_params_from_guide(\n        marginal_guide,\n        *args,\n        key=subkey,\n        design=design,\n    )\n\n    # Optimize the loss function first to get the optimal parameters\n    # for marginal guide\n    params = opt_eig_ape_loss(\n        loss_fn=loss_fn(\n            model,\n            marginal_guide,\n            design,\n            *args,\n            observation_labels=observation_labels,\n            target_labels=target_labels,\n            num_particles=num_particles,\n            evaluation=False,\n        ),\n        params=params,\n        num_steps=num_optimization_steps,\n        optim=optimizer,\n        key=subkey,\n        callbacks=callbacks,\n    )\n\n    key, subkey = jax.random.split(key)\n    # Evaluate the loss\n    _, aux = loss_fn(\n        model,\n        marginal_guide,\n        design,\n        *args,\n        observation_labels=observation_labels,\n        target_labels=target_labels,\n        num_particles=final_num_particles,\n        evaluation=True,\n    )(params, subkey)\n\n    return aux.eig, {\n        \"params\": params,\n    }\n</code></pre>"},{"location":"api/boed/#src.inspeqtor.experimental.boed.vectorized_for_eig","title":"vectorized_for_eig","text":"<pre><code>vectorized_for_eig(model)\n</code></pre> <p>Vectorization function for the EIG function</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Probabilistic model.</p> required Source code in <code>src/inspeqtor/experimental/boed.py</code> <pre><code>def vectorized_for_eig(model):\n    \"\"\"Vectorization function for the EIG function\n\n    Args:\n        model (typing.Any): Probabilistic model.\n    \"\"\"\n\n    def wrapper(\n        design: jnp.ndarray,\n        *args,\n        **kwargs,\n    ):\n        # This wrapper has the same call signature as the probabilistic graybox model\n        # Expect the design to has shape == (extra, design, feature)\n        with plate_stack(prefix=\"vectorized_plate\", sizes=[*design.shape[:2]]):\n            return model(design, *args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api/constant/","title":"constant","text":""},{"location":"api/constant/#src.inspeqtor.experimental.constant","title":"src.inspeqtor.experimental.constant","text":""},{"location":"api/control/","title":"control","text":""},{"location":"api/control/#src.inspeqtor.experimental.control","title":"src.inspeqtor.experimental.control","text":""},{"location":"api/control/#src.inspeqtor.experimental.control.BaseControl","title":"BaseControl  <code>dataclass</code>","text":"Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>@dataclass\nclass BaseControl(ABC):\n    duration: int\n\n    def __post_init__(self):\n        self.t_eval = jnp.arange(0, self.duration, 1)\n        self.validate()\n\n    def validate(self):\n        # Validate that all attributes are json serializable\n        try:\n            json.dumps(self.to_dict())\n        except TypeError as e:\n            raise TypeError(\n                f\"Cannot serialize {self.__class__.__name__} to json\"\n            ) from e\n\n        lower, upper = self.get_bounds()\n        # Validate that the sampling function is working\n        key = jax.random.PRNGKey(0)\n        params = sample_params(key, lower, upper)\n        waveform = self.get_waveform(params)\n\n        assert all(\n            [isinstance(k, str) for k in params.keys()]\n        ), \"All key of params dict must be string\"\n        assert all(\n            [isinstance(v, float) for v in params.values()]\n        ), \"All value of params dict must be float\"\n        assert isinstance(waveform, jax.Array), \"Waveform must be jax.Array\"\n\n        # Validate that params is serializable and deserializable\n        try:\n            reread_params = json.loads(json.dumps(params))\n            assert params == reread_params\n\n        except TypeError as e:\n            raise TypeError(\n                f\"Cannot serialize params dict of {self.__class__.__name__} to json\"\n            ) from e\n\n    @abstractmethod\n    def get_bounds(\n        self, *arg, **kwarg\n    ) -&gt; tuple[ParametersDictType, ParametersDictType]: ...\n\n    @abstractmethod\n    def get_envelope(self, params: ParametersDictType) -&gt; typing.Callable:\n        raise NotImplementedError(\"get_envelopes method is not implemented\")\n\n    def get_waveform(self, params: ParametersDictType) -&gt; jnp.ndarray:\n        \"\"\"Get the discrete waveform of the pulse\n\n        Args:\n            params (ParametersDictType): Control parameter\n\n        Returns:\n            jnp.ndarray: Waveform of the control.\n        \"\"\"\n        return jax.vmap(self.get_envelope(params), in_axes=(0,))((self.t_eval))\n\n    def to_dict(self) -&gt; dict[str, typing.Union[int, float, str]]:\n        \"\"\"Convert the control configuration to dictionary\n\n        Returns:\n            dict[str, typing.Union[int, float, str]]: Configuration of the control\n        \"\"\"\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, data):\n        \"\"\"Construct the control instace from the dictionary.\n\n        Args:\n            data (dict): Dictionary for construction of the control instance.\n\n        Returns:\n            The instance of the control.\n        \"\"\"\n        return cls(**data)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.BaseControl.get_waveform","title":"get_waveform","text":"<pre><code>get_waveform(params: ParametersDictType) -&gt; ndarray\n</code></pre> <p>Get the discrete waveform of the pulse</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ParametersDictType</code> <p>Control parameter</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Waveform of the control.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def get_waveform(self, params: ParametersDictType) -&gt; jnp.ndarray:\n    \"\"\"Get the discrete waveform of the pulse\n\n    Args:\n        params (ParametersDictType): Control parameter\n\n    Returns:\n        jnp.ndarray: Waveform of the control.\n    \"\"\"\n    return jax.vmap(self.get_envelope(params), in_axes=(0,))((self.t_eval))\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.BaseControl.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Union[int, float, str]]\n</code></pre> <p>Convert the control configuration to dictionary</p> <p>Returns:</p> Type Description <code>dict[str, Union[int, float, str]]</code> <p>dict[str, typing.Union[int, float, str]]: Configuration of the control</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def to_dict(self) -&gt; dict[str, typing.Union[int, float, str]]:\n    \"\"\"Convert the control configuration to dictionary\n\n    Returns:\n        dict[str, typing.Union[int, float, str]]: Configuration of the control\n    \"\"\"\n    return asdict(self)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.BaseControl.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data)\n</code></pre> <p>Construct the control instace from the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionary for construction of the control instance.</p> required <p>Returns:</p> Type Description <p>The instance of the control.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>@classmethod\ndef from_dict(cls, data):\n    \"\"\"Construct the control instace from the dictionary.\n\n    Args:\n        data (dict): Dictionary for construction of the control instance.\n\n    Returns:\n        The instance of the control.\n    \"\"\"\n    return cls(**data)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence","title":"ControlSequence  <code>dataclass</code>","text":"<p>Control sequence, expect to be sum of atomic control.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>@dataclass\nclass ControlSequence:\n    \"\"\"Control sequence, expect to be sum of atomic control.\"\"\"\n\n    controls: typing.Sequence[BaseControl]\n    total_dt: int\n    validate: bool = True\n\n    def __post_init__(self):\n        # validate that each pulse have len of total_dt\n        if self.validate:\n            self._validate()\n\n    def _validate(self):\n        # Must check that the sum of the pulse lengths is equal to the total length of the pulse sequence\n        key = jax.random.PRNGKey(0)\n        subkeys = jax.random.split(key, self.total_dt)\n        for pulse_key, pulse in zip(subkeys, self.controls):\n            params = sample_params(pulse_key, *pulse.get_bounds())\n            waveform = pulse.get_waveform(params)\n            assert isinstance(waveform, jax.Array)\n            # Assert the waveform is of the correct length\n            assert waveform.shape == (self.total_dt,)\n            # Assert that all key of params dict is string and all value is jax.Array\n            assert all(\n                [isinstance(k, str) for k in params.keys()]\n            ), \"All key of params dict must be string\"\n            assert all(\n                [isinstance(v, (float, int, jnp.ndarray)) for v in params.values()]\n            ), \"All value of params dict must be float or jax.Array\"\n\n        params = self.sample_params(key)\n\n        # Assert that the bounds have the same pytree structure as the parameters\n        lower, upper = self.get_bounds()\n\n        assert jax.tree.structure(lower) == jax.tree.structure(params)\n        assert jax.tree.structure(upper) == jax.tree.structure(params)\n\n    def sample_params(self, key: jax.Array) -&gt; list[ParametersDictType]:\n        \"\"\"Sample control parameter\n\n        Args:\n            key (jax.Array): Random key\n\n        Returns:\n            list[ParametersDictType]: control parameters\n        \"\"\"\n        # Split key for each pulse\n        subkeys = jax.random.split(key, self.total_dt)\n\n        params_list: list[ParametersDictType] = []\n        for pulse_key, pulse in zip(subkeys, self.controls):\n            params = sample_params(pulse_key, *pulse.get_bounds())\n            params_list.append(params)\n\n        return params_list\n\n    def get_waveform(self, params_list: list[ParametersDictType]) -&gt; jnp.ndarray:\n        \"\"\"\n        Samples the pulse sequence by generating random parameters for each pulse and computing the total waveform.\n\n        Parameters:\n            key (Key): The random key used for generating the parameters.\n\n        Returns:\n            tuple[list[ParametersDictType], Complex[Array, \"time\"]]: A tuple containing a list of parameter dictionaries for each pulse and the total waveform.\n\n        Example:\n            key = jax.random.PRNGKey(0)\n            params_list, total_waveform = sample(key)\n        \"\"\"\n        # Create base waveform\n        total_waveform = jnp.zeros(self.total_dt, dtype=jnp.complex64)\n\n        for _params, _pulse in zip(params_list, self.controls):\n            waveform = _pulse.get_waveform(_params)\n            total_waveform += waveform\n\n        return total_waveform\n\n    def get_envelope(self, params_list: list[ParametersDictType]) -&gt; typing.Callable:\n        \"\"\"Create envelope function with given control parameters\n\n        Args:\n            params_list (list[ParametersDictType]): control parameter to be used\n\n        Returns:\n            typing.Callable: Envelope function\n        \"\"\"\n        callables = []\n        for _params, _pulse in zip(params_list, self.controls):\n            callables.append(_pulse.get_envelope(_params))\n\n        # Create a function that returns the sum of the envelopes\n        def envelope(t):\n            return sum([c(t) for c in callables])\n\n        return envelope\n\n    def get_bounds(self) -&gt; tuple[list[ParametersDictType], list[ParametersDictType]]:\n        \"\"\"Get the bounds of the controls\n\n        Returns:\n            tuple[list[ParametersDictType], list[ParametersDictType]]: tuple of list of lower and upper bounds.\n        \"\"\"\n        lower_bounds = []\n        upper_bounds = []\n        for pulse in self.controls:\n            lower, upper = pulse.get_bounds()\n            lower_bounds.append(lower)\n            upper_bounds.append(upper)\n\n        return lower_bounds, upper_bounds\n\n    def get_parameter_names(self) -&gt; list[list[str]]:\n        \"\"\"Get the name of the control parameters in the control sequence.\n\n        Returns:\n            list[list[str]]: Structured name of control parameters.\n        \"\"\"\n        # Sample the pulse sequence to get the parameter names\n        key = jax.random.key(0)\n        params_list = self.sample_params(key)\n\n        # Get the parameter names for each pulse\n        parameter_names = []\n        for params in params_list:\n            parameter_names.append(list(params.keys()))\n\n        return parameter_names\n\n    def to_dict(self) -&gt; dict[str, typing.Any]:\n        \"\"\"Convert control sequence to dictionary.\n\n        Returns:\n            dict[str, typing.Any]: Control sequence configuration dict.\n        \"\"\"\n        return {\n            **asdict(self),\n            \"controls\": [\n                {**pulse.to_dict(), \"_name\": pulse.__class__.__name__}\n                for pulse in self.controls\n            ],\n        }\n\n    @classmethod\n    def from_dict(\n        cls, data: dict[str, typing.Any], controls: typing.Sequence[type[BaseControl]]\n    ) -&gt; \"ControlSequence\":\n        \"\"\"Construct the control sequence from dict.\n\n        Args:\n            data (dict[str, typing.Any]): Dict contain information for sequence construction\n            control (typing.Sequence[type[BasePulse]]): Constructor of the controls\n\n        Returns:\n            ControlSequence: Instance of the control sequence.\n        \"\"\"\n        parsed_data = []\n        for d, pulse in zip(data[\"controls\"], controls):\n            assert isinstance(d, dict), f\"Expected dict, got {type(d)}\"\n\n            # remove the _name key\n            d.pop(\"_name\")\n            parsed_data.append(pulse.from_dict(d))\n\n        data[\"controls\"] = parsed_data\n        data[\"validate\"] = True\n\n        return cls(**data)\n\n    def to_file(self, path: typing.Union[str, pathlib.Path]):\n        \"\"\"Save configuration of the pulse to file given folder path.\n\n        Args:\n            path (typing.Union[str, pathlib.Path]): Path to the folder to save sequence, will be created if not existed.\n        \"\"\"\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n\n        os.makedirs(path, exist_ok=True)\n        with open(path / \"control_sequence.json\", \"w\") as f:\n            json.dump(self.to_dict(), f, indent=4)\n\n    @classmethod\n    def from_file(\n        cls,\n        path: typing.Union[str, pathlib.Path],\n        controls: typing.Sequence[type[BaseControl]],\n    ) -&gt; \"ControlSequence\":\n        \"\"\"Construct control seqence from path\n\n        Args:\n            path (typing.Union[str, pathlib.Path]): Path to configuration of control sequence.\n            controls (typing.Sequence[type[BasePulse]]): Constructor of the control in the sequence.\n\n        Returns:\n            ControlSequence: Control sequence instance.\n        \"\"\"\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n\n        with open(path / \"control_sequence.json\", \"r\") as f:\n            dict_control_sequence = json.load(f)\n\n        return cls.from_dict(dict_control_sequence, controls=controls)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.sample_params","title":"sample_params","text":"<pre><code>sample_params(key: Array) -&gt; list[ParametersDictType]\n</code></pre> <p>Sample control parameter</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Array</code> <p>Random key</p> required <p>Returns:</p> Type Description <code>list[ParametersDictType]</code> <p>list[ParametersDictType]: control parameters</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def sample_params(self, key: jax.Array) -&gt; list[ParametersDictType]:\n    \"\"\"Sample control parameter\n\n    Args:\n        key (jax.Array): Random key\n\n    Returns:\n        list[ParametersDictType]: control parameters\n    \"\"\"\n    # Split key for each pulse\n    subkeys = jax.random.split(key, self.total_dt)\n\n    params_list: list[ParametersDictType] = []\n    for pulse_key, pulse in zip(subkeys, self.controls):\n        params = sample_params(pulse_key, *pulse.get_bounds())\n        params_list.append(params)\n\n    return params_list\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.get_waveform","title":"get_waveform","text":"<pre><code>get_waveform(params_list: list[ParametersDictType]) -&gt; ndarray\n</code></pre> <p>Samples the pulse sequence by generating random parameters for each pulse and computing the total waveform.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Key</code> <p>The random key used for generating the parameters.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>tuple[list[ParametersDictType], Complex[Array, \"time\"]]: A tuple containing a list of parameter dictionaries for each pulse and the total waveform.</p> Example <p>key = jax.random.PRNGKey(0) params_list, total_waveform = sample(key)</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def get_waveform(self, params_list: list[ParametersDictType]) -&gt; jnp.ndarray:\n    \"\"\"\n    Samples the pulse sequence by generating random parameters for each pulse and computing the total waveform.\n\n    Parameters:\n        key (Key): The random key used for generating the parameters.\n\n    Returns:\n        tuple[list[ParametersDictType], Complex[Array, \"time\"]]: A tuple containing a list of parameter dictionaries for each pulse and the total waveform.\n\n    Example:\n        key = jax.random.PRNGKey(0)\n        params_list, total_waveform = sample(key)\n    \"\"\"\n    # Create base waveform\n    total_waveform = jnp.zeros(self.total_dt, dtype=jnp.complex64)\n\n    for _params, _pulse in zip(params_list, self.controls):\n        waveform = _pulse.get_waveform(_params)\n        total_waveform += waveform\n\n    return total_waveform\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.get_envelope","title":"get_envelope","text":"<pre><code>get_envelope(params_list: list[ParametersDictType]) -&gt; Callable\n</code></pre> <p>Create envelope function with given control parameters</p> <p>Parameters:</p> Name Type Description Default <code>params_list</code> <code>list[ParametersDictType]</code> <p>control parameter to be used</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>typing.Callable: Envelope function</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def get_envelope(self, params_list: list[ParametersDictType]) -&gt; typing.Callable:\n    \"\"\"Create envelope function with given control parameters\n\n    Args:\n        params_list (list[ParametersDictType]): control parameter to be used\n\n    Returns:\n        typing.Callable: Envelope function\n    \"\"\"\n    callables = []\n    for _params, _pulse in zip(params_list, self.controls):\n        callables.append(_pulse.get_envelope(_params))\n\n    # Create a function that returns the sum of the envelopes\n    def envelope(t):\n        return sum([c(t) for c in callables])\n\n    return envelope\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.get_bounds","title":"get_bounds","text":"<pre><code>get_bounds() -&gt; tuple[list[ParametersDictType], list[ParametersDictType]]\n</code></pre> <p>Get the bounds of the controls</p> <p>Returns:</p> Type Description <code>tuple[list[ParametersDictType], list[ParametersDictType]]</code> <p>tuple[list[ParametersDictType], list[ParametersDictType]]: tuple of list of lower and upper bounds.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def get_bounds(self) -&gt; tuple[list[ParametersDictType], list[ParametersDictType]]:\n    \"\"\"Get the bounds of the controls\n\n    Returns:\n        tuple[list[ParametersDictType], list[ParametersDictType]]: tuple of list of lower and upper bounds.\n    \"\"\"\n    lower_bounds = []\n    upper_bounds = []\n    for pulse in self.controls:\n        lower, upper = pulse.get_bounds()\n        lower_bounds.append(lower)\n        upper_bounds.append(upper)\n\n    return lower_bounds, upper_bounds\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.get_parameter_names","title":"get_parameter_names","text":"<pre><code>get_parameter_names() -&gt; list[list[str]]\n</code></pre> <p>Get the name of the control parameters in the control sequence.</p> <p>Returns:</p> Type Description <code>list[list[str]]</code> <p>list[list[str]]: Structured name of control parameters.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def get_parameter_names(self) -&gt; list[list[str]]:\n    \"\"\"Get the name of the control parameters in the control sequence.\n\n    Returns:\n        list[list[str]]: Structured name of control parameters.\n    \"\"\"\n    # Sample the pulse sequence to get the parameter names\n    key = jax.random.key(0)\n    params_list = self.sample_params(key)\n\n    # Get the parameter names for each pulse\n    parameter_names = []\n    for params in params_list:\n        parameter_names.append(list(params.keys()))\n\n    return parameter_names\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict[str, Any]\n</code></pre> <p>Convert control sequence to dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, typing.Any]: Control sequence configuration dict.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def to_dict(self) -&gt; dict[str, typing.Any]:\n    \"\"\"Convert control sequence to dictionary.\n\n    Returns:\n        dict[str, typing.Any]: Control sequence configuration dict.\n    \"\"\"\n    return {\n        **asdict(self),\n        \"controls\": [\n            {**pulse.to_dict(), \"_name\": pulse.__class__.__name__}\n            for pulse in self.controls\n        ],\n    }\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict[str, Any], controls: Sequence[type[BaseControl]]) -&gt; ControlSequence\n</code></pre> <p>Construct the control sequence from dict.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dict contain information for sequence construction</p> required <code>control</code> <code>Sequence[type[BasePulse]]</code> <p>Constructor of the controls</p> required <p>Returns:</p> Name Type Description <code>ControlSequence</code> <code>ControlSequence</code> <p>Instance of the control sequence.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>@classmethod\ndef from_dict(\n    cls, data: dict[str, typing.Any], controls: typing.Sequence[type[BaseControl]]\n) -&gt; \"ControlSequence\":\n    \"\"\"Construct the control sequence from dict.\n\n    Args:\n        data (dict[str, typing.Any]): Dict contain information for sequence construction\n        control (typing.Sequence[type[BasePulse]]): Constructor of the controls\n\n    Returns:\n        ControlSequence: Instance of the control sequence.\n    \"\"\"\n    parsed_data = []\n    for d, pulse in zip(data[\"controls\"], controls):\n        assert isinstance(d, dict), f\"Expected dict, got {type(d)}\"\n\n        # remove the _name key\n        d.pop(\"_name\")\n        parsed_data.append(pulse.from_dict(d))\n\n    data[\"controls\"] = parsed_data\n    data[\"validate\"] = True\n\n    return cls(**data)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.to_file","title":"to_file","text":"<pre><code>to_file(path: Union[str, Path])\n</code></pre> <p>Save configuration of the pulse to file given folder path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the folder to save sequence, will be created if not existed.</p> required Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def to_file(self, path: typing.Union[str, pathlib.Path]):\n    \"\"\"Save configuration of the pulse to file given folder path.\n\n    Args:\n        path (typing.Union[str, pathlib.Path]): Path to the folder to save sequence, will be created if not existed.\n    \"\"\"\n    if isinstance(path, str):\n        path = pathlib.Path(path)\n\n    os.makedirs(path, exist_ok=True)\n    with open(path / \"control_sequence.json\", \"w\") as f:\n        json.dump(self.to_dict(), f, indent=4)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.ControlSequence.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: Union[str, Path], controls: Sequence[type[BaseControl]]) -&gt; ControlSequence\n</code></pre> <p>Construct control seqence from path</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to configuration of control sequence.</p> required <code>controls</code> <code>Sequence[type[BasePulse]]</code> <p>Constructor of the control in the sequence.</p> required <p>Returns:</p> Name Type Description <code>ControlSequence</code> <code>ControlSequence</code> <p>Control sequence instance.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    path: typing.Union[str, pathlib.Path],\n    controls: typing.Sequence[type[BaseControl]],\n) -&gt; \"ControlSequence\":\n    \"\"\"Construct control seqence from path\n\n    Args:\n        path (typing.Union[str, pathlib.Path]): Path to configuration of control sequence.\n        controls (typing.Sequence[type[BasePulse]]): Constructor of the control in the sequence.\n\n    Returns:\n        ControlSequence: Control sequence instance.\n    \"\"\"\n    if isinstance(path, str):\n        path = pathlib.Path(path)\n\n    with open(path / \"control_sequence.json\", \"r\") as f:\n        dict_control_sequence = json.load(f)\n\n    return cls.from_dict(dict_control_sequence, controls=controls)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.sample_params","title":"sample_params","text":"<pre><code>sample_params(key: ndarray, lower: ParametersDictType, upper: ParametersDictType) -&gt; ParametersDictType\n</code></pre> <p>Sample parameters with the same shape with given lower and upper bounds</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>Random key</p> required <code>lower</code> <code>ParametersDictType</code> <p>Lower bound</p> required <code>upper</code> <code>ParametersDictType</code> <p>Upper bound</p> required <p>Returns:</p> Name Type Description <code>ParametersDictType</code> <code>ParametersDictType</code> <p>Dict of the sampled parameters</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def sample_params(\n    key: jnp.ndarray, lower: ParametersDictType, upper: ParametersDictType\n) -&gt; ParametersDictType:\n    \"\"\"Sample parameters with the same shape with given lower and upper bounds\n\n    Args:\n        key (jnp.ndarray): Random key\n        lower (ParametersDictType): Lower bound\n        upper (ParametersDictType): Upper bound\n\n    Returns:\n        ParametersDictType: Dict of the sampled parameters\n    \"\"\"\n    # This function is general because it is depend only on lower and upper structure\n    param: ParametersDictType = {}\n    param_names = lower.keys()\n    for name in param_names:\n        sample_key, key = jax.random.split(key)\n        param[name] = jax.random.uniform(\n            sample_key, shape=(), dtype=float, minval=lower[name], maxval=upper[name]\n        )\n\n    # return jax.tree.map(float, param)\n    return param\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.array_to_list_of_params","title":"array_to_list_of_params","text":"<pre><code>array_to_list_of_params(array: ndarray, parameter_structure: list[list[str]]) -&gt; list[ParametersDictType]\n</code></pre> <p>Convert the array of control parameter to the list form</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>Control parameter array</p> required <code>parameter_structure</code> <code>list[list[str]]</code> <p>The structure of the control sequence</p> required <p>Returns:</p> Type Description <code>list[ParametersDictType]</code> <p>list[ParametersDictType]: Control parameter in the list form.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def array_to_list_of_params(\n    array: jnp.ndarray, parameter_structure: list[list[str]]\n) -&gt; list[ParametersDictType]:\n    \"\"\"Convert the array of control parameter to the list form\n\n    Args:\n        array (jnp.ndarray): Control parameter array\n        parameter_structure (list[list[str]]): The structure of the control sequence\n\n    Returns:\n        list[ParametersDictType]: Control parameter in the list form.\n    \"\"\"\n    temp: list[ParametersDictType] = []\n    idx = 0\n    for sub_pulse in parameter_structure:\n        temp_dict: ParametersDictType = {}\n        for param in sub_pulse:\n            temp_dict[param] = array[idx]\n            idx += 1\n        temp.append(temp_dict)\n\n    return temp\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.list_of_params_to_array","title":"list_of_params_to_array","text":"<pre><code>list_of_params_to_array(params: list[ParametersDictType], parameter_structure: list[list[str]]) -&gt; ndarray\n</code></pre> <p>Convert the control parameter in the list form to flatten array form</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>list[ParametersDictType]</code> <p>Control parameter in the list form</p> required <code>parameter_structure</code> <code>list[list[str]]</code> <p>The structure of the control sequence</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Control parameters array</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def list_of_params_to_array(\n    params: list[ParametersDictType], parameter_structure: list[list[str]]\n) -&gt; jnp.ndarray:\n    \"\"\"Convert the control parameter in the list form to flatten array form\n\n    Args:\n        params (list[ParametersDictType]): Control parameter in the list form\n        parameter_structure (list[list[str]]): The structure of the control sequence\n\n    Returns:\n        jnp.ndarray: Control parameters array\n    \"\"\"\n    temp = []\n    for subp_idx, sub_pulse in enumerate(parameter_structure):\n        for param in sub_pulse:\n            temp.append(params[subp_idx][param])\n\n    return jnp.array(temp)\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.get_param_array_converter","title":"get_param_array_converter","text":"<pre><code>get_param_array_converter(control_sequence: ControlSequence)\n</code></pre> <p>This function returns two functions that can convert between a list of parameter dictionaries and a flat array.</p> <p>array_to_list_of_params_fn, list_of_params_to_array_fn = get_param_array_converter(control_sequence)</p> <pre><code>Args:\ncontrol_sequence (ControlSequence): The pulse sequence object.\n</code></pre> <p>Returns:</p> Type Description <p>typing.Any: A tuple containing two functions. The first function converts an array to a list of parameter dictionaries, and the second function converts a list of parameter dictionaries to an array.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def get_param_array_converter(control_sequence: ControlSequence):\n    \"\"\"This function returns two functions that can convert between a list of parameter dictionaries and a flat array.\n\n    &gt;&gt;&gt; array_to_list_of_params_fn, list_of_params_to_array_fn = get_param_array_converter(control_sequence)\n\n        Args:\n        control_sequence (ControlSequence): The pulse sequence object.\n\n    Returns:\n        typing.Any: A tuple containing two functions. The first function converts an array to a list of parameter dictionaries, and the second function converts a list of parameter dictionaries to an array.\n    \"\"\"\n    structure = control_sequence.get_parameter_names()\n\n    def array_to_list_of_params_fn(\n        array: jnp.ndarray,\n    ) -&gt; list[ParametersDictType]:\n        return array_to_list_of_params(array, structure)\n\n    def list_of_params_to_array_fn(\n        params: list[ParametersDictType],\n    ) -&gt; jnp.ndarray:\n        return list_of_params_to_array(params, structure)\n\n    return array_to_list_of_params_fn, list_of_params_to_array_fn\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.construct_control_sequence_reader","title":"construct_control_sequence_reader","text":"<pre><code>construct_control_sequence_reader(controls: list[type[BaseControl]] = []) -&gt; Callable[[Union[str, Path]], ControlSequence]\n</code></pre> <p>Construct the control sequence reader</p> <p>Parameters:</p> Name Type Description Default <code>controls</code> <code>list[type[BasePulse]]</code> <p>List of control constructor. Defaults to [].</p> <code>[]</code> <p>Returns:</p> Type Description <code>Callable[[Union[str, Path]], ControlSequence]</code> <p>typing.Callable[[typing.Union[str, pathlib.Path]], controlsequence]: Control sequence reader that will automatically contruct control sequence from path.</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def construct_control_sequence_reader(\n    controls: list[type[BaseControl]] = [],\n) -&gt; typing.Callable[[typing.Union[str, pathlib.Path]], ControlSequence]:\n    \"\"\"Construct the control sequence reader\n\n    Args:\n        controls (list[type[BasePulse]], optional): List of control constructor. Defaults to [].\n\n    Returns:\n        typing.Callable[[typing.Union[str, pathlib.Path]], controlsequence]: Control sequence reader that will automatically contruct control sequence from path.\n    \"\"\"\n    default_controls: list[type[BaseControl]] = []\n\n    # Merge the default controls with the provided controls\n    controls_list = default_controls + controls\n\n    def control_sequence_reader(\n        path: typing.Union[str, pathlib.Path],\n    ) -&gt; ControlSequence:\n        \"\"\"Construct control sequence from path\n\n        Args:\n            path (typing.Union[str, pathlib.Path]): Path of the saved control sequence configuration.\n\n        Returns:\n            ControlSeqence: Control sequence instance.\n        \"\"\"\n        if isinstance(path, str):\n            path = pathlib.Path(path)\n\n        with open(path / \"control_sequence.json\", \"r\") as f:\n            control_sequence_dict = json.load(f)\n\n        parsed_controls = []\n\n        for pulse_dict in control_sequence_dict[\"controls\"]:\n            for control_class in controls_list:\n                if pulse_dict[\"_name\"] == control_class.__name__:\n                    parsed_controls.append(control_class)\n\n        return ControlSequence.from_dict(\n            control_sequence_dict, controls=parsed_controls\n        )\n\n    return control_sequence_reader\n</code></pre>"},{"location":"api/control/#src.inspeqtor.experimental.control.get_envelope_transformer","title":"get_envelope_transformer","text":"<pre><code>get_envelope_transformer(control_sequence: ControlSequence)\n</code></pre> <p>Generate get_envelope function with control parameter array as an input instead of list form</p> <p>Parameters:</p> Name Type Description Default <code>control_sequence</code> <code>ControlSequence</code> <p>Control seqence instance</p> required <p>Returns:</p> Type Description <p>typing.Callable[[jnp.ndarray], typing.Any]: Transformed get envelope function</p> Source code in <code>src/inspeqtor/experimental/control.py</code> <pre><code>def get_envelope_transformer(control_sequence: ControlSequence):\n    \"\"\"Generate get_envelope function with control parameter array as an input instead of list form\n\n    Args:\n        control_sequence (ControlSequence): Control seqence instance\n\n    Returns:\n        typing.Callable[[jnp.ndarray], typing.Any]: Transformed get envelope function\n    \"\"\"\n    structure = control_sequence.get_parameter_names()\n\n    def array_to_list_of_params_fn(array: jnp.ndarray):\n        return array_to_list_of_params(array, structure)\n\n    def get_envelope(params: jnp.ndarray) -&gt; typing.Callable[..., typing.Any]:\n        return control_sequence.get_envelope(array_to_list_of_params_fn(params))\n\n    return get_envelope\n</code></pre>"},{"location":"api/data/","title":"data","text":""},{"location":"api/data/#src.inspeqtor.experimental.data","title":"src.inspeqtor.experimental.data","text":""},{"location":"api/data/#src.inspeqtor.experimental.data.Operator","title":"Operator  <code>dataclass</code>","text":"<p>Dataclass for accessing qubit operators. Support X, Y, Z, Hadamard, S, Sdg, and I gate.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Provided operator is not supperted</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@dataclass\nclass Operator:\n    \"\"\"Dataclass for accessing qubit operators. Support X, Y, Z, Hadamard, S, Sdg, and I gate.\n\n    Raises:\n        ValueError: Provided operator is not supperted\n\n    \"\"\"\n\n    _pauli_x = jnp.array([[0, 1], [1, 0]], dtype=jnp.complex64)\n    _pauli_y = jnp.array([[0, -1j], [1j, 0]], dtype=jnp.complex64)\n    _pauli_z = jnp.array([[1, 0], [0, -1]], dtype=jnp.complex64)\n    _hadamard = jnp.array([[1, 1], [1, -1]], dtype=jnp.complex64) / jnp.sqrt(2)\n    _s_gate = jnp.array([[1, 0], [0, 1j]], dtype=jnp.complex64)\n    _sdg_gate = jnp.array([[1, 0], [0, -1j]], dtype=jnp.complex64)\n    _identity = jnp.array([[1, 0], [0, 1]], dtype=jnp.complex64)\n\n    @classmethod\n    def from_label(cls, op: str) -&gt; jnp.ndarray:\n        \"\"\"Initialize the operator from the label\n\n        Args:\n            op (str): The label of the operator\n\n        Raises:\n            ValueError: Operator not supported\n\n        Returns:\n            jnp.ndarray: The operator\n        \"\"\"\n\n        if op == \"X\":\n            operator = cls._pauli_x\n        elif op == \"Y\":\n            operator = cls._pauli_y\n        elif op == \"Z\":\n            operator = cls._pauli_z\n        elif op == \"H\":\n            operator = cls._hadamard\n        elif op == \"S\":\n            operator = cls._s_gate\n        elif op == \"Sdg\":\n            operator = cls._sdg_gate\n        elif op == \"I\":\n            operator = cls._identity\n        else:\n            raise ValueError(f\"Operator {op} is not supported\")\n\n        return operator\n\n    @classmethod\n    def to_qutrit(cls, op: jnp.ndarray, value: float = 1.0) -&gt; jnp.ndarray:\n        \"\"\"Add extra dimension to the operator\n\n        Args:\n            op (jnp.ndarray): Qubit operator\n            value (float, optional): Value to be add at the extra dimension diagonal entry. Defaults to 1.0.\n\n        Returns:\n            jnp.ndarray: New operator for qutrit space.\n        \"\"\"\n        return add_hilbert_level(op, x=jnp.array([value]))\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.Operator.from_label","title":"from_label  <code>classmethod</code>","text":"<pre><code>from_label(op: str) -&gt; ndarray\n</code></pre> <p>Initialize the operator from the label</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>str</code> <p>The label of the operator</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Operator not supported</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The operator</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@classmethod\ndef from_label(cls, op: str) -&gt; jnp.ndarray:\n    \"\"\"Initialize the operator from the label\n\n    Args:\n        op (str): The label of the operator\n\n    Raises:\n        ValueError: Operator not supported\n\n    Returns:\n        jnp.ndarray: The operator\n    \"\"\"\n\n    if op == \"X\":\n        operator = cls._pauli_x\n    elif op == \"Y\":\n        operator = cls._pauli_y\n    elif op == \"Z\":\n        operator = cls._pauli_z\n    elif op == \"H\":\n        operator = cls._hadamard\n    elif op == \"S\":\n        operator = cls._s_gate\n    elif op == \"Sdg\":\n        operator = cls._sdg_gate\n    elif op == \"I\":\n        operator = cls._identity\n    else:\n        raise ValueError(f\"Operator {op} is not supported\")\n\n    return operator\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.Operator.to_qutrit","title":"to_qutrit  <code>classmethod</code>","text":"<pre><code>to_qutrit(op: ndarray, value: float = 1.0) -&gt; ndarray\n</code></pre> <p>Add extra dimension to the operator</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>ndarray</code> <p>Qubit operator</p> required <code>value</code> <code>float</code> <p>Value to be add at the extra dimension diagonal entry. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: New operator for qutrit space.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@classmethod\ndef to_qutrit(cls, op: jnp.ndarray, value: float = 1.0) -&gt; jnp.ndarray:\n    \"\"\"Add extra dimension to the operator\n\n    Args:\n        op (jnp.ndarray): Qubit operator\n        value (float, optional): Value to be add at the extra dimension diagonal entry. Defaults to 1.0.\n\n    Returns:\n        jnp.ndarray: New operator for qutrit space.\n    \"\"\"\n    return add_hilbert_level(op, x=jnp.array([value]))\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.State","title":"State  <code>dataclass</code>","text":"<p>Dataclass for accessing eigenvector corresponded to eigenvalue of Pauli operator X, Y, and Z.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Provided state is not supported</p> <code>ValueError</code> <p>Provided state is not qubit</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@dataclass\nclass State:\n    \"\"\"Dataclass for accessing eigenvector corresponded to eigenvalue of Pauli operator X, Y, and Z.\n\n    Raises:\n        ValueError: Provided state is not supported\n        ValueError: Provided state is not qubit\n    \"\"\"\n\n    _zero = jnp.array([1, 0], dtype=jnp.complex64)\n    _one = jnp.array([0, 1], dtype=jnp.complex64)\n    _plus = jnp.array([1, 1], dtype=jnp.complex64) / jnp.sqrt(2)\n    _minus = jnp.array([1, -1], dtype=jnp.complex64) / jnp.sqrt(2)\n    _right = jnp.array([1, 1j], dtype=jnp.complex64) / jnp.sqrt(2)\n    _left = jnp.array([1, -1j], dtype=jnp.complex64) / jnp.sqrt(2)\n\n    @classmethod\n    def from_label(cls, state: str, dm: bool = False) -&gt; jnp.ndarray:\n        \"\"\"Initialize the state from the label\n\n        Args:\n            state (str): The label of the state\n            dm (bool, optional): Initialized as statevector or density matrix. Defaults to False.\n\n        Raises:\n            ValueError: State not supported\n\n        Returns:\n            jnp.ndarray: The state\n        \"\"\"\n\n        if state in [\"0\", \"Z+\"]:\n            state_vec = cls._zero\n        elif state in [\"1\", \"Z-\"]:\n            state_vec = cls._one\n        elif state in [\"+\", \"X+\"]:\n            state_vec = cls._plus\n        elif state in [\"-\", \"X-\"]:\n            state_vec = cls._minus\n        elif state in [\"r\", \"Y+\"]:\n            state_vec = cls._right\n        elif state in [\"l\", \"Y-\"]:\n            state_vec = cls._left\n        else:\n            raise ValueError(f\"State {state} is not supported\")\n\n        state_vec = state_vec.reshape(2, 1)\n\n        return state_vec if not dm else jnp.outer(state_vec, state_vec.conj())\n\n    @classmethod\n    def to_qutrit(cls, state: jnp.ndarray) -&gt; jnp.ndarray:\n        \"\"\"Promote qubit state to qutrit with zero probability\n\n        Args:\n            state (jnp.ndarray): Density matrix of 2 x 2 qubit state.\n\n        Raises:\n            ValueError: Provided state is not qubit\n\n        Returns:\n            jnp.ndarray: Qutrit density matrix\n        \"\"\"\n        if state.shape != (2, 2):\n            raise ValueError(\"Shape of the state is not as expected, expect (2, 2)\")\n\n        return add_hilbert_level(state, x=jnp.array([0.0]))\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.State.from_label","title":"from_label  <code>classmethod</code>","text":"<pre><code>from_label(state: str, dm: bool = False) -&gt; ndarray\n</code></pre> <p>Initialize the state from the label</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>str</code> <p>The label of the state</p> required <code>dm</code> <code>bool</code> <p>Initialized as statevector or density matrix. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>State not supported</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The state</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@classmethod\ndef from_label(cls, state: str, dm: bool = False) -&gt; jnp.ndarray:\n    \"\"\"Initialize the state from the label\n\n    Args:\n        state (str): The label of the state\n        dm (bool, optional): Initialized as statevector or density matrix. Defaults to False.\n\n    Raises:\n        ValueError: State not supported\n\n    Returns:\n        jnp.ndarray: The state\n    \"\"\"\n\n    if state in [\"0\", \"Z+\"]:\n        state_vec = cls._zero\n    elif state in [\"1\", \"Z-\"]:\n        state_vec = cls._one\n    elif state in [\"+\", \"X+\"]:\n        state_vec = cls._plus\n    elif state in [\"-\", \"X-\"]:\n        state_vec = cls._minus\n    elif state in [\"r\", \"Y+\"]:\n        state_vec = cls._right\n    elif state in [\"l\", \"Y-\"]:\n        state_vec = cls._left\n    else:\n        raise ValueError(f\"State {state} is not supported\")\n\n    state_vec = state_vec.reshape(2, 1)\n\n    return state_vec if not dm else jnp.outer(state_vec, state_vec.conj())\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.State.to_qutrit","title":"to_qutrit  <code>classmethod</code>","text":"<pre><code>to_qutrit(state: ndarray) -&gt; ndarray\n</code></pre> <p>Promote qubit state to qutrit with zero probability</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ndarray</code> <p>Density matrix of 2 x 2 qubit state.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Provided state is not qubit</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Qutrit density matrix</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@classmethod\ndef to_qutrit(cls, state: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Promote qubit state to qutrit with zero probability\n\n    Args:\n        state (jnp.ndarray): Density matrix of 2 x 2 qubit state.\n\n    Raises:\n        ValueError: Provided state is not qubit\n\n    Returns:\n        jnp.ndarray: Qutrit density matrix\n    \"\"\"\n    if state.shape != (2, 2):\n        raise ValueError(\"Shape of the state is not as expected, expect (2, 2)\")\n\n    return add_hilbert_level(state, x=jnp.array([0.0]))\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.QubitInformation","title":"QubitInformation  <code>dataclass</code>","text":"<p>Dataclass to store qubit information</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>str</code> <p>The string representation of unit, currently support \"GHz\", \"2piGHz\", \"2piHz\", or \"Hz\".</p> required <code>qubit_idx</code> <code>int</code> <p>the index of the qubit.</p> required <code>anharmonicity</code> <code>float</code> <p>Anhamonicity of the qubit, kept for the sake of completeness.</p> required <code>frequency</code> <code>float</code> <p>Qubit frequency.</p> required <code>drive_strength</code> <code>float</code> <p>Drive strength of qubit, might be specific for IBMQ platform.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Fail to convert unit to GHz</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@dataclass\nclass QubitInformation:\n    \"\"\"Dataclass to store qubit information\n\n    Args:\n        unit (str): The string representation of unit, currently support \"GHz\", \"2piGHz\", \"2piHz\", or \"Hz\".\n        qubit_idx (int): the index of the qubit.\n        anharmonicity (float): Anhamonicity of the qubit, kept for the sake of completeness.\n        frequency (float): Qubit frequency.\n        drive_strength (float): Drive strength of qubit, might be specific for IBMQ platform.\n\n    Raises:\n        ValueError: Fail to convert unit to GHz\n    \"\"\"\n\n    unit: str\n    qubit_idx: int\n    anharmonicity: float\n    frequency: float\n    drive_strength: float\n    date: str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    def __post_init__(self):\n        self.convert_unit_to_ghz()\n\n    def convert_unit_to_ghz(self):\n        \"\"\"Convert the unit of data stored in self to unit of GHz\n\n        Raises:\n            ValueError: Data stored in the unsupported unit\n        \"\"\"\n        if self.unit == \"GHz\":\n            pass\n        elif self.unit == \"Hz\":\n            self.anharmonicity = self.anharmonicity * 1e-9\n            self.frequency = self.frequency * 1e-9\n            self.drive_strength = self.drive_strength * 1e-9\n        elif self.unit == \"2piGHz\":\n            self.anharmonicity = self.anharmonicity / (2 * jnp.pi)\n            self.frequency = self.frequency / (2 * jnp.pi)\n            self.drive_strength = self.drive_strength / (2 * jnp.pi)\n        elif self.unit == \"2piHz\":\n            self.anharmonicity = self.anharmonicity / (2 * jnp.pi) * 1e-9\n            self.frequency = self.frequency / (2 * jnp.pi) * 1e-9\n            self.drive_strength = self.drive_strength / (2 * jnp.pi) * 1e-9\n        else:\n            raise ValueError(\"Unit must be GHz, 2piGHz, 2piHz, or Hz\")\n\n        # Set unit to GHz\n        self.unit = \"GHz\"\n\n    def to_dict(self):\n        return asdict(self)\n\n    @classmethod\n    def from_dict(cls, dict_qubit_info: dict):\n        return cls(**dict_qubit_info)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.QubitInformation.convert_unit_to_ghz","title":"convert_unit_to_ghz","text":"<pre><code>convert_unit_to_ghz()\n</code></pre> <p>Convert the unit of data stored in self to unit of GHz</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Data stored in the unsupported unit</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def convert_unit_to_ghz(self):\n    \"\"\"Convert the unit of data stored in self to unit of GHz\n\n    Raises:\n        ValueError: Data stored in the unsupported unit\n    \"\"\"\n    if self.unit == \"GHz\":\n        pass\n    elif self.unit == \"Hz\":\n        self.anharmonicity = self.anharmonicity * 1e-9\n        self.frequency = self.frequency * 1e-9\n        self.drive_strength = self.drive_strength * 1e-9\n    elif self.unit == \"2piGHz\":\n        self.anharmonicity = self.anharmonicity / (2 * jnp.pi)\n        self.frequency = self.frequency / (2 * jnp.pi)\n        self.drive_strength = self.drive_strength / (2 * jnp.pi)\n    elif self.unit == \"2piHz\":\n        self.anharmonicity = self.anharmonicity / (2 * jnp.pi) * 1e-9\n        self.frequency = self.frequency / (2 * jnp.pi) * 1e-9\n        self.drive_strength = self.drive_strength / (2 * jnp.pi) * 1e-9\n    else:\n        raise ValueError(\"Unit must be GHz, 2piGHz, 2piHz, or Hz\")\n\n    # Set unit to GHz\n    self.unit = \"GHz\"\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExpectationValue","title":"ExpectationValue  <code>dataclass</code>","text":"<p>Dataclass to store expectation value information</p> <p>Parameters:</p> Name Type Description Default <code>initial_state</code> <code>str</code> <p>String representation of inital state. Currently support \"+\", \"-\", \"r\", \"l\", \"0\", \"1\".</p> required <code>observable</code> <code>str</code> <p>String representation of quantum observable.  Currently support \"X\", \"Y\", \"Z\".</p> required <code>expectation_value</code> <code>None | float</code> <p>the expectation value. Default to None</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Not support initial state</p> <code>ValueError</code> <p>Not support observable</p> <code>ValueError</code> <p>Not support initial state</p> <code>ValueError</code> <p>Not support observable</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@dataclass\nclass ExpectationValue:\n    \"\"\"Dataclass to store expectation value information\n\n    Args:\n        initial_state (str): String representation of inital state. Currently support \"+\", \"-\", \"r\", \"l\", \"0\", \"1\".\n        observable (str): String representation of quantum observable.  Currently support \"X\", \"Y\", \"Z\".\n        expectation_value (None | float): the expectation value. Default to None\n\n    Raises:\n        ValueError: Not support initial state\n        ValueError: Not support observable\n        ValueError: Not support initial state\n        ValueError: Not support observable\n    \"\"\"\n\n    initial_state: str\n    observable: str\n    expectation_value: None | float = None\n\n    # Not serialized\n    initial_statevector: jnp.ndarray = field(init=False)\n    initial_density_matrix: jnp.ndarray = field(init=False)\n    observable_matrix: jnp.ndarray = field(init=False)\n\n    def __post_init__(self):\n        if self.initial_state not in [\"+\", \"-\", \"r\", \"l\", \"0\", \"1\"]:\n            raise ValueError(f\"Initial state {self.initial_state} is not supported\")\n        if self.observable not in [\"X\", \"Y\", \"Z\"]:\n            raise ValueError(f\"Observable {self.observable} is not supported\")\n\n        self.initial_statevector = State.from_label(self.initial_state)\n        self.initial_density_matrix = State.from_label(self.initial_state, dm=True)\n        self.observable_matrix = Operator.from_label(self.observable)\n\n    def to_dict(self):\n        return {\n            \"initial_state\": self.initial_state,\n            \"observable\": self.observable,\n            \"expectation_value\": self.expectation_value,\n        }\n\n    def __eq__(self, __value: object) -&gt; bool:\n        if not isinstance(__value, ExpectationValue):\n            return False\n\n        return (\n            self.initial_state == __value.initial_state\n            and self.observable == __value.observable\n            and self.expectation_value == __value.expectation_value\n        )\n\n    def __str__(self):\n        return f\"{self.initial_state}/{self.observable} = {self.expectation_value}\"\n\n    # Overwrite the __repr__ method of the class\n    def __repr__(self):\n        return f'{self.__class__.__name__}(initial_state=\"{self.initial_state}\", observable=\"{self.observable}\", expectation_value={self.expectation_value})'\n\n    @classmethod\n    def from_dict(cls, data):\n        return cls(**data)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentConfiguration","title":"ExperimentConfiguration  <code>dataclass</code>","text":"<p>Experiment configuration dataclass</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@dataclass\nclass ExperimentConfiguration:\n    \"\"\"Experiment configuration dataclass\"\"\"\n\n    qubits: typing.Sequence[QubitInformation]\n    expectation_values_order: typing.Sequence[ExpectationValue]\n    parameter_names: typing.Sequence[\n        typing.Sequence[str]\n    ]  # Get from the pulse sequence .get_parameter_names()\n    backend_name: str\n    shots: int\n    EXPERIMENT_IDENTIFIER: str\n    EXPERIMENT_TAGS: typing.Sequence[str]\n    description: str\n    device_cycle_time_ns: float\n    sequence_duration_dt: int\n    instance: str\n    sample_size: int\n    date: str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    additional_info: dict[str, str | int | float] = field(default_factory=dict)\n\n    def to_dict(self):\n        return {\n            **asdict(self),\n            \"qubits\": [qubit.to_dict() for qubit in self.qubits],\n            \"expectation_values_order\": [\n                exp.to_dict() for exp in self.expectation_values_order\n            ],\n        }\n\n    @classmethod\n    def from_dict(cls, dict_experiment_config):\n        dict_experiment_config[\"qubits\"] = [\n            QubitInformation.from_dict(qubit)\n            for qubit in dict_experiment_config[\"qubits\"]\n        ]\n\n        dict_experiment_config[\"expectation_values_order\"] = [\n            ExpectationValue.from_dict(exp)\n            for exp in dict_experiment_config[\"expectation_values_order\"]\n        ]\n\n        return cls(**dict_experiment_config)\n\n    def to_file(self, path: typing.Union[Path, str]):\n        if isinstance(path, str):\n            path = Path(path)\n\n        # os.makedirs(path, exist_ok=True)\n        path.mkdir(parents=True, exist_ok=True)\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(self.to_dict(), f, indent=4)\n\n    @classmethod\n    def from_file(cls, path: typing.Union[Path, str]):\n        if isinstance(path, str):\n            path = Path(path)\n        with open(path / \"config.json\", \"r\") as f:\n            dict_experiment_config = json.load(f)\n\n        return cls.from_dict(dict_experiment_config)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData","title":"ExperimentData  <code>dataclass</code>","text":"<p>Dataclass for processing of the characterization dataset. A difference between preprocess and postprocess dataset is that postprocess group expectation values same control parameter id within single row instead of multiple rows.</p> <p>Parameters:</p> Name Type Description Default <code>experiment_config</code> <code>ExperimentConfiguration</code> <p>Experiment configuration</p> required <code>preprocess_data</code> <code>DataFrame</code> <p>Pandas dataframe containing the preprocess dataset</p> required <code>_postprocessed_data</code> <code>DataFrame | None</code> <p>(pd.DataFrame): Provide this optional argument to skip dataset postprocessing.</p> <code>None</code> <code>keep_decimal</code> <code>int</code> <p>the precision of floating point to keep.</p> <code>10</code> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@dataclass\nclass ExperimentData:\n    \"\"\"Dataclass for processing of the characterization dataset.\n    A difference between preprocess and postprocess dataset is that postprocess group\n    expectation values same control parameter id within single row instead of multiple rows.\n\n    Args:\n        experiment_config (ExperimentConfiguration): Experiment configuration\n        preprocess_data (pd.DataFrame): Pandas dataframe containing the preprocess dataset\n        _postprocessed_data: (pd.DataFrame): Provide this optional argument to skip dataset postprocessing.\n        keep_decimal (int): the precision of floating point to keep.\n    \"\"\"\n\n    experiment_config: ExperimentConfiguration\n    preprocess_data: pd.DataFrame\n    # optional\n    _postprocessed_data: pd.DataFrame | None = field(default=None)\n\n    # Setting\n    keep_decimal: int = 10\n    # Postprocessing\n    postprocessed_data: pd.DataFrame = field(init=False)\n    parameter_columns: list[str] = field(init=False)\n    parameters: np.ndarray = field(init=False)\n\n    def __post_init__(self):\n        self.preprocess_data = self.preprocess_data.round(self.keep_decimal)\n\n        # Validate that self.preprocess_data have all the required columns\n        self.validate_preprocess_data()\n        logging.info(\"Preprocess data validated\")\n\n        if self._postprocessed_data is not None:\n            self.postprocessed_data = self._postprocessed_data\n            logging.info(\"Postprocess data set\")\n\n        else:\n            post_data = self.transform_preprocess_data_to_postprocess_data()\n            logging.info(\"Preprocess data transformed to postprocess data\")\n\n            self.postprocessed_data = post_data.round(self.keep_decimal)\n\n        # Validate the data with schema\n        self.validate_postprocess_data(self.postprocessed_data)\n        logging.info(\"Postprocess data validated\")\n\n        self.parameter_columns = flatten_parameter_name_with_prefix(\n            self.experiment_config.parameter_names\n        )\n        num_features = len(self.experiment_config.parameter_names[0])\n        num_controls = len(self.experiment_config.parameter_names)\n\n        try:\n            temp_params = np.array(\n                self.postprocessed_data[self.parameter_columns]\n                .to_numpy()\n                .reshape(\n                    (self.experiment_config.sample_size, num_controls, num_features)\n                )\n            )\n        except Exception:\n            logging.info(\n                \"Could not reshape parameters with shape (sample_size, num_controls, num_features), automatically reshaping to (sample_size, -1)\"\n            )\n            temp_params = np.array(\n                self.postprocessed_data[self.parameter_columns]\n                .to_numpy()\n                .reshape((self.experiment_config.sample_size, -1))\n            )\n            logging.info(f\"Parameters reshaped to {temp_params.shape}\")\n\n        self.parameters = temp_params\n\n        logging.info(\"Parameters converted to numpy array\")\n\n        assert (\n            self.preprocess_data[self.parameter_columns]\n            .drop_duplicates(ignore_index=True)\n            .equals(\n                self.postprocessed_data[self.parameter_columns].drop_duplicates(\n                    ignore_index=True\n                )\n            )\n        ), \"The preprocess_data and postprocessed_data does not have the same parameters.\"\n        logging.info(\"Preprocess data and postprocess data have the same parameters\")\n\n    def __eq__(self, __value: object) -&gt; bool:\n        if not isinstance(__value, ExperimentData):\n            return False\n\n        return (\n            self.experiment_config == __value.experiment_config\n            and self.preprocess_data.equals(__value.preprocess_data)\n        )\n\n    def validate_preprocess_data(self):\n        \"\"\"Validate that the preprocess_data have all the required columns.\n\n        Required columns:\n            - EXPECTATION_VALUE\n            - INITIAL_STATE\n            - OBSERVABLE\n            - PARAMETERS_ID\n        \"\"\"\n        for col in REQUIRED_COLUMNS:\n            if col.required:\n                assert (\n                    col.name in self.preprocess_data.columns\n                ), f\"Column {col.name} is required but not found in the preprocess_data.\"\n\n        # Validate that the preprocess_data have all expected parameters columns\n        required_parameters_columns = flatten_parameter_name_with_prefix(\n            self.experiment_config.parameter_names\n        )\n\n        for _col in required_parameters_columns:\n            assert (\n                _col in self.preprocess_data.columns\n            ), f\"Column {_col} is required but not found in the preprocess_data.\"\n\n    def validate_postprocess_data(self, post_data: pd.DataFrame):\n        \"\"\"Validate postprocess dataset, by check the requirements given by `PredefinedCol` instance of each column\n        that required in the postprocessed dataset.\n\n        Args:\n            post_data (pd.DataFrame): Postprocessed dataset to be validated.\n        \"\"\"\n        logging.info(\"Validating postprocess data\")\n        # Validate that the postprocess_data have all the required columns\n        for col in REQUIRED_COLUMNS:\n            if col.required:\n                assert (\n                    col.name in post_data.columns\n                ), f\"Column {col.name} is required but not found in the postprocess_data.\"\n\n        # Validate the check functions\n        for col in REQUIRED_COLUMNS:\n            for check in col.checks:\n                assert all(\n                    [check(v) for v in post_data[col.name]]\n                ), f\"Column {col.name} failed the check function {check}\"\n\n        # Validate that the postprocess_data have all expected parameters columns\n        required_parameters_columns = flatten_parameter_name_with_prefix(\n            self.experiment_config.parameter_names\n        )\n        for _col in required_parameters_columns:\n            assert (\n                _col in post_data.columns\n            ), f\"Column {_col} is required but not found in the postprocess_data.\"\n\n    def transform_preprocess_data_to_postprocess_data(self) -&gt; pd.DataFrame:\n        \"\"\"Internal method to post process the dataset.\n\n        Todo:\n            Use new experimental implementation from_long to wide dataframe\n\n        Raises:\n            ValueError: There is duplicate entry of the expectation value.\n\n        Returns:\n            pd.DataFrame: Postprocessed experiment dataset.\n        \"\"\"\n        # Postprocess the data squeezing the data into the expectation values\n        # Required columns: PARAMETERS_ID, OBSERVABLE, INITIAL_STATE, EXPECTATION_VALUE, + experiment_config.parameter_names\n        post_data = []\n\n        for params_id in range(self.experiment_config.sample_size):\n            # NOTE: Assume that parameters_id starts from 0 and is continuous to sample_size - 1\n            rows = self.preprocess_data.loc[\n                self.preprocess_data[PARAMETERS_ID.name] == params_id\n            ]\n\n            expectation_values = {}\n            for _, exp_order in enumerate(\n                self.experiment_config.expectation_values_order\n            ):\n                expectation_value = rows.loc[\n                    (rows[OBSERVABLE.name] == exp_order.observable)\n                    &amp; (rows[INITIAL_STATE.name] == exp_order.initial_state)\n                ][EXPECTATION_VALUE.name].values\n\n                if expectation_value.shape[0] != 1:\n                    raise ValueError(\n                        f\"Expectation value for params_id {params_id}, initial_state {exp_order.initial_state}, observable {exp_order.observable} is not unique. The length is {len(expectation_value)}.\"\n                    )\n\n                expectation_values[\n                    f\"{EXPECTATION_VALUE.name}/{exp_order.initial_state}/{exp_order.observable}\"\n                ] = expectation_value[0]\n\n            drop_duplicates_row = rows.drop_duplicates(\n                subset=flatten_parameter_name_with_prefix(\n                    self.experiment_config.parameter_names\n                )\n            )\n            # Assert that only one row is returned\n            assert drop_duplicates_row.shape[0] == 1\n            pulse_parameters = drop_duplicates_row.to_dict(orient=\"records\")[0]\n\n            new_row = {\n                PARAMETERS_ID.name: params_id,\n                **expectation_values,\n                **{str(k): v for k, v in pulse_parameters.items()},\n            }\n\n            post_data.append(new_row)\n\n        return pd.DataFrame(post_data)\n\n    def get_parameters_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Get dataframe with only the columns of control parameters.\n\n        Returns:\n            pd.DataFrame: Dataframe with only the columns of control parameters.\n        \"\"\"\n        return self.postprocessed_data[self.parameter_columns]\n\n    def get_expectation_values(self) -&gt; np.ndarray:\n        \"\"\"Get the expectation value of the shape (sample_size, num_expectation_value)\n\n        Returns:\n            np.ndarray: expectation value of the shape (sample_size, num_expectation_value)\n        \"\"\"\n        expectation_value = self.postprocessed_data[\n            [\n                f\"expectation_value/{col.initial_state}/{col.observable}\"\n                for col in self.experiment_config.expectation_values_order\n            ]\n        ].to_numpy()\n\n        return np.array(expectation_value)\n\n    def get_parameters_dict_list(self) -&gt; list[list[ParametersDictType]]:\n        \"\"\"Get the list, where each element is list of dict of the control parameters of the dataset.\n\n        Returns:\n            list[list[ParametersDictType]]: The list of list of dict of parameter.\n        \"\"\"\n        _temp = self.postprocessed_data[self.parameter_columns]\n\n        _params_list = [\n            get_parameters_dict_list(self.experiment_config.parameter_names, row)\n            for _, row in _temp.iterrows()\n        ]\n\n        return _params_list\n\n    def save_to_folder(self, path: typing.Union[Path, str]):\n        \"\"\"Save the experiment data to given folder\n\n        Args:\n            path (typing.Union[Path, str]): Path of the folder for experiment data to be saved.\n        \"\"\"\n        if isinstance(path, str):\n            path = Path(path)\n\n        # os.makedirs(path, exist_ok=True)\n        path.mkdir(parents=True, exist_ok=True)\n        self.experiment_config.to_file(path)\n        self.preprocess_data.to_csv(path / \"preprocess_data.csv\", index=False)\n        self.postprocessed_data.to_csv(path / \"postprocessed_data.csv\", index=False)\n\n    @classmethod\n    def from_folder(cls, path: typing.Union[Path, str]) -&gt; \"ExperimentData\":\n        \"\"\"Read the experiment data from path\n\n        Args:\n            path (typing.Union[Path, str]): path to the folder contain experiment data. Expected to be used with `self.save_to_folder` method.\n\n        Returns:\n            ExperimentData: Intance of `ExperimentData` read from path.\n        \"\"\"\n        if isinstance(path, str):\n            path = Path(path)\n\n        experiment_config = ExperimentConfiguration.from_file(path)\n        preprocess_data = pd.read_csv(\n            path / \"preprocess_data.csv\",\n        )\n\n        # Check if postprocessed_data exists\n        if not (path / \"postprocessed_data.csv\").exists():\n            # if not os.path.exists(path / \"postprocessed_data.csv\"):\n            postprocessed_data = None\n        else:\n            postprocessed_data = pd.read_csv(\n                path / \"postprocessed_data.csv\",\n            )\n\n        return cls(\n            experiment_config=experiment_config,\n            preprocess_data=preprocess_data,\n            _postprocessed_data=postprocessed_data,\n        )\n\n    def analysis_sum_of_expectation_values(self) -&gt; pd.DataFrame:\n        paulis = [\"X\", \"Y\", \"Z\"]\n        initial_states = [(\"0\", \"1\"), (\"+\", \"-\"), (\"r\", \"l\")]\n        data = {}\n        for pauli in paulis:\n            for initial_state in initial_states:\n                _name = f\"{pauli}/{initial_state[0]}/{initial_state[1]}\"\n\n                res = (\n                    self.postprocessed_data[\n                        f\"expectation_value/{initial_state[0]}/{pauli}\"\n                    ]\n                    + self.postprocessed_data[\n                        f\"expectation_value/{initial_state[1]}/{pauli}\"\n                    ]\n                )\n\n                data[_name] = res.to_numpy()\n\n        return pd.DataFrame(data)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.validate_preprocess_data","title":"validate_preprocess_data","text":"<pre><code>validate_preprocess_data()\n</code></pre> <p>Validate that the preprocess_data have all the required columns.</p> Required columns <ul> <li>EXPECTATION_VALUE</li> <li>INITIAL_STATE</li> <li>OBSERVABLE</li> <li>PARAMETERS_ID</li> </ul> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def validate_preprocess_data(self):\n    \"\"\"Validate that the preprocess_data have all the required columns.\n\n    Required columns:\n        - EXPECTATION_VALUE\n        - INITIAL_STATE\n        - OBSERVABLE\n        - PARAMETERS_ID\n    \"\"\"\n    for col in REQUIRED_COLUMNS:\n        if col.required:\n            assert (\n                col.name in self.preprocess_data.columns\n            ), f\"Column {col.name} is required but not found in the preprocess_data.\"\n\n    # Validate that the preprocess_data have all expected parameters columns\n    required_parameters_columns = flatten_parameter_name_with_prefix(\n        self.experiment_config.parameter_names\n    )\n\n    for _col in required_parameters_columns:\n        assert (\n            _col in self.preprocess_data.columns\n        ), f\"Column {_col} is required but not found in the preprocess_data.\"\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.validate_postprocess_data","title":"validate_postprocess_data","text":"<pre><code>validate_postprocess_data(post_data: DataFrame)\n</code></pre> <p>Validate postprocess dataset, by check the requirements given by <code>PredefinedCol</code> instance of each column that required in the postprocessed dataset.</p> <p>Parameters:</p> Name Type Description Default <code>post_data</code> <code>DataFrame</code> <p>Postprocessed dataset to be validated.</p> required Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def validate_postprocess_data(self, post_data: pd.DataFrame):\n    \"\"\"Validate postprocess dataset, by check the requirements given by `PredefinedCol` instance of each column\n    that required in the postprocessed dataset.\n\n    Args:\n        post_data (pd.DataFrame): Postprocessed dataset to be validated.\n    \"\"\"\n    logging.info(\"Validating postprocess data\")\n    # Validate that the postprocess_data have all the required columns\n    for col in REQUIRED_COLUMNS:\n        if col.required:\n            assert (\n                col.name in post_data.columns\n            ), f\"Column {col.name} is required but not found in the postprocess_data.\"\n\n    # Validate the check functions\n    for col in REQUIRED_COLUMNS:\n        for check in col.checks:\n            assert all(\n                [check(v) for v in post_data[col.name]]\n            ), f\"Column {col.name} failed the check function {check}\"\n\n    # Validate that the postprocess_data have all expected parameters columns\n    required_parameters_columns = flatten_parameter_name_with_prefix(\n        self.experiment_config.parameter_names\n    )\n    for _col in required_parameters_columns:\n        assert (\n            _col in post_data.columns\n        ), f\"Column {_col} is required but not found in the postprocess_data.\"\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.transform_preprocess_data_to_postprocess_data","title":"transform_preprocess_data_to_postprocess_data","text":"<pre><code>transform_preprocess_data_to_postprocess_data() -&gt; DataFrame\n</code></pre> <p>Internal method to post process the dataset.</p> Todo <p>Use new experimental implementation from_long to wide dataframe</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>There is duplicate entry of the expectation value.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Postprocessed experiment dataset.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def transform_preprocess_data_to_postprocess_data(self) -&gt; pd.DataFrame:\n    \"\"\"Internal method to post process the dataset.\n\n    Todo:\n        Use new experimental implementation from_long to wide dataframe\n\n    Raises:\n        ValueError: There is duplicate entry of the expectation value.\n\n    Returns:\n        pd.DataFrame: Postprocessed experiment dataset.\n    \"\"\"\n    # Postprocess the data squeezing the data into the expectation values\n    # Required columns: PARAMETERS_ID, OBSERVABLE, INITIAL_STATE, EXPECTATION_VALUE, + experiment_config.parameter_names\n    post_data = []\n\n    for params_id in range(self.experiment_config.sample_size):\n        # NOTE: Assume that parameters_id starts from 0 and is continuous to sample_size - 1\n        rows = self.preprocess_data.loc[\n            self.preprocess_data[PARAMETERS_ID.name] == params_id\n        ]\n\n        expectation_values = {}\n        for _, exp_order in enumerate(\n            self.experiment_config.expectation_values_order\n        ):\n            expectation_value = rows.loc[\n                (rows[OBSERVABLE.name] == exp_order.observable)\n                &amp; (rows[INITIAL_STATE.name] == exp_order.initial_state)\n            ][EXPECTATION_VALUE.name].values\n\n            if expectation_value.shape[0] != 1:\n                raise ValueError(\n                    f\"Expectation value for params_id {params_id}, initial_state {exp_order.initial_state}, observable {exp_order.observable} is not unique. The length is {len(expectation_value)}.\"\n                )\n\n            expectation_values[\n                f\"{EXPECTATION_VALUE.name}/{exp_order.initial_state}/{exp_order.observable}\"\n            ] = expectation_value[0]\n\n        drop_duplicates_row = rows.drop_duplicates(\n            subset=flatten_parameter_name_with_prefix(\n                self.experiment_config.parameter_names\n            )\n        )\n        # Assert that only one row is returned\n        assert drop_duplicates_row.shape[0] == 1\n        pulse_parameters = drop_duplicates_row.to_dict(orient=\"records\")[0]\n\n        new_row = {\n            PARAMETERS_ID.name: params_id,\n            **expectation_values,\n            **{str(k): v for k, v in pulse_parameters.items()},\n        }\n\n        post_data.append(new_row)\n\n    return pd.DataFrame(post_data)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.get_parameters_dataframe","title":"get_parameters_dataframe","text":"<pre><code>get_parameters_dataframe() -&gt; DataFrame\n</code></pre> <p>Get dataframe with only the columns of control parameters.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with only the columns of control parameters.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def get_parameters_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Get dataframe with only the columns of control parameters.\n\n    Returns:\n        pd.DataFrame: Dataframe with only the columns of control parameters.\n    \"\"\"\n    return self.postprocessed_data[self.parameter_columns]\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.get_expectation_values","title":"get_expectation_values","text":"<pre><code>get_expectation_values() -&gt; ndarray\n</code></pre> <p>Get the expectation value of the shape (sample_size, num_expectation_value)</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: expectation value of the shape (sample_size, num_expectation_value)</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def get_expectation_values(self) -&gt; np.ndarray:\n    \"\"\"Get the expectation value of the shape (sample_size, num_expectation_value)\n\n    Returns:\n        np.ndarray: expectation value of the shape (sample_size, num_expectation_value)\n    \"\"\"\n    expectation_value = self.postprocessed_data[\n        [\n            f\"expectation_value/{col.initial_state}/{col.observable}\"\n            for col in self.experiment_config.expectation_values_order\n        ]\n    ].to_numpy()\n\n    return np.array(expectation_value)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.get_parameters_dict_list","title":"get_parameters_dict_list","text":"<pre><code>get_parameters_dict_list() -&gt; list[list[ParametersDictType]]\n</code></pre> <p>Get the list, where each element is list of dict of the control parameters of the dataset.</p> <p>Returns:</p> Type Description <code>list[list[ParametersDictType]]</code> <p>list[list[ParametersDictType]]: The list of list of dict of parameter.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def get_parameters_dict_list(self) -&gt; list[list[ParametersDictType]]:\n    \"\"\"Get the list, where each element is list of dict of the control parameters of the dataset.\n\n    Returns:\n        list[list[ParametersDictType]]: The list of list of dict of parameter.\n    \"\"\"\n    _temp = self.postprocessed_data[self.parameter_columns]\n\n    _params_list = [\n        get_parameters_dict_list(self.experiment_config.parameter_names, row)\n        for _, row in _temp.iterrows()\n    ]\n\n    return _params_list\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.save_to_folder","title":"save_to_folder","text":"<pre><code>save_to_folder(path: Union[Path, str])\n</code></pre> <p>Save the experiment data to given folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Path of the folder for experiment data to be saved.</p> required Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def save_to_folder(self, path: typing.Union[Path, str]):\n    \"\"\"Save the experiment data to given folder\n\n    Args:\n        path (typing.Union[Path, str]): Path of the folder for experiment data to be saved.\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n\n    # os.makedirs(path, exist_ok=True)\n    path.mkdir(parents=True, exist_ok=True)\n    self.experiment_config.to_file(path)\n    self.preprocess_data.to_csv(path / \"preprocess_data.csv\", index=False)\n    self.postprocessed_data.to_csv(path / \"postprocessed_data.csv\", index=False)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.ExperimentData.from_folder","title":"from_folder  <code>classmethod</code>","text":"<pre><code>from_folder(path: Union[Path, str]) -&gt; ExperimentData\n</code></pre> <p>Read the experiment data from path</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>path to the folder contain experiment data. Expected to be used with <code>self.save_to_folder</code> method.</p> required <p>Returns:</p> Name Type Description <code>ExperimentData</code> <code>ExperimentData</code> <p>Intance of <code>ExperimentData</code> read from path.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>@classmethod\ndef from_folder(cls, path: typing.Union[Path, str]) -&gt; \"ExperimentData\":\n    \"\"\"Read the experiment data from path\n\n    Args:\n        path (typing.Union[Path, str]): path to the folder contain experiment data. Expected to be used with `self.save_to_folder` method.\n\n    Returns:\n        ExperimentData: Intance of `ExperimentData` read from path.\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n\n    experiment_config = ExperimentConfiguration.from_file(path)\n    preprocess_data = pd.read_csv(\n        path / \"preprocess_data.csv\",\n    )\n\n    # Check if postprocessed_data exists\n    if not (path / \"postprocessed_data.csv\").exists():\n        # if not os.path.exists(path / \"postprocessed_data.csv\"):\n        postprocessed_data = None\n    else:\n        postprocessed_data = pd.read_csv(\n            path / \"postprocessed_data.csv\",\n        )\n\n    return cls(\n        experiment_config=experiment_config,\n        preprocess_data=preprocess_data,\n        _postprocessed_data=postprocessed_data,\n    )\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.add_hilbert_level","title":"add_hilbert_level","text":"<pre><code>add_hilbert_level(op: ndarray, x: ndarray) -&gt; ndarray\n</code></pre> <p>Add a level to the operator or state</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>ndarray</code> <p>The qubit operator or state</p> required <code>is_state</code> <code>bool</code> <p>True if the operator is a state, False if the operator is an operator</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The qutrit operator or state</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def add_hilbert_level(op: jnp.ndarray, x: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Add a level to the operator or state\n\n    Args:\n        op (jnp.ndarray): The qubit operator or state\n        is_state (bool): True if the operator is a state, False if the operator is an operator\n\n    Returns:\n        jnp.ndarray: The qutrit operator or state\n    \"\"\"\n    return jax.scipy.linalg.block_diag(op, x)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.flatten_parameter_name_with_prefix","title":"flatten_parameter_name_with_prefix","text":"<pre><code>flatten_parameter_name_with_prefix(parameter_names: Sequence[Sequence[str]]) -&gt; list[str]\n</code></pre> <p>Create a flatten list of parameter names with prefix parameter/{i}/</p> <p>Parameters:</p> Name Type Description Default <code>parameter_names</code> <code>Sequence[Sequence[str]]</code> <p>The list of parameter names from the pulse sequence                                                      or the experiment configuration</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The flatten list of parameter names with prefix parameter/{i}/</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def flatten_parameter_name_with_prefix(\n    parameter_names: typing.Sequence[typing.Sequence[str]],\n) -&gt; list[str]:\n    \"\"\"Create a flatten list of parameter names with prefix parameter/{i}/\n\n    Args:\n        parameter_names (typing.Sequence[typing.Sequence[str]]): The list of parameter names from the pulse sequence\n                                                                 or the experiment configuration\n\n    Returns:\n        list[str]: The flatten list of parameter names with prefix parameter/{i}/\n    \"\"\"\n    return [\n        f\"parameter/{i}/{name}\"\n        for i, names in enumerate(parameter_names)\n        for name in names\n    ]\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.transform_parameter_name","title":"transform_parameter_name","text":"<pre><code>transform_parameter_name(name: str) -&gt; str\n</code></pre> <p>Remove \"parameter/{i}/\" from provided name</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the control parameters</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Name that have \"parameter/{i}/\" strip.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def transform_parameter_name(name: str) -&gt; str:\n    \"\"\"Remove \"parameter/{i}/\" from provided name\n\n    Args:\n        name (str): Name of the control parameters\n\n    Returns:\n        str: Name that have \"parameter/{i}/\" strip.\n    \"\"\"\n    if name.startswith(\"parameter/\"):\n        return \"/\".join(name.split(\"/\")[2:])\n    else:\n        return name\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.get_parameters_dict_list","title":"get_parameters_dict_list","text":"<pre><code>get_parameters_dict_list(parameters_name: Sequence[Sequence[str]], parameters_row: Series) -&gt; list[ParametersDictType]\n</code></pre> <p>Get the list of dict containing name and value of each control in the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>parameters_name</code> <code>Sequence[Sequence[str]]</code> <p>description</p> required <code>parameters_row</code> <code>Series</code> <p>description</p> required <p>Returns:</p> Type Description <code>list[ParametersDictType]</code> <p>list[ParametersDictType]: description</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def get_parameters_dict_list(\n    parameters_name: typing.Sequence[typing.Sequence[str]], parameters_row: pd.Series\n) -&gt; list[ParametersDictType]:\n    \"\"\"Get the list of dict containing name and value of each control in the sequence.\n\n    Args:\n        parameters_name (typing.Sequence[typing.Sequence[str]]): _description_\n        parameters_row (pd.Series): _description_\n\n    Returns:\n        list[ParametersDictType]: _description_\n    \"\"\"\n    recovered_parameters: list[ParametersDictType] = [\n        {\n            # Split to remove the parameter/{i}/\n            transform_parameter_name(k): v\n            for k, v in parameters_row.items()\n            # Check if the key is parameter/{i}/ and the value is float\n            if isinstance(k, str)\n            and k.startswith(f\"parameter/{i}/\")\n            and isinstance(v, (float, int))\n        }\n        for i in range(len(parameters_name))\n    ]\n\n    return recovered_parameters\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.save_to_json","title":"save_to_json","text":"<pre><code>save_to_json(data: dict, path: Union[str, Path])\n</code></pre> <p>Save the dictionary as json to the path</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dict to be save to file</p> required <code>path</code> <code>Union[str, Path]</code> <p>Path to save file.</p> required Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def save_to_json(data: dict, path: typing.Union[str, Path]):\n    \"\"\"Save the dictionary as json to the path\n\n    Args:\n        data (dict): Dict to be save to file\n        path (typing.Union[str, Path]): Path to save file.\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n\n    path.parent.mkdir(exist_ok=True)\n    with open(path, \"w\") as f:\n        json.dump(data, f, indent=4)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.read_from_json","title":"read_from_json","text":"<pre><code>read_from_json(path: Union[str, Path], dataclass: Union[None, type[DataclassVar]] = None) -&gt; Union[dict, DataclassVar]\n</code></pre> <p>Construct provided <code>dataclass</code> instance with json file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to json file</p> required <code>dataclass</code> <code>Union[None, type[DataclassVar]]</code> <p>The constructor of the dataclass. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, DataclassVar]</code> <p>typing.Union[dict, DataclassVar]: Dataclass instance, if dataclass is not provideded, return dict instead.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def read_from_json(\n    path: typing.Union[str, Path],\n    dataclass: typing.Union[None, type[DataclassVar]] = None,\n) -&gt; typing.Union[dict, DataclassVar]:\n    \"\"\"Construct provided `dataclass` instance with json file\n\n    Args:\n        path (typing.Union[str, Path]): Path to json file\n        dataclass (typing.Union[None, type[DataclassVar]], optional): The constructor of the dataclass. Defaults to None.\n\n    Returns:\n        typing.Union[dict, DataclassVar]: Dataclass instance, if dataclass is not provideded, return dict instead.\n    \"\"\"\n    if isinstance(path, str):\n        path = Path(path)\n    with open(path, \"r\") as f:\n        config_dict = json.load(f)\n\n    if dataclass is None:\n        return config_dict\n    else:\n        return dataclass(**config_dict)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.load_pytree_from_json","title":"load_pytree_from_json","text":"<pre><code>load_pytree_from_json(path: str | Path, parse_fn=default_parse_fn)\n</code></pre> <p>Load pytree from json</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to JSON file containing pytree</p> required <code>array_keys</code> <code>list[str]</code> <p>list of key to convert to jnp.numpy. Defaults to [].</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Provided path is not point to .json file</p> <p>Returns:</p> Type Description <p>typing.Any: Pytree loaded from JSON</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def load_pytree_from_json(path: str | Path, parse_fn=default_parse_fn):\n    \"\"\"Load pytree from json\n\n    Args:\n        path (str | Path): Path to JSON file containing pytree\n        array_keys (list[str], optional): list of key to convert to jnp.numpy. Defaults to [].\n\n    Raises:\n        ValueError: Provided path is not point to .json file\n\n    Returns:\n        typing.Any: Pytree loaded from JSON\n    \"\"\"\n\n    # Validate that file extension is .json\n    extension = str(path).split(\".\")[-1]\n\n    if extension != \"json\":\n        raise ValueError(\"File extension must be json\")\n\n    if isinstance(path, str):\n        path = Path(path)\n\n    with open(path, \"r\") as f:\n        data = json.load(f)\n\n    data = recursive_parse(data, parse_fn=parse_fn)\n\n    return data\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.save_pytree_to_json","title":"save_pytree_to_json","text":"<pre><code>save_pytree_to_json(pytree, path: str | Path)\n</code></pre> <p>Save given pytree to json file, the path must end with extension of .json</p> <p>Parameters:</p> Name Type Description Default <code>pytree</code> <code>Any</code> <p>The pytree to save</p> required <code>path</code> <code>str | Path</code> <p>File path to save</p> required Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def save_pytree_to_json(pytree, path: str | Path):\n    \"\"\"Save given pytree to json file, the path must end with extension of .json\n\n    Args:\n        pytree (typing.Any): The pytree to save\n        path (str | Path): File path to save\n\n    \"\"\"\n\n    # Convert jax.ndarray\n    data = jax.tree.map(\n        lambda x: x.tolist() if isinstance(x, jnp.ndarray) else x, pytree\n    )\n    # Convert ParamShape\n    data = jax.tree.map(param_shape_to_dict, data, is_leaf=is_param_shape)\n\n    if isinstance(path, str):\n        path = Path(path)\n\n    path.parent.mkdir(exist_ok=True, parents=True)\n\n    with open(path, \"w\") as f:\n        json.dump(data, f, indent=4)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.from_long_to_wide","title":"from_long_to_wide","text":"<pre><code>from_long_to_wide(preprocessed_df: DataFrame)\n</code></pre> <p>An experimental function to transform preprocess dataframe to postprocess dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>preprocessed_df</code> <code>DataFrame</code> <p>The preprocess dataframe</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: The postprocessed dataframe</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def from_long_to_wide(preprocessed_df: pd.DataFrame):\n    \"\"\"An experimental function to transform preprocess dataframe to postprocess dataframe.\n\n    Args:\n        preprocessed_df (pd.DataFrame): The preprocess dataframe\n\n    Returns:\n        pd.DataFrame: The postprocessed dataframe\n    \"\"\"\n    # Handle the expectation value using unstack\n    expvals_df = preprocessed_df.pivot(\n        index=\"parameters_id\",\n        columns=[\"initial_state\", \"observable\"],\n        values=\"expectation_value\",  # Note: string, not a list\n    )\n\n    # Rename columns using another idiomatic approach\n    expvals_df.columns = [\n        f\"expectation_value/{state}/{obs}\" for state, obs in expvals_df.columns\n    ]\n\n    # Handle parameters columns\n    params_df = (\n        preprocessed_df.groupby(\"parameters_id\")\n        .first()\n        .drop(\n            [\"expectation_value\", \"initial_state\", \"observable\"], axis=1, inplace=False\n        )\n    )\n\n    # Combine with join\n    return params_df.join(expvals_df)\n</code></pre>"},{"location":"api/data/#src.inspeqtor.experimental.data.from_wide_to_long_simple","title":"from_wide_to_long_simple","text":"<pre><code>from_wide_to_long_simple(wide_df: DataFrame)\n</code></pre> <p>A more concise version to convert a wide DataFrame back to the long format.</p> Source code in <code>src/inspeqtor/experimental/data.py</code> <pre><code>def from_wide_to_long_simple(wide_df: pd.DataFrame):\n    \"\"\"\n    A more concise version to convert a wide DataFrame back to the long format.\n    \"\"\"\n    # Work with the index as a column\n    df = wide_df.reset_index()\n\n    # 1. Identify all columns that should NOT be melted (the \"id\" columns)\n    id_vars = [col for col in df.columns if not col.startswith(\"expectation_value/\")]\n\n    # 2. Melt the DataFrame. pd.melt automatically uses all columns NOT in id_vars as value_vars.\n    long_df = df.melt(\n        id_vars=id_vars, var_name=\"descriptor\", value_name=\"expectation_value\"\n    )\n\n    # 3. Split the descriptor and assign new columns in one step\n    long_df[[\"_,\", \"initial_state\", \"observable\"]] = long_df[\"descriptor\"].str.split(\n        \"/\", expand=True\n    )\n\n    # 4. Clean up the DataFrame by dropping temporary columns and sorting\n    return (\n        long_df.drop(columns=[\"descriptor\", \"_,\"])\n        .sort_values(\"parameters_id\")\n        .reset_index(drop=True)\n    )\n</code></pre>"},{"location":"api/linen/","title":"linen","text":""},{"location":"api/linen/#src.inspeqtor.experimental.models.linen","title":"src.inspeqtor.experimental.models.linen","text":""},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.loss_fn","title":"loss_fn","text":"<pre><code>loss_fn(params: VariableDict, control_parameters: ndarray, unitaries: ndarray, expectation_values: ndarray, model: Module, predictive_fn: Callable, loss_metric: LossMetric, calculate_metric_fn: Callable = calculate_metric, **model_kwargs) -&gt; tuple[ndarray, dict[str, ndarray]]\n</code></pre> <p>This function implement a unified interface for nn.Module.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>VariableDict</code> <p>Model parameters to be optimized</p> required <code>control_parameters</code> <code>ndarray</code> <p>Control parameters parametrized Hamiltonian</p> required <code>unitaries</code> <code>ndarray</code> <p>The Ideal unitary operators corresponding to the control parameters</p> required <code>expectation_values</code> <code>ndarray</code> <p>Experimental expectation values to calculate the loss value</p> required <code>model</code> <code>Module</code> <p>Flax linen Blackbox part of the graybox model.</p> required <code>predictive_fn</code> <code>Callable</code> <p>Function for calculating expectation value from the model</p> required <code>loss_metric</code> <code>LossMetric</code> <p>The choice of loss value to be minimized.</p> required <code>calculate_metric_fn</code> <code>Callable</code> <p>Function for metrics calculation from prediction and experimental value. Defaults to calculate_metric</p> <code>calculate_metric</code> <p>Returns:</p> Type Description <code>tuple[ndarray, dict[str, ndarray]]</code> <p>tuple[jnp.ndarray, dict[str, jnp.ndarray]]: The loss value and other metrics.</p> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>@deprecated.deprecated(reason=\"use make_loss_fn instead\")\ndef loss_fn(\n    params: VariableDict,\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    expectation_values: jnp.ndarray,\n    model: nn.Module,\n    predictive_fn: typing.Callable,\n    loss_metric: LossMetric,\n    calculate_metric_fn: typing.Callable = calculate_metric,\n    **model_kwargs,\n) -&gt; tuple[jnp.ndarray, dict[str, jnp.ndarray]]:\n    \"\"\"This function implement a unified interface for nn.Module.\n\n    Args:\n        params (VariableDict): Model parameters to be optimized\n        control_parameters (jnp.ndarray): Control parameters parametrized Hamiltonian\n        unitaries (jnp.ndarray): The Ideal unitary operators corresponding to the control parameters\n        expectation_values (jnp.ndarray): Experimental expectation values to calculate the loss value\n        model (nn.Module): Flax linen Blackbox part of the graybox model.\n        predictive_fn (typing.Callable): Function for calculating expectation value from the model\n        loss_metric (LossMetric): The choice of loss value to be minimized.\n        calculate_metric_fn (typing.Callable): Function for metrics calculation from prediction and experimental value. Defaults to calculate_metric\n\n    Returns:\n        tuple[jnp.ndarray, dict[str, jnp.ndarray]]: The loss value and other metrics.\n    \"\"\"\n    # Calculate the metrics\n    predicted_expectation_value = predictive_fn(\n        model=model,\n        model_params=params,\n        control_parameters=control_parameters,\n        unitaries=unitaries,\n        **model_kwargs,\n    )\n\n    metrics = calculate_metric_fn(\n        unitaries, expectation_values, predicted_expectation_value\n    )\n\n    # Take mean of all the metrics\n    metrics = jax.tree.map(jnp.mean, metrics)\n\n    # ! Grab the metric in the `metrics`\n    loss = metrics[loss_metric]\n\n    return (loss, metrics)\n</code></pre>"},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.wo_predictive_fn","title":"wo_predictive_fn","text":"<pre><code>wo_predictive_fn(control_parameters: ndarray, unitaries: ndarray, model: Module, model_params: VariableDict, **model_kwargs)\n</code></pre> <p>To Calculate the metrics of the model 1. MSE Loss between the predicted expectation values and the experimental expectation values 2. Average Gate Fidelity between the Pauli matrices to the Wo_model matrices 3. AGF Loss between the prediction from model and the experimental expectation values</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be used for prediction</p> required <code>model_params</code> <code>VariableDict</code> <p>The model parameters</p> required <code>control_parameters</code> <code>ndarray</code> <p>The pulse parameters</p> required <code>unitaries</code> <code>ndarray</code> <p>Ideal unitaries</p> required <code>expectation_values</code> <code>ndarray</code> <p>Experimental expectation values</p> required <code>model_kwargs</code> <code>dict</code> <p>Model keyword arguments</p> <code>{}</code> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>def wo_predictive_fn(\n    # Input data to the model\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    # The model to be used for prediction\n    model: nn.Module,\n    model_params: VariableDict,\n    # model keyword arguments\n    **model_kwargs,\n):\n    \"\"\"To Calculate the metrics of the model\n    1. MSE Loss between the predicted expectation values and the experimental expectation values\n    2. Average Gate Fidelity between the Pauli matrices to the Wo_model matrices\n    3. AGF Loss between the prediction from model and the experimental expectation values\n\n    Args:\n        model (sq.model.nn.Module): The model to be used for prediction\n        model_params (sq.model.VariableDict): The model parameters\n        control_parameters (jnp.ndarray): The pulse parameters\n        unitaries (jnp.ndarray): Ideal unitaries\n        expectation_values (jnp.ndarray): Experimental expectation values\n        model_kwargs (dict): Model keyword arguments\n    \"\"\"\n\n    # Calculate Wo_params\n    Wo = model.apply(model_params, control_parameters, **model_kwargs)\n\n    return observable_to_expvals(Wo, unitaries)\n</code></pre>"},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.noisy_unitary_predictive_fn","title":"noisy_unitary_predictive_fn","text":"<pre><code>noisy_unitary_predictive_fn(control_parameters: ndarray, unitaries: ndarray, model: UnitaryModel, model_params: VariableDict, **model_kwargs)\n</code></pre> <p>Caculate for unitary-based Blackbox model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be used for prediction</p> required <code>model_params</code> <code>VariableDict</code> <p>The model parameters</p> required <code>control_parameters</code> <code>ndarray</code> <p>The pulse parameters</p> required <code>unitaries</code> <code>ndarray</code> <p>Ideal unitaries</p> required <code>expectation_values</code> <code>ndarray</code> <p>Experimental expectation values</p> required <code>model_kwargs</code> <code>dict</code> <p>Model keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>def noisy_unitary_predictive_fn(\n    # Input data to the model\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    # The model to be used for prediction\n    model: UnitaryModel,\n    model_params: VariableDict,\n    # model keyword arguments\n    **model_kwargs,\n):\n    \"\"\"Caculate for unitary-based Blackbox model\n\n    Args:\n        model (sq.model.nn.Module): The model to be used for prediction\n        model_params (sq.model.VariableDict): The model parameters\n        control_parameters (jnp.ndarray): The pulse parameters\n        unitaries (jnp.ndarray): Ideal unitaries\n        expectation_values (jnp.ndarray): Experimental expectation values\n        model_kwargs (dict): Model keyword arguments\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n\n    # Predict Unitary parameters\n    unitary_params = model.apply(model_params, control_parameters, **model_kwargs)\n\n    return unitary_to_expvals(unitary_params, unitaries)\n</code></pre>"},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.toggling_unitary_predictive_fn","title":"toggling_unitary_predictive_fn","text":"<pre><code>toggling_unitary_predictive_fn(control_parameters: ndarray, unitaries: ndarray, model: UnitaryModel, model_params: VariableDict, ignore_spam: bool = False, **model_kwargs)\n</code></pre> <p>Calcuate for unitary-based Blackbox model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be used for prediction</p> required <code>model_params</code> <code>VariableDict</code> <p>The model parameters</p> required <code>control_parameters</code> <code>ndarray</code> <p>The pulse parameters</p> required <code>unitaries</code> <code>ndarray</code> <p>Ideal unitaries</p> required <code>expectation_values</code> <code>ndarray</code> <p>Experimental expectation values</p> required <code>model_kwargs</code> <code>dict</code> <p>Model keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>def toggling_unitary_predictive_fn(\n    # Input data to the model\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    # The model to be used for prediction\n    model: UnitaryModel,\n    model_params: VariableDict,\n    # model keyword arguments\n    ignore_spam: bool = False,\n    **model_kwargs,\n):\n    \"\"\"Calcuate for unitary-based Blackbox model\n\n    Args:\n        model (sq.model.nn.Module): The model to be used for prediction\n        model_params (sq.model.VariableDict): The model parameters\n        control_parameters (jnp.ndarray): The pulse parameters\n        unitaries (jnp.ndarray): Ideal unitaries\n        expectation_values (jnp.ndarray): Experimental expectation values\n        model_kwargs (dict): Model keyword arguments\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n\n    # Predict Unitary parameters\n    unitary_params = model.apply(model_params, control_parameters, **model_kwargs)\n\n    if not ignore_spam:\n        return toggling_unitary_with_spam_to_expvals(\n            output={\n                \"model_params\": unitary_params,\n                \"spam_params\": model_params[\"spam\"],\n            },\n            unitaries=unitaries,\n        )\n    else:\n        return toggling_unitary_to_expvals(\n            unitary_params,  # type: ignore\n            unitaries=unitaries,\n        )\n</code></pre>"},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.make_loss_fn_old","title":"make_loss_fn_old","text":"<pre><code>make_loss_fn_old(predictive_fn: Callable, model: Module, calculate_metric_fn: Callable = calculate_metric, loss_metric: LossMetric = MSEE)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>predictive_fn</code> <code>Callable</code> <p>Function for calculating expectation value from the model</p> required <code>model</code> <code>Module</code> <p>Flax linen Blackbox part of the graybox model.</p> required <code>loss_metric</code> <code>LossMetric</code> <p>The choice of loss value to be minimized. Defaults to LossMetric.MSEE.</p> <code>MSEE</code> <code>calculate_metric_fn</code> <code>Callable</code> <p>Function for metrics calculation from prediction and experimental value. Defaults to calculate_metric.</p> <code>calculate_metric</code> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>@deprecated.deprecated\ndef make_loss_fn_old(\n    predictive_fn: typing.Callable,\n    model: nn.Module,\n    calculate_metric_fn: typing.Callable = calculate_metric,\n    loss_metric: LossMetric = LossMetric.MSEE,\n):\n    \"\"\"_summary_\n\n    Args:\n        predictive_fn (typing.Callable): Function for calculating expectation value from the model\n        model (nn.Module): Flax linen Blackbox part of the graybox model.\n        loss_metric (LossMetric): The choice of loss value to be minimized. Defaults to LossMetric.MSEE.\n        calculate_metric_fn (typing.Callable): Function for metrics calculation from prediction and experimental value. Defaults to calculate_metric.\n    \"\"\"\n\n    def loss_fn(\n        params: VariableDict,\n        control_parameters: jnp.ndarray,\n        unitaries: jnp.ndarray,\n        expectation_values: jnp.ndarray,\n        **model_kwargs,\n    ) -&gt; tuple[jnp.ndarray, dict[str, jnp.ndarray]]:\n        \"\"\"This function implement a unified interface for nn.Module.\n\n        Args:\n            params (VariableDict): Model parameters to be optimized\n            control_parameters (jnp.ndarray): Control parameters parametrized Hamiltonian\n            unitaries (jnp.ndarray): The Ideal unitary operators corresponding to the control parameters\n            expectation_values (jnp.ndarray): Experimental expectation values to calculate the loss value\n\n        Returns:\n            tuple[jnp.ndarray, dict[str, jnp.ndarray]]: The loss value and other metrics.\n        \"\"\"\n        # Calculate the metrics\n        predicted_expectation_value = predictive_fn(\n            model=model,\n            model_params=params,\n            control_parameters=control_parameters,\n            unitaries=unitaries,\n            **model_kwargs,\n        )\n\n        metrics = calculate_metric_fn(\n            unitaries, expectation_values, predicted_expectation_value\n        )\n\n        # Take mean of all the metrics\n        metrics = jax.tree.map(jnp.mean, metrics)\n\n        # ! Grab the metric in the `metrics`\n        loss = metrics[loss_metric]\n\n        return (loss, metrics)\n\n    return loss_fn\n</code></pre>"},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.make_loss_fn","title":"make_loss_fn","text":"<pre><code>make_loss_fn(adapter_fn: Callable, model: Module, calculate_metric_fn: Callable = calculate_metric, loss_metric: LossMetric = MSEE)\n</code></pre> <p>summary</p> <p>Parameters:</p> Name Type Description Default <code>predictive_fn</code> <code>Callable</code> <p>Function for calculating expectation value from the model</p> required <code>model</code> <code>Module</code> <p>Flax linen Blackbox part of the graybox model.</p> required <code>loss_metric</code> <code>LossMetric</code> <p>The choice of loss value to be minimized. Defaults to LossMetric.MSEE.</p> <code>MSEE</code> <code>calculate_metric_fn</code> <code>Callable</code> <p>Function for metrics calculation from prediction and experimental value. Defaults to calculate_metric.</p> <code>calculate_metric</code> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>def make_loss_fn(\n    adapter_fn: typing.Callable,\n    model: nn.Module,\n    calculate_metric_fn: typing.Callable = calculate_metric,\n    loss_metric: LossMetric = LossMetric.MSEE,\n):\n    \"\"\"_summary_\n\n    Args:\n        predictive_fn (typing.Callable): Function for calculating expectation value from the model\n        model (nn.Module): Flax linen Blackbox part of the graybox model.\n        loss_metric (LossMetric): The choice of loss value to be minimized. Defaults to LossMetric.MSEE.\n        calculate_metric_fn (typing.Callable): Function for metrics calculation from prediction and experimental value. Defaults to calculate_metric.\n    \"\"\"\n\n    def loss_fn(\n        params: VariableDict,\n        control_parameters: jnp.ndarray,\n        unitaries: jnp.ndarray,\n        expectation_values: jnp.ndarray,\n        **model_kwargs,\n    ) -&gt; tuple[jnp.ndarray, dict[str, jnp.ndarray]]:\n        \"\"\"This function implement a unified interface for nn.Module.\n\n        Args:\n            params (VariableDict): Model parameters to be optimized\n            control_parameters (jnp.ndarray): Control parameters parametrized Hamiltonian\n            unitaries (jnp.ndarray): The Ideal unitary operators corresponding to the control parameters\n            expectation_values (jnp.ndarray): Experimental expectation values to calculate the loss value\n\n        Returns:\n            tuple[jnp.ndarray, dict[str, jnp.ndarray]]: The loss value and other metrics.\n        \"\"\"\n        output = model.apply(params, control_parameters, **model_kwargs)\n        predicted_expectation_value = adapter_fn(output, unitaries=unitaries)\n\n        # Calculate the metrics\n        metrics = calculate_metric_fn(\n            unitaries, expectation_values, predicted_expectation_value\n        )\n\n        # Take mean of all the metrics\n        metrics = jax.tree.map(jnp.mean, metrics)\n\n        # ! Grab the metric in the `metrics`\n        loss = metrics[loss_metric]\n\n        return (loss, metrics)\n\n    return loss_fn\n</code></pre>"},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.create_step","title":"create_step","text":"<pre><code>create_step(optimizer: GradientTransformation, loss_fn: Callable[..., ndarray] | Callable[..., Tuple[ndarray, Any]], has_aux: bool = False)\n</code></pre> <p>The create_step function creates a training step function and a test step function.</p> <p>loss_fn should have the following signature: <pre><code>def loss_fn(params: jaxtyping.PyTree, *args) -&gt; jnp.ndarray:\n    ...\n    return loss_value\n</code></pre> where <code>params</code> is the parameters to be optimized, and <code>args</code> are the inputs for the loss function.</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <code>GradientTransformation</code> <p><code>optax</code> optimizer.</p> required <code>loss_fn</code> <code>Callable[[PyTree, ...], ndarray]</code> <p>Loss function, which takes in the model parameters, inputs, and targets, and returns the loss value.</p> required <code>has_aux</code> <code>bool</code> <p>Whether the loss function return aux data or not. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>typing.Any: train_step, test_step</p> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>def create_step(\n    optimizer: optax.GradientTransformation,\n    loss_fn: (\n        typing.Callable[..., jnp.ndarray]\n        | typing.Callable[..., typing.Tuple[jnp.ndarray, typing.Any]]\n    ),\n    has_aux: bool = False,\n):\n    \"\"\"The create_step function creates a training step function and a test step function.\n\n    loss_fn should have the following signature:\n    ```py\n    def loss_fn(params: jaxtyping.PyTree, *args) -&gt; jnp.ndarray:\n        ...\n        return loss_value\n    ```\n    where `params` is the parameters to be optimized, and `args` are the inputs for the loss function.\n\n    Args:\n        optimizer (optax.GradientTransformation): `optax` optimizer.\n        loss_fn (typing.Callable[[jaxtyping.PyTree, ...], jnp.ndarray]): Loss function, which takes in the model parameters, inputs, and targets, and returns the loss value.\n        has_aux (bool, optional): Whether the loss function return aux data or not. Defaults to False.\n\n    Returns:\n        _typing.Any_: train_step, test_step\n    \"\"\"\n\n    # * Generalized training step\n    @jax.jit\n    def train_step(\n        params: jaxtyping.PyTree,\n        optimizer_state: optax.OptState,\n        *args,\n        **kwargs,\n    ):\n        loss_value, grads = jax.value_and_grad(loss_fn, has_aux=has_aux)(\n            params, *args, **kwargs\n        )\n        updates, opt_state = optimizer.update(grads, optimizer_state, params)\n        params = optax.apply_updates(params, updates)\n\n        return params, opt_state, loss_value\n\n    @jax.jit\n    def test_step(\n        params: jaxtyping.PyTree,\n        *args,\n        **kwargs,\n    ):\n        return loss_fn(params, *args, **kwargs)\n\n    return train_step, test_step\n</code></pre>"},{"location":"api/linen/#src.inspeqtor.experimental.models.linen.train_model","title":"train_model","text":"<pre><code>train_model(key: ndarray, train_data: DataBundled, val_data: DataBundled, test_data: DataBundled, model: Module, optimizer: GradientTransformation, loss_fn: Callable, callbacks: list[Callable] = [], NUM_EPOCH: int = 1000, model_params: VariableDict | None = None, opt_state: OptState | None = None)\n</code></pre> <p>Train the BlackBox model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # The number of epochs break down\n... NUM_EPOCH = 150\n... # Total number of iterations as 90% of data is used for training\n... # 10% of the data is used for testing\n... total_iterations = 9 * NUM_EPOCH\n... # The step for optimizer if set to 8 * NUM_EPOCH (should be less than total_iterations)\n... step_for_optimizer = 8 * NUM_EPOCH\n... optimizer = get_default_optimizer(step_for_optimizer)\n... # The warmup steps for the optimizer\n... warmup_steps = 0.1 * step_for_optimizer\n... # The cool down steps for the optimizer\n... cool_down_steps = total_iterations - step_for_optimizer\n... total_iterations, step_for_optimizer, warmup_steps, cool_down_steps\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>Random key</p> required <code>model</code> <code>Module</code> <p>The model to be used for training</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>The optimizer to be used for training</p> required <code>loss_fn</code> <code>Callable</code> <p>The loss function to be used for training</p> required <code>callbacks</code> <code>list[Callable]</code> <p>list of callback functions. Defaults to [].</p> <code>[]</code> <code>NUM_EPOCH</code> <code>int</code> <p>The number of epochs. Defaults to 1_000.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>The model parameters, optimizer state, and the histories</p> Source code in <code>src/inspeqtor/experimental/models/linen.py</code> <pre><code>def train_model(\n    # Random key\n    key: jnp.ndarray,\n    # Data\n    train_data: DataBundled,\n    val_data: DataBundled,\n    test_data: DataBundled,\n    # Model to be used for training\n    model: nn.Module,\n    optimizer: optax.GradientTransformation,\n    # Loss function to be used\n    loss_fn: typing.Callable,\n    # Callbacks to be used\n    callbacks: list[typing.Callable] = [],\n    # Number of epochs\n    NUM_EPOCH: int = 1_000,\n    # Optional state\n    model_params: VariableDict | None = None,\n    opt_state: optax.OptState | None = None,\n):\n    \"\"\"Train the BlackBox model\n\n    Examples:\n        &gt;&gt;&gt; # The number of epochs break down\n        ... NUM_EPOCH = 150\n        ... # Total number of iterations as 90% of data is used for training\n        ... # 10% of the data is used for testing\n        ... total_iterations = 9 * NUM_EPOCH\n        ... # The step for optimizer if set to 8 * NUM_EPOCH (should be less than total_iterations)\n        ... step_for_optimizer = 8 * NUM_EPOCH\n        ... optimizer = get_default_optimizer(step_for_optimizer)\n        ... # The warmup steps for the optimizer\n        ... warmup_steps = 0.1 * step_for_optimizer\n        ... # The cool down steps for the optimizer\n        ... cool_down_steps = total_iterations - step_for_optimizer\n        ... total_iterations, step_for_optimizer, warmup_steps, cool_down_steps\n\n    Args:\n        key (jnp.ndarray): Random key\n        model (nn.Module): The model to be used for training\n        optimizer (optax.GradientTransformation): The optimizer to be used for training\n        loss_fn (typing.Callable): The loss function to be used for training\n        callbacks (list[typing.Callable], optional): list of callback functions. Defaults to [].\n        NUM_EPOCH (int, optional): The number of epochs. Defaults to 1_000.\n\n    Returns:\n        tuple: The model parameters, optimizer state, and the histories\n    \"\"\"\n\n    key, loader_key, init_key = jax.random.split(key, 3)\n\n    train_p, train_u, train_ex = (\n        train_data.control_params,\n        train_data.unitaries,\n        train_data.observables,\n    )\n    val_p, val_u, val_ex = (\n        val_data.control_params,\n        val_data.unitaries,\n        val_data.observables,\n    )\n    test_p, test_u, test_ex = (\n        test_data.control_params,\n        test_data.unitaries,\n        test_data.observables,\n    )\n\n    BATCH_SIZE = val_p.shape[0]\n\n    if model_params is None:\n        # Initialize the model parameters if it is None\n        model_params = model.init(init_key, train_p[0])\n\n    if opt_state is None:\n        # Initalize the optimizer state if it is None\n        opt_state = optimizer.init(model_params)\n\n    # histories: list[dict[str, typing.Any]] = []\n    histories: list[HistoryEntryV3] = []\n\n    train_step, eval_step = create_step(\n        optimizer=optimizer, loss_fn=loss_fn, has_aux=True\n    )\n\n    for (step, batch_idx, is_last_batch, epoch_idx), (\n        batch_p,\n        batch_u,\n        batch_ex,\n    ) in dataloader(\n        (train_p, train_u, train_ex),\n        batch_size=BATCH_SIZE,\n        num_epochs=NUM_EPOCH,\n        key=loader_key,\n    ):\n        model_params, opt_state, (loss, aux) = train_step(\n            model_params, opt_state, batch_p, batch_u, batch_ex\n        )\n\n        histories.append(HistoryEntryV3(step=step, loss=loss, loop=\"train\", aux=aux))\n\n        if is_last_batch:\n            # Validation\n            (val_loss, aux) = eval_step(model_params, val_p, val_u, val_ex)\n\n            histories.append(\n                HistoryEntryV3(step=step, loss=val_loss, loop=\"val\", aux=aux)\n            )\n\n            # Testing\n            (test_loss, aux) = eval_step(model_params, test_p, test_u, test_ex)\n\n            histories.append(\n                HistoryEntryV3(step=step, loss=test_loss, loop=\"test\", aux=aux)\n            )\n\n            for callback in callbacks:\n                callback(model_params, opt_state, histories)\n\n    return model_params, opt_state, histories\n</code></pre>"},{"location":"api/nnx/","title":"nnx","text":""},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx","title":"src.inspeqtor.experimental.models.nnx","text":""},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.Blackbox","title":"Blackbox","text":"<p>The abstract class for interfacing the Blackbox model of the Graybox</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>class Blackbox(nnx.Module):\n    \"\"\"The abstract class for interfacing the Blackbox model of the Graybox\"\"\"\n\n    def __init__(self, *, rngs: nnx.Rngs) -&gt; None:\n        super().__init__()\n\n    def __call__(self, *args: typing.Any, **kwds: typing.Any) -&gt; typing.Any:\n        raise NotImplementedError()\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.WoModel","title":"WoModel","text":"<p>\\(\\hat{W}_{O}\\) based blackbox model.</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>class WoModel(Blackbox):\n    \"\"\"$\\\\hat{W}_{O}$ based blackbox model.\"\"\"\n\n    def __init__(\n        self, shared_layers: list[int], pauli_layers: list[int], *, rngs: nnx.Rngs\n    ):\n        \"\"\"\n        Args:\n            shared_layers (list[int]): Each integer in the list is a size of the width of each hidden layer in the shared layers.\n            pauli_layers (list[int]): Each integer in the list is a size of the width of each hidden layer in the Pauli layers.\n            rngs (nnx.Rngs): Random number generator of `nnx`.\n        \"\"\"\n\n        self.shared_layers = {\n            f\"shared/{idx}\": nnx.Linear(\n                in_features=in_features, out_features=out_features, rngs=rngs\n            )\n            for idx, (in_features, out_features) in enumerate(\n                zip(shared_layers[:-1], shared_layers[1:])\n            )\n        }\n\n        self.num_shared_layers = len(shared_layers) - 1\n        self.num_pauli_layers = len(pauli_layers) - 1\n\n        self.pauli_layers = {}\n        self.unitary_layers = {}\n        self.diagonal_layers = {}\n        for pauli in [\"X\", \"Y\", \"Z\"]:\n            layers = {\n                f\"pauli/{idx}\": nnx.Linear(\n                    in_features=in_features, out_features=out_features, rngs=rngs\n                )\n                for idx, (in_features, out_features) in enumerate(\n                    zip(pauli_layers[:-1], pauli_layers[1:])\n                )\n            }\n\n            self.pauli_layers[pauli] = layers\n\n            self.unitary_layers[pauli] = nnx.Linear(\n                in_features=pauli_layers[-1], out_features=3, rngs=rngs\n            )\n            self.diagonal_layers[pauli] = nnx.Linear(\n                in_features=pauli_layers[-1], out_features=2, rngs=rngs\n            )\n\n    def __call__(self, x: jnp.ndarray):\n        for idx in range(self.num_shared_layers):\n            layer = self.shared_layers[f\"shared/{idx}\"]\n            x = nnx.relu(layer(x))\n\n        observables: dict[str, jnp.ndarray] = dict()\n        for pauli, pauli_layer in self.pauli_layers.items():\n            _x = jnp.copy(x)\n            for idx in range(self.num_pauli_layers):\n                layer = pauli_layer[f\"pauli/{idx}\"]\n                _x = nnx.relu(layer(_x))\n\n            unitary_param = self.unitary_layers[pauli](_x)\n            diagonal_param = self.diagonal_layers[pauli](_x)\n\n            unitary_param = 2 * jnp.pi * nnx.hard_sigmoid(unitary_param)\n            diagonal_param = (2 * nnx.hard_sigmoid(diagonal_param)) - 1\n\n            observables[pauli] = hermitian(unitary_param, diagonal_param)\n\n        return observables\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.WoModel.__init__","title":"__init__","text":"<pre><code>__init__(shared_layers: list[int], pauli_layers: list[int], *, rngs: Rngs)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>shared_layers</code> <code>list[int]</code> <p>Each integer in the list is a size of the width of each hidden layer in the shared layers.</p> required <code>pauli_layers</code> <code>list[int]</code> <p>Each integer in the list is a size of the width of each hidden layer in the Pauli layers.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator of <code>nnx</code>.</p> required Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def __init__(\n    self, shared_layers: list[int], pauli_layers: list[int], *, rngs: nnx.Rngs\n):\n    \"\"\"\n    Args:\n        shared_layers (list[int]): Each integer in the list is a size of the width of each hidden layer in the shared layers.\n        pauli_layers (list[int]): Each integer in the list is a size of the width of each hidden layer in the Pauli layers.\n        rngs (nnx.Rngs): Random number generator of `nnx`.\n    \"\"\"\n\n    self.shared_layers = {\n        f\"shared/{idx}\": nnx.Linear(\n            in_features=in_features, out_features=out_features, rngs=rngs\n        )\n        for idx, (in_features, out_features) in enumerate(\n            zip(shared_layers[:-1], shared_layers[1:])\n        )\n    }\n\n    self.num_shared_layers = len(shared_layers) - 1\n    self.num_pauli_layers = len(pauli_layers) - 1\n\n    self.pauli_layers = {}\n    self.unitary_layers = {}\n    self.diagonal_layers = {}\n    for pauli in [\"X\", \"Y\", \"Z\"]:\n        layers = {\n            f\"pauli/{idx}\": nnx.Linear(\n                in_features=in_features, out_features=out_features, rngs=rngs\n            )\n            for idx, (in_features, out_features) in enumerate(\n                zip(pauli_layers[:-1], pauli_layers[1:])\n            )\n        }\n\n        self.pauli_layers[pauli] = layers\n\n        self.unitary_layers[pauli] = nnx.Linear(\n            in_features=pauli_layers[-1], out_features=3, rngs=rngs\n        )\n        self.diagonal_layers[pauli] = nnx.Linear(\n            in_features=pauli_layers[-1], out_features=2, rngs=rngs\n        )\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.UnitaryModel","title":"UnitaryModel","text":"<p>Unitary-based model, predicting parameters parametrized unitary operator in range \\([0, 2\\pi]\\).</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>class UnitaryModel(Blackbox):\n    \"\"\"Unitary-based model, predicting parameters parametrized unitary operator in range $[0, 2\\\\pi]$.\"\"\"\n\n    def __init__(self, hidden_sizes: list[int], *, rngs: nnx.Rngs) -&gt; None:\n        \"\"\"\n\n        Args:\n            hidden_sizes (list[int]): Each integer in the list is a size of the width of each hidden layer in the shared layers\n            rngs (nnx.Rngs): Random number generator of `nnx`.\n        \"\"\"\n        self.hidden_sizes = hidden_sizes\n        self.NUM_UNITARY_PARAMS = 4\n\n        self.hidden_layers = {\n            f\"hidden_layers/{idx}\": nnx.Linear(\n                in_features=hidden_size, out_features=hidden_size, rngs=rngs\n            )\n            for idx, hidden_size in enumerate(self.hidden_sizes)\n        }\n\n        # Initialize the final layer for unitary parameters\n        self.final_layer = nnx.Linear(\n            in_features=self.hidden_sizes[-1],\n            out_features=self.NUM_UNITARY_PARAMS,\n            rngs=rngs,\n        )\n\n    def __call__(self, x: jnp.ndarray) -&gt; jnp.ndarray:\n        # Apply the hidden layers with ReLU activation\n        for _, layer in self.hidden_layers.items():\n            x = nnx.relu(layer(x))\n\n        # Apply the final layer and transform the output\n        x = self.final_layer(x)\n        x = 2 * jnp.pi * nnx.hard_sigmoid(x)\n\n        return x\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.UnitaryModel.__init__","title":"__init__","text":"<pre><code>__init__(hidden_sizes: list[int], *, rngs: Rngs) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>hidden_sizes</code> <code>list[int]</code> <p>Each integer in the list is a size of the width of each hidden layer in the shared layers</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator of <code>nnx</code>.</p> required Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def __init__(self, hidden_sizes: list[int], *, rngs: nnx.Rngs) -&gt; None:\n    \"\"\"\n\n    Args:\n        hidden_sizes (list[int]): Each integer in the list is a size of the width of each hidden layer in the shared layers\n        rngs (nnx.Rngs): Random number generator of `nnx`.\n    \"\"\"\n    self.hidden_sizes = hidden_sizes\n    self.NUM_UNITARY_PARAMS = 4\n\n    self.hidden_layers = {\n        f\"hidden_layers/{idx}\": nnx.Linear(\n            in_features=hidden_size, out_features=hidden_size, rngs=rngs\n        )\n        for idx, hidden_size in enumerate(self.hidden_sizes)\n    }\n\n    # Initialize the final layer for unitary parameters\n    self.final_layer = nnx.Linear(\n        in_features=self.hidden_sizes[-1],\n        out_features=self.NUM_UNITARY_PARAMS,\n        rngs=rngs,\n    )\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.UnitarySPAMModel","title":"UnitarySPAMModel","text":"<p>Composite class of unitary-based model and the SPAM model.</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>class UnitarySPAMModel(Blackbox):\n    \"\"\"Composite class of unitary-based model and the SPAM model.\"\"\"\n\n    def __init__(\n        self, unitary_model: UnitaryModel, spam_params, *, rngs: nnx.Rngs\n    ) -&gt; None:\n        \"\"\"\n\n        Args:\n            unitary_model (UnitaryModel): Unitary-based model that have already initialized.\n            rngs (nnx.Rngs): Random number generator of `nnx`.\n        \"\"\"\n        self.unitary_model = unitary_model\n        self.spam_params = jax.tree.map(nnx.Param, spam_params)\n\n    def __call__(self, x: jnp.ndarray) -&gt; dict:\n        return {\"model_params\": self.unitary_model(x), \"spam_params\": self.spam_params}\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.UnitarySPAMModel.__init__","title":"__init__","text":"<pre><code>__init__(unitary_model: UnitaryModel, spam_params, *, rngs: Rngs) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>unitary_model</code> <code>UnitaryModel</code> <p>Unitary-based model that have already initialized.</p> required <code>rngs</code> <code>Rngs</code> <p>Random number generator of <code>nnx</code>.</p> required Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def __init__(\n    self, unitary_model: UnitaryModel, spam_params, *, rngs: nnx.Rngs\n) -&gt; None:\n    \"\"\"\n\n    Args:\n        unitary_model (UnitaryModel): Unitary-based model that have already initialized.\n        rngs (nnx.Rngs): Random number generator of `nnx`.\n    \"\"\"\n    self.unitary_model = unitary_model\n    self.spam_params = jax.tree.map(nnx.Param, spam_params)\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.wo_predictive_fn","title":"wo_predictive_fn","text":"<pre><code>wo_predictive_fn(control_parameters: ndarray, unitaries: ndarray, model: WoModel) -&gt; ndarray\n</code></pre> <p>Adapter function for \\(\\hat{W}_{O}\\) based model to be used with <code>make_loss_fn</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WoModel</code> <p>\\(\\hat{W}_{O}\\) based model</p> required <code>data</code> <code>DataBundled</code> <p>A bundled of data for the predictive model training.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Predicted expectation values.</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def wo_predictive_fn(\n    # Input data to the model\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    model: WoModel,\n) -&gt; jnp.ndarray:\n    \"\"\"Adapter function for $\\\\hat{W}_{O}$ based model to be used with `make_loss_fn`.\n\n    Args:\n        model (WoModel): $\\\\hat{W}_{O}$ based model\n        data (DataBundled): A bundled of data for the predictive model training.\n\n    Returns:\n        jnp.ndarray: Predicted expectation values.\n    \"\"\"\n    output = model(control_parameters)\n    return observable_to_expvals(output, unitaries)\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.noisy_unitary_predictive_fn","title":"noisy_unitary_predictive_fn","text":"<pre><code>noisy_unitary_predictive_fn(control_parameters: ndarray, unitaries: ndarray, model: UnitaryModel) -&gt; ndarray\n</code></pre> <p>Adapter function for unitary-based model to be used with <code>make_loss_fn</code></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UnitaryModel</code> <p>Unitary-based model.</p> required <code>data</code> <code>DataBundled</code> <p>A bundled of data for the predictive model training.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Predicted expectation values.</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def noisy_unitary_predictive_fn(\n    # Input data to the model\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    model: UnitaryModel,\n) -&gt; jnp.ndarray:\n    \"\"\"Adapter function for unitary-based model to be used with `make_loss_fn`\n\n    Args:\n        model (UnitaryModel): Unitary-based model.\n        data (DataBundled): A bundled of data for the predictive model training.\n\n    Returns:\n        jnp.ndarray: Predicted expectation values.\n    \"\"\"\n    unitary_params = model(control_parameters)\n\n    return unitary_to_expvals(unitary_params, unitaries)\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.toggling_unitary_predictive_fn","title":"toggling_unitary_predictive_fn","text":"<pre><code>toggling_unitary_predictive_fn(control_parameters: ndarray, unitaries: ndarray, model: UnitaryModel) -&gt; ndarray\n</code></pre> <p>Adapter function for rotating toggling frame unitary based model to be used with <code>make_loss_fn</code></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UnitaryModel</code> <p>Unitary-based model.</p> required <code>data</code> <code>DataBundled</code> <p>A bundled of data for the predictive model training.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Predicted expectation values.</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def toggling_unitary_predictive_fn(\n    # Input data to the model\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    model: UnitaryModel,\n) -&gt; jnp.ndarray:\n    \"\"\"Adapter function for rotating toggling frame unitary based model to be used with `make_loss_fn`\n\n    Args:\n        model (UnitaryModel): Unitary-based model.\n        data (DataBundled): A bundled of data for the predictive model training.\n\n    Returns:\n        jnp.ndarray: Predicted expectation values.\n    \"\"\"\n    unitary_params = model(control_parameters)\n\n    return toggling_unitary_to_expvals(unitary_params, unitaries)\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.toggling_unitary_with_spam_predictive_fn","title":"toggling_unitary_with_spam_predictive_fn","text":"<pre><code>toggling_unitary_with_spam_predictive_fn(control_parameters: ndarray, unitaries: ndarray, model: UnitarySPAMModel) -&gt; ndarray\n</code></pre> <p>Adapter function for a composite rotating toggling frame unitary based model to be used with <code>make_loss_fn</code></p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>UnitaryModel</code> <p>Unitary-based SPAM model.</p> required <code>data</code> <code>DataBundled</code> <p>A bundled of data for the predictive model training.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Predicted expectation values.</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def toggling_unitary_with_spam_predictive_fn(\n    # Input data to the model\n    control_parameters: jnp.ndarray,\n    unitaries: jnp.ndarray,\n    model: UnitarySPAMModel,\n) -&gt; jnp.ndarray:\n    \"\"\"Adapter function for a composite rotating toggling\n    frame unitary based model to be used with `make_loss_fn`\n\n    Args:\n        model (UnitaryModel): Unitary-based SPAM model.\n        data (DataBundled): A bundled of data for the predictive model training.\n\n    Returns:\n        jnp.ndarray: Predicted expectation values.\n    \"\"\"\n    unitary_params = model(control_parameters)\n\n    return toggling_unitary_with_spam_to_expvals(\n        {\n            \"model_params\": unitary_params,\n            \"spam_params\": model.spam_params,\n        },\n        unitaries,\n    )\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.make_loss_fn_old","title":"make_loss_fn_old","text":"<pre><code>make_loss_fn_old(predictive_fn, calculate_metric_fn=calculate_metric, loss_metric: LossMetric = MSEE)\n</code></pre> <p>A function for preparing loss function to be used for model training.</p> <p>Parameters:</p> Name Type Description Default <code>predictive_fn</code> <code>Any</code> <p>Adaptor function specifically for each model.</p> required <code>calculate_metric_fn</code> <code>Any</code> <p>Function for calculating metrics. Defaults to calculate_metric.</p> <code>calculate_metric</code> <code>loss_metric</code> <code>LossMetric</code> <p>The chosen loss function to be optimized. Defaults to LossMetric.MSEE.</p> <code>MSEE</code> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>@deprecated.deprecated\ndef make_loss_fn_old(\n    predictive_fn,\n    calculate_metric_fn=calculate_metric,\n    loss_metric: LossMetric = LossMetric.MSEE,\n):\n    \"\"\"A function for preparing loss function to be used for model training.\n\n    Args:\n        predictive_fn (typing.Any): Adaptor function specifically for each model.\n        calculate_metric_fn (typing.Any, optional): Function for calculating metrics. Defaults to calculate_metric.\n        loss_metric (LossMetric, optional): The chosen loss function to be optimized. Defaults to LossMetric.MSEE.\n    \"\"\"\n\n    def loss_fn(model: Blackbox, data: DataBundled):\n        expval = predictive_fn(data.control_params, data.unitaries, model)\n\n        metrics = calculate_metric_fn(data.unitaries, data.observables, expval)\n        # Take mean of all the metrics\n        metrics = jax.tree.map(jnp.mean, metrics)\n        loss = metrics[loss_metric]\n\n        return loss, metrics\n\n    return loss_fn\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.make_loss_fn","title":"make_loss_fn","text":"<pre><code>make_loss_fn(adapter_fn, calculate_metric_fn=calculate_metric, loss_metric: LossMetric = MSEE)\n</code></pre> <p>A function for preparing loss function to be used for model training.</p> <p>Parameters:</p> Name Type Description Default <code>predictive_fn</code> <code>Any</code> <p>Adaptor function specifically for each model.</p> required <code>calculate_metric_fn</code> <code>Any</code> <p>Function for calculating metrics. Defaults to calculate_metric.</p> <code>calculate_metric</code> <code>loss_metric</code> <code>LossMetric</code> <p>The chosen loss function to be optimized. Defaults to LossMetric.MSEE.</p> <code>MSEE</code> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def make_loss_fn(\n    adapter_fn,\n    calculate_metric_fn=calculate_metric,\n    loss_metric: LossMetric = LossMetric.MSEE,\n):\n    \"\"\"A function for preparing loss function to be used for model training.\n\n    Args:\n        predictive_fn (typing.Any): Adaptor function specifically for each model.\n        calculate_metric_fn (typing.Any, optional): Function for calculating metrics. Defaults to calculate_metric.\n        loss_metric (LossMetric, optional): The chosen loss function to be optimized. Defaults to LossMetric.MSEE.\n    \"\"\"\n\n    def loss_fn(model: Blackbox, data: DataBundled):\n        output = model(data.control_params)\n\n        expval = adapter_fn(output, data.unitaries)\n\n        metrics = calculate_metric_fn(data.unitaries, data.observables, expval)\n        # Take mean of all the metrics\n        metrics = jax.tree.map(jnp.mean, metrics)\n        loss = metrics[loss_metric]\n\n        return loss, metrics\n\n    return loss_fn\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.create_step","title":"create_step","text":"<pre><code>create_step(loss_fn: Callable[[Blackbox, DataBundled], tuple[ndarray, Any]])\n</code></pre> <p>A function to create the traning and evaluating step for model. The train step will update the model parameters and optimizer parameters inplace.</p> <p>Parameters:</p> Name Type Description Default <code>loss_fn</code> <code>Callable[[Blackbox, DataBundled], tuple[ndarray, Any]]</code> <p>Loss function returned from <code>make_loss_fn</code></p> required <p>Returns:</p> Type Description <p>typing.Any: The tuple of training and eval step functions.</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def create_step(\n    loss_fn: typing.Callable[[Blackbox, DataBundled], tuple[jnp.ndarray, typing.Any]],\n):\n    \"\"\"A function to create the traning and evaluating step for model.\n    The train step will update the model parameters and optimizer parameters inplace.\n\n    Args:\n        loss_fn (typing.Callable[[Blackbox, DataBundled], tuple[jnp.ndarray, typing.Any]]): Loss function returned from `make_loss_fn`\n\n    Returns:\n        typing.Any: The tuple of training and eval step functions.\n    \"\"\"\n\n    @nnx.jit\n    def train_step(\n        model: Blackbox,\n        optimizer: nnx.Optimizer,\n        metrics: nnx.MultiMetric,\n        data: DataBundled,\n    ):\n        \"\"\"Train for a single step.\"\"\"\n        model.train()  # Switch to train mode\n        grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n        (loss, aux), grads = grad_fn(model, data)\n        metrics.update(loss=loss)  # In-place updates.\n        optimizer.update(grads)  # In-place updates.\n\n        return loss, aux\n\n    @nnx.jit\n    def eval_step(model: Blackbox, metrics: nnx.MultiMetric, data):\n        model.eval()\n        loss, aux = loss_fn(model, data)\n        metrics.update(loss=loss)  # In-place updates.\n\n        return loss, aux\n\n    return train_step, eval_step\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.reconstruct_model","title":"reconstruct_model","text":"<pre><code>reconstruct_model(model_params, config, Model: type[T]) -&gt; T\n</code></pre> <p>Reconstruct the model from the model parameters, config, and model initializer.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; _, state = nnx.split(blackbox)\n&gt;&gt;&gt; model_params = nnx.to_pure_dict(state)\n&gt;&gt;&gt; config = {\n...    \"shared_layers\": [8],\n...    \"pauli_layers\": [8]\n... }\n&gt;&gt;&gt; model_data = sq.model.ModelData(params=model_params, config=config)\n# save and load to and from disk!\n&gt;&gt;&gt; blackbox = sq.models.nnx.reconstruct_model(model_data.params, model_data.config, sq.models.nnx.WoModel)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_params</code> <code>Any</code> <p>The pytree containing model parameters.</p> required <code>config</code> <code>Any</code> <p>The model configuration for model initialization.</p> required <code>Model</code> <code>type[T]</code> <p>The model initializer.</p> required <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>description</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def reconstruct_model(model_params, config, Model: type[T]) -&gt; T:\n    \"\"\"Reconstruct the model from the model parameters, config, and model initializer.\n\n    Examples:\n        &gt;&gt;&gt; _, state = nnx.split(blackbox)\n        &gt;&gt;&gt; model_params = nnx.to_pure_dict(state)\n        &gt;&gt;&gt; config = {\n        ...    \"shared_layers\": [8],\n        ...    \"pauli_layers\": [8]\n        ... }\n        &gt;&gt;&gt; model_data = sq.model.ModelData(params=model_params, config=config)\n        # save and load to and from disk!\n        &gt;&gt;&gt; blackbox = sq.models.nnx.reconstruct_model(model_data.params, model_data.config, sq.models.nnx.WoModel)\n\n    Args:\n        model_params (typing.Any): The pytree containing model parameters.\n        config (typing.Any): The model configuration for model initialization.\n        Model (type[T]): The model initializer.\n\n    Returns:\n        T: _description_\n    \"\"\"\n    abstract_model = nnx.eval_shape(lambda: Model(**config, rngs=nnx.Rngs(0)))\n    graphdef, abstract_state = nnx.split(abstract_model)\n    nnx.replace_by_pure_dict(abstract_state, model_params)\n\n    return nnx.merge(graphdef, abstract_state)\n</code></pre>"},{"location":"api/nnx/#src.inspeqtor.experimental.models.nnx.train_model","title":"train_model","text":"<pre><code>train_model(key: ndarray, train_data: DataBundled, val_data: DataBundled, test_data: DataBundled, model: Blackbox, optimizer: GradientTransformation, loss_fn: Callable, callbacks: list[Callable] = [], NUM_EPOCH: int = 1000, _optimizer: Optimizer | None = None)\n</code></pre> <p>Train the BlackBox model</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # The number of epochs break down\n... NUM_EPOCH = 150\n... # Total number of iterations as 90% of data is used for training\n... # 10% of the data is used for testing\n... total_iterations = 9 * NUM_EPOCH\n... # The step for optimizer if set to 8 * NUM_EPOCH (should be less than total_iterations)\n... step_for_optimizer = 8 * NUM_EPOCH\n... optimizer = get_default_optimizer(step_for_optimizer)\n... # The warmup steps for the optimizer\n... warmup_steps = 0.1 * step_for_optimizer\n... # The cool down steps for the optimizer\n... cool_down_steps = total_iterations - step_for_optimizer\n... total_iterations, step_for_optimizer, warmup_steps, cool_down_steps\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>Random key</p> required <code>model</code> <code>Module</code> <p>The model to be used for training</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>The optimizer to be used for training</p> required <code>loss_fn</code> <code>Callable</code> <p>The loss function to be used for training</p> required <code>callbacks</code> <code>list[Callable]</code> <p>list of callback functions. Defaults to [].</p> <code>[]</code> <code>NUM_EPOCH</code> <code>int</code> <p>The number of epochs. Defaults to 1_000.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>The model parameters, optimizer state, and the histories</p> Source code in <code>src/inspeqtor/experimental/models/nnx.py</code> <pre><code>def train_model(\n    # Random key\n    key: jnp.ndarray,\n    # Data\n    train_data: DataBundled,\n    val_data: DataBundled,\n    test_data: DataBundled,\n    # Model to be used for training\n    model: Blackbox,\n    optimizer: optax.GradientTransformation,\n    # Loss function to be used\n    loss_fn: typing.Callable,\n    # Callbacks to be used\n    callbacks: list[typing.Callable] = [],\n    # Number of epochs\n    NUM_EPOCH: int = 1_000,\n    _optimizer: nnx.Optimizer | None = None,\n):\n    \"\"\"Train the BlackBox model\n\n    Examples:\n        &gt;&gt;&gt; # The number of epochs break down\n        ... NUM_EPOCH = 150\n        ... # Total number of iterations as 90% of data is used for training\n        ... # 10% of the data is used for testing\n        ... total_iterations = 9 * NUM_EPOCH\n        ... # The step for optimizer if set to 8 * NUM_EPOCH (should be less than total_iterations)\n        ... step_for_optimizer = 8 * NUM_EPOCH\n        ... optimizer = get_default_optimizer(step_for_optimizer)\n        ... # The warmup steps for the optimizer\n        ... warmup_steps = 0.1 * step_for_optimizer\n        ... # The cool down steps for the optimizer\n        ... cool_down_steps = total_iterations - step_for_optimizer\n        ... total_iterations, step_for_optimizer, warmup_steps, cool_down_steps\n\n    Args:\n        key (jnp.ndarray): Random key\n        model (nn.Module): The model to be used for training\n        optimizer (optax.GradientTransformation): The optimizer to be used for training\n        loss_fn (typing.Callable): The loss function to be used for training\n        callbacks (list[typing.Callable], optional): list of callback functions. Defaults to [].\n        NUM_EPOCH (int, optional): The number of epochs. Defaults to 1_000.\n\n    Returns:\n        tuple: The model parameters, optimizer state, and the histories\n    \"\"\"\n\n    key, loader_key = jax.random.split(key)\n\n    BATCH_SIZE = val_data.control_params.shape[0]\n\n    histories: list[HistoryEntryV3] = []\n\n    if _optimizer is None:\n        _optimizer = nnx.Optimizer(\n            model,\n            optimizer,\n            wrt=nnx.Param,\n        )\n\n    metrics = nnx.MultiMetric(\n        loss=nnx.metrics.Average(\"loss\"),\n    )\n\n    train_step, eval_step = create_step(loss_fn=loss_fn)\n\n    for (step, batch_idx, is_last_batch, epoch_idx), (\n        batch_p,\n        batch_u,\n        batch_ex,\n    ) in dataloader(\n        (\n            train_data.control_params,\n            train_data.unitaries,\n            train_data.observables,\n        ),\n        batch_size=BATCH_SIZE,\n        num_epochs=NUM_EPOCH,\n        key=loader_key,\n    ):\n        train_step(\n            model,\n            _optimizer,\n            metrics,\n            DataBundled(\n                control_params=batch_p, unitaries=batch_u, observables=batch_ex\n            ),\n        )\n\n        histories.append(\n            HistoryEntryV3(\n                step=step, loss=metrics.compute()[\"loss\"], loop=\"train\", aux={}\n            )\n        )\n        metrics.reset()  # Reset the metrics for the train set.\n\n        if is_last_batch:\n            # Validation\n            eval_step(model, metrics, val_data)\n            histories.append(\n                HistoryEntryV3(\n                    step=step, loss=metrics.compute()[\"loss\"], loop=\"val\", aux={}\n                )\n            )\n            metrics.reset()  # Reset the metrics for the val set.\n            # Testing\n            eval_step(model, metrics, test_data)\n            histories.append(\n                HistoryEntryV3(\n                    step=step, loss=metrics.compute()[\"loss\"], loop=\"test\", aux={}\n                )\n            )\n            metrics.reset()  # Reset the metrics for the test set.\n\n            for callback in callbacks:\n                callback(model, _optimizer, histories)\n\n    return model, _optimizer, histories\n</code></pre>"},{"location":"api/optimize/","title":"optimize","text":""},{"location":"api/optimize/#src.inspeqtor.experimental.optimize","title":"src.inspeqtor.experimental.optimize","text":""},{"location":"api/optimize/#src.inspeqtor.experimental.optimize.get_default_optimizer","title":"get_default_optimizer","text":"<pre><code>get_default_optimizer(n_iterations: int) -&gt; GradientTransformation\n</code></pre> <p>Generate present optimizer from number of training iteration.</p> <p>Parameters:</p> Name Type Description Default <code>n_iterations</code> <code>int</code> <p>Training iteration</p> required <p>Returns:</p> Type Description <code>GradientTransformation</code> <p>optax.GradientTransformation: Optax optimizer.</p> Source code in <code>src/inspeqtor/experimental/optimize.py</code> <pre><code>def get_default_optimizer(n_iterations: int) -&gt; optax.GradientTransformation:\n    \"\"\"Generate present optimizer from number of training iteration.\n\n    Args:\n        n_iterations (int): Training iteration\n\n    Returns:\n        optax.GradientTransformation: Optax optimizer.\n    \"\"\"\n    return optax.adamw(\n        learning_rate=optax.warmup_cosine_decay_schedule(\n            init_value=1e-6,\n            peak_value=1e-2,\n            warmup_steps=int(0.1 * n_iterations),\n            decay_steps=n_iterations,\n            end_value=1e-6,\n        )\n    )\n</code></pre>"},{"location":"api/optimize/#src.inspeqtor.experimental.optimize.minimize","title":"minimize","text":"<pre><code>minimize(params: ArrayTree, func: Callable[[ndarray], tuple[ndarray, Any]], optimizer: GradientTransformation, lower: ArrayTree | None = None, upper: ArrayTree | None = None, maxiter: int = 1000, callbacks: list[Callable] = []) -&gt; tuple[ArrayTree, list[Any]]\n</code></pre> <p>Optimize the loss function with bounded parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ArrayTree</code> <p>Intiial parameters to be optimized</p> required <code>lower</code> <code>ArrayTree</code> <p>Lower bound of the parameters</p> <code>None</code> <code>upper</code> <code>ArrayTree</code> <p>Upper bound of the parameters</p> <code>None</code> <code>func</code> <code>Callable[[ndarray], tuple[ndarray, Any]]</code> <p>Loss function</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Instance of optax optimizer</p> required <code>maxiter</code> <code>int</code> <p>Number of optimization step. Defaults to 1000.</p> <code>1000</code> <p>Returns:</p> Type Description <code>tuple[ArrayTree, list[Any]]</code> <p>tuple[chex.ArrayTree, list[typing.Any]]: Tuple of parameters and optimization history</p> Source code in <code>src/inspeqtor/experimental/optimize.py</code> <pre><code>def minimize(\n    params: chex.ArrayTree,\n    func: typing.Callable[[jnp.ndarray], tuple[jnp.ndarray, typing.Any]],\n    optimizer: optax.GradientTransformation,\n    lower: chex.ArrayTree | None = None,\n    upper: chex.ArrayTree | None = None,\n    maxiter: int = 1000,\n    callbacks: list[typing.Callable] = [],\n) -&gt; tuple[chex.ArrayTree, list[typing.Any]]:\n    \"\"\"Optimize the loss function with bounded parameters.\n\n    Args:\n        params (chex.ArrayTree): Intiial parameters to be optimized\n        lower (chex.ArrayTree): Lower bound of the parameters\n        upper (chex.ArrayTree): Upper bound of the parameters\n        func (typing.Callable[[jnp.ndarray], tuple[jnp.ndarray, typing.Any]]): Loss function\n        optimizer (optax.GradientTransformation): Instance of optax optimizer\n        maxiter (int, optional): Number of optimization step. Defaults to 1000.\n\n    Returns:\n        tuple[chex.ArrayTree, list[typing.Any]]: Tuple of parameters and optimization history\n    \"\"\"\n    opt_state = optimizer.init(params)\n    history = []\n\n    for step_idx in range(maxiter):\n        grads, aux = jax.grad(func, has_aux=True)(params)\n        updates, opt_state = optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n\n        if lower is not None and upper is not None:\n            # Apply projection\n            params = optax.projections.projection_box(params, lower, upper)\n\n        # Log the history\n        aux[\"params\"] = params\n        history.append(aux)\n\n        for callback in callbacks:\n            callback(step_idx, aux)\n\n    return params, history\n</code></pre>"},{"location":"api/optimize/#src.inspeqtor.experimental.optimize.stochastic_minimize","title":"stochastic_minimize","text":"<pre><code>stochastic_minimize(key: ndarray, params: ArrayTree, func: Callable[[ndarray, ndarray], tuple[ndarray, Any]], optimizer: GradientTransformation, lower: ArrayTree | None = None, upper: ArrayTree | None = None, maxiter: int = 1000, callbacks: list[Callable] = []) -&gt; tuple[ArrayTree, list[Any]]\n</code></pre> <p>Optimize the loss function with bounded parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>ArrayTree</code> <p>Intiial parameters to be optimized</p> required <code>lower</code> <code>ArrayTree</code> <p>Lower bound of the parameters</p> <code>None</code> <code>upper</code> <code>ArrayTree</code> <p>Upper bound of the parameters</p> <code>None</code> <code>func</code> <code>Callable[[ndarray], tuple[ndarray, Any]]</code> <p>Loss function</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Instance of optax optimizer</p> required <code>maxiter</code> <code>int</code> <p>Number of optimization step. Defaults to 1000.</p> <code>1000</code> <p>Returns:</p> Type Description <code>tuple[ArrayTree, list[Any]]</code> <p>tuple[chex.ArrayTree, list[typing.Any]]: Tuple of parameters and optimization history</p> Source code in <code>src/inspeqtor/experimental/optimize.py</code> <pre><code>def stochastic_minimize(\n    key: jnp.ndarray,\n    params: chex.ArrayTree,\n    func: typing.Callable[[jnp.ndarray, jnp.ndarray], tuple[jnp.ndarray, typing.Any]],\n    optimizer: optax.GradientTransformation,\n    lower: chex.ArrayTree | None = None,\n    upper: chex.ArrayTree | None = None,\n    maxiter: int = 1000,\n    callbacks: list[typing.Callable] = [],\n) -&gt; tuple[chex.ArrayTree, list[typing.Any]]:\n    \"\"\"Optimize the loss function with bounded parameters.\n\n    Args:\n        params (chex.ArrayTree): Intiial parameters to be optimized\n        lower (chex.ArrayTree): Lower bound of the parameters\n        upper (chex.ArrayTree): Upper bound of the parameters\n        func (typing.Callable[[jnp.ndarray], tuple[jnp.ndarray, typing.Any]]): Loss function\n        optimizer (optax.GradientTransformation): Instance of optax optimizer\n        maxiter (int, optional): Number of optimization step. Defaults to 1000.\n\n    Returns:\n        tuple[chex.ArrayTree, list[typing.Any]]: Tuple of parameters and optimization history\n    \"\"\"\n    opt_state = optimizer.init(params)\n    history = []\n\n    for step_idx in range(maxiter):\n        key, _ = jax.random.split(key)\n        grads, aux = jax.grad(func, has_aux=True)(params, key)\n        updates, opt_state = optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n\n        if lower is not None and upper is not None:\n            # Apply projection\n            params = optax.projections.projection_box(params, lower, upper)\n\n        # Log the history\n        aux[\"params\"] = params\n        history.append(aux)\n\n        for callback in callbacks:\n            callback(step_idx, aux)\n\n    return params, history\n</code></pre>"},{"location":"api/physics/","title":"physics","text":""},{"location":"api/physics/#src.inspeqtor.experimental.physics","title":"src.inspeqtor.experimental.physics","text":""},{"location":"api/physics/#src.inspeqtor.experimental.physics.normalizer","title":"normalizer","text":"<pre><code>normalizer(matrix: ndarray) -&gt; ndarray\n</code></pre> <p>Normalize the given matrix with QR decomposition and return matrix Q    which is unitary</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>The matrix to normalize to unitary matrix</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The unitary matrix</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def normalizer(matrix: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Normalize the given matrix with QR decomposition and return matrix Q\n       which is unitary\n\n    Args:\n        matrix (jnp.ndarray): The matrix to normalize to unitary matrix\n\n    Returns:\n        jnp.ndarray: The unitary matrix\n    \"\"\"\n    return jnp.linalg.qr(matrix).Q  # type: ignore\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.solver","title":"solver","text":"<pre><code>solver(args: HamiltonianArgs, t_eval: ndarray, hamiltonian: Callable[[HamiltonianArgs, ndarray], ndarray], y0: ndarray, t0: float, t1: float, rtol: float = 1e-07, atol: float = 1e-07, max_steps: int = int(2 ** 16)) -&gt; ndarray\n</code></pre> <p>Solve the Schrodinger equation using the given Hamiltonian</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>HamiltonianArgs</code> <p>The arguments for the Hamiltonian</p> required <code>t_eval</code> <code>ndarray</code> <p>The time points to evaluate the solution</p> required <code>hamiltonian</code> <code>Callable[[HamiltonianArgs, ndarray], ndarray]</code> <p>The Hamiltonian function</p> required <code>y0</code> <code>ndarray</code> <p>The initial state, set to jnp.eye(2, dtype=jnp.complex128) for unitary matrix</p> required <code>t0</code> <code>float</code> <p>The initial time</p> required <code>t1</code> <code>float</code> <p>The final time</p> required <code>rtol</code> <code>float</code> <p>description. Defaults to 1e-7.</p> <code>1e-07</code> <code>atol</code> <code>float</code> <p>description. Defaults to 1e-7.</p> <code>1e-07</code> <code>max_steps</code> <code>int</code> <p>The maxmimum step of evalution of solver. Defaults to int(2**16).</p> <code>int(2 ** 16)</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The solution of the Schrodinger equation at the given time points</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def solver(\n    args: HamiltonianArgs,\n    t_eval: jnp.ndarray,\n    hamiltonian: typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray],\n    y0: jnp.ndarray,\n    t0: float,\n    t1: float,\n    rtol: float = 1e-7,\n    atol: float = 1e-7,\n    max_steps: int = int(2**16),\n) -&gt; jnp.ndarray:\n    \"\"\"Solve the Schrodinger equation using the given Hamiltonian\n\n    Args:\n        args (HamiltonianArgs): The arguments for the Hamiltonian\n        t_eval (jnp.ndarray): The time points to evaluate the solution\n        hamiltonian (typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray]): The Hamiltonian function\n        y0 (jnp.ndarray): The initial state, set to jnp.eye(2, dtype=jnp.complex128) for unitary matrix\n        t0 (float): The initial time\n        t1 (float): The final time\n        rtol (float, optional): _description_. Defaults to 1e-7.\n        atol (float, optional): _description_. Defaults to 1e-7.\n        max_steps (int, optional): The maxmimum step of evalution of solver. Defaults to int(2**16).\n\n    Returns:\n        jnp.ndarray: The solution of the Schrodinger equation at the given time points\n    \"\"\"\n\n    # * Increase time_step to increase accuracy of solver,\n    # *     then you have to increase the max_steps too.\n    # * Using just a basic solver\n    def rhs(t: jnp.ndarray, y: jnp.ndarray, args: HamiltonianArgs):\n        return -1j * hamiltonian(args, t) @ y\n\n    term = diffrax.ODETerm(rhs)  # type: ignore\n    solver = diffrax.Tsit5()\n\n    solution = diffrax.diffeqsolve(\n        term,\n        solver,\n        t0=t0,\n        t1=t1,\n        dt0=None,\n        stepsize_controller=diffrax.PIDController(\n            rtol=rtol,\n            atol=atol,\n        ),\n        y0=y0,\n        args=args,\n        saveat=diffrax.SaveAt(ts=t_eval),\n        max_steps=max_steps,\n    )\n\n    # Normailized the solution\n    ys = solution.ys\n    # assert isinstance(ys, jnp.ndarray)\n\n    return jax.vmap(normalizer)(ys)  # type: ignore\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.auto_rotating_frame_hamiltonian","title":"auto_rotating_frame_hamiltonian","text":"<pre><code>auto_rotating_frame_hamiltonian(hamiltonian: Callable[[HamiltonianArgs, ndarray], ndarray], frame: ndarray, explicit_deriv: bool = False)\n</code></pre> <p>Implement the Hamiltonian in the rotating frame with H_I = U(t) @ H @ U^dagger(t) + i * U(t) @ dU^dagger(t)/dt</p> <p>Parameters:</p> Name Type Description Default <code>hamiltonian</code> <code>Callable</code> <p>The hamiltonian function</p> required <code>frame</code> <code>ndarray</code> <p>The frame matrix</p> required Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def auto_rotating_frame_hamiltonian(\n    hamiltonian: typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray],\n    frame: jnp.ndarray,\n    explicit_deriv: bool = False,\n):\n    \"\"\"Implement the Hamiltonian in the rotating frame with\n    H_I = U(t) @ H @ U^dagger(t) + i * U(t) @ dU^dagger(t)/dt\n\n    Args:\n        hamiltonian (Callable): The hamiltonian function\n        frame (jnp.ndarray): The frame matrix\n    \"\"\"\n\n    is_diagonal = False\n    # Check if the frame is diagonal matrix\n    if jnp.count_nonzero(frame - jnp.diag(jnp.diagonal(frame))) == 0:\n        is_diagonal = True\n\n    # Check if the jax_enable_x64 is True\n    if not jax.config.read(\"jax_enable_x64\") or is_diagonal:\n\n        def frame_unitary(t: jnp.ndarray) -&gt; jnp.ndarray:\n            # NOTE: This is the same as the below, as we sure that the frame is diagonal\n            return jnp.diag(jnp.exp(1j * jnp.diagonal(frame) * t))\n\n    else:\n\n        def frame_unitary(t: jnp.ndarray) -&gt; jnp.ndarray:\n            return jax.scipy.linalg.expm(1j * frame * t)\n\n    def derivative_frame_unitary(t: jnp.ndarray) -&gt; jnp.ndarray:\n        # NOTE: Assume that the frame is time independent.\n        return 1j * frame @ frame_unitary(t)\n\n    def rotating_frame_hamiltonian_v0(args: HamiltonianArgs, t: jnp.ndarray):\n        return frame_unitary(t) @ hamiltonian(args, t) @ jnp.transpose(\n            jnp.conjugate(frame_unitary(t))\n        ) + 1j * (\n            derivative_frame_unitary(t) @ jnp.transpose(jnp.conjugate(frame_unitary(t)))\n        )\n\n    def rotating_frame_hamiltonian(args: HamiltonianArgs, t: jnp.ndarray):\n        # NOTE: Assume that the product of derivative and conjugate of frame unitary is identity\n        return (\n            frame_unitary(t)\n            @ hamiltonian(args, t)\n            @ jnp.transpose(jnp.conjugate(frame_unitary(t)))\n            - frame\n        )\n\n    return (\n        rotating_frame_hamiltonian_v0 if explicit_deriv else rotating_frame_hamiltonian\n    )\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.explicit_auto_rotating_frame_hamiltonian","title":"explicit_auto_rotating_frame_hamiltonian","text":"<pre><code>explicit_auto_rotating_frame_hamiltonian(hamiltonian: Callable[[HamiltonianArgs, ndarray], ndarray], frame: ndarray)\n</code></pre> <p>Implement the Hamiltonian in the rotating frame with H_I = U(t) @ H @ U^dagger(t) + i * U(t) @ dU^dagger(t)/dt</p> Note <p>This is the implementation of <code>auto_rotating_frame_hamiltonian</code> that perform explicit derivative.</p> <p>Parameters:</p> Name Type Description Default <code>hamiltonian</code> <code>Callable</code> <p>The hamiltonian function</p> required <code>frame</code> <code>ndarray</code> <p>The frame matrix</p> required Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def explicit_auto_rotating_frame_hamiltonian(\n    hamiltonian: typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray],\n    frame: jnp.ndarray,\n):\n    \"\"\"Implement the Hamiltonian in the rotating frame with\n    H_I = U(t) @ H @ U^dagger(t) + i * U(t) @ dU^dagger(t)/dt\n\n    Note:\n        This is the implementation of `auto_rotating_frame_hamiltonian`\n        that perform explicit derivative.\n\n    Args:\n        hamiltonian (Callable): The hamiltonian function\n        frame (jnp.ndarray): The frame matrix\n    \"\"\"\n\n    def frame_unitary(t: jnp.ndarray) -&gt; jnp.ndarray:\n        return jax.scipy.linalg.expm(1j * frame * t)\n\n    def derivative_frame_unitary(t: jnp.ndarray) -&gt; jnp.ndarray:\n        # NOTE: Assume that the frame is time independent.\n        return 1j * frame @ frame_unitary(t)\n\n    def rotating_frame_hamiltonian_v0(args: HamiltonianArgs, t: jnp.ndarray):\n        return frame_unitary(t) @ hamiltonian(args, t) @ jnp.transpose(\n            jnp.conjugate(frame_unitary(t))\n        ) + 1j * (\n            derivative_frame_unitary(t) @ jnp.transpose(jnp.conjugate(frame_unitary(t)))\n        )\n\n    return rotating_frame_hamiltonian_v0\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.a","title":"a","text":"<pre><code>a(dims: int) -&gt; ndarray\n</code></pre> <p>Annihilation operator of given dims</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>int</code> <p>Number of states</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Annihilation operator</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def a(dims: int) -&gt; jnp.ndarray:\n    \"\"\"Annihilation operator of given dims\n\n    Args:\n        dims (int): Number of states\n\n    Returns:\n        jnp.ndarray: Annihilation operator\n    \"\"\"\n    return jnp.diag(jnp.sqrt(jnp.arange(1, dims)), 1)\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.a_dag","title":"a_dag","text":"<pre><code>a_dag(dims: int) -&gt; ndarray\n</code></pre> <p>Creation operator of given dims</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>int</code> <p>Number of states</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Creation operator</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def a_dag(dims: int) -&gt; jnp.ndarray:\n    \"\"\"Creation operator of given dims\n\n    Args:\n        dims (int): Number of states\n\n    Returns:\n        jnp.ndarray: Creation operator\n    \"\"\"\n    return jnp.diag(jnp.sqrt(jnp.arange(1, dims)), -1)\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.N","title":"N","text":"<pre><code>N(dims: int) -&gt; ndarray\n</code></pre> <p>Number operator of given dims</p> <p>Parameters:</p> Name Type Description Default <code>dims</code> <code>int</code> <p>Number of states</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Number operator</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def N(dims: int) -&gt; jnp.ndarray:\n    \"\"\"Number operator of given dims\n\n    Args:\n        dims (int): Number of states\n\n    Returns:\n        jnp.ndarray: Number operator\n    \"\"\"\n    return jnp.diag(jnp.arange(dims))\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.gen_hamiltonian_from","title":"gen_hamiltonian_from","text":"<pre><code>gen_hamiltonian_from(qubit_informations: list[QubitInformation], coupling_constants: list[CouplingInformation], dims: int = 2) -&gt; dict[str, HamiltonianTerm]\n</code></pre> <p>Generate dict of Hamiltonian from given qubits and coupling information.</p> <p>Parameters:</p> Name Type Description Default <code>qubit_informations</code> <code>list[QubitInformation]</code> <p>Qubit information</p> required <code>coupling_constants</code> <code>list[CouplingInformation]</code> <p>Coupling information</p> required <code>dims</code> <code>int</code> <p>The level of the quantum system. Defaults to 2, i.e. qubit system.</p> <code>2</code> <p>Returns:</p> Type Description <code>dict[str, HamiltonianTerm]</code> <p>dict[str, HamiltonianTerm]: description</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def gen_hamiltonian_from(\n    qubit_informations: list[QubitInformation],\n    coupling_constants: list[CouplingInformation],\n    dims: int = 2,\n) -&gt; dict[str, HamiltonianTerm]:\n    \"\"\"Generate dict of Hamiltonian from given qubits and coupling information.\n\n    Args:\n        qubit_informations (list[QubitInformation]): Qubit information\n        coupling_constants (list[CouplingInformation]): Coupling information\n        dims (int, optional): The level of the quantum system. Defaults to 2, i.e. qubit system.\n\n    Returns:\n        dict[str, HamiltonianTerm]: _description_\n    \"\"\"\n    num_qubits = len(qubit_informations)\n\n    operators: dict[str, HamiltonianTerm] = {}\n\n    for idx, qubit in enumerate(qubit_informations):\n        # The static Hamiltonian terms\n        static_i = 2 * jnp.pi * qubit.frequency * (jnp.eye(dims) - 2 * N(dims)) / 2\n\n        operators[ChannelID(qubit_idx=qubit.qubit_idx, type=TermType.STATIC).hash()] = (\n            HamiltonianTerm(\n                qubit_idx=qubit.qubit_idx,\n                type=TermType.STATIC,\n                controlable=False,\n                operator=tensor(static_i, idx, num_qubits),\n            )\n        )\n\n        # The anharmonicity term\n        anhar_i = 2 * jnp.pi * qubit.anharmonicity * (N(dims) @ N(dims) - N(dims)) / 2\n\n        operators[\n            ChannelID(qubit_idx=qubit.qubit_idx, type=TermType.ANHAMONIC).hash()\n        ] = HamiltonianTerm(\n            qubit_idx=qubit.qubit_idx,\n            type=TermType.ANHAMONIC,\n            controlable=False,\n            operator=tensor(anhar_i, idx, num_qubits),\n        )\n\n        # The drive terms\n        drive_i = 2 * jnp.pi * qubit.drive_strength * (a(dims) + a_dag(dims))\n\n        operators[ChannelID(qubit_idx=qubit.qubit_idx, type=TermType.DRIVE).hash()] = (\n            HamiltonianTerm(\n                qubit_idx=qubit.qubit_idx,\n                type=TermType.DRIVE,\n                controlable=True,\n                operator=tensor(drive_i, idx, num_qubits),\n            )\n        )\n\n        # The control terms that drive with another qubit frequency\n        control_i = 2 * jnp.pi * qubit.drive_strength * (a(dims) + a_dag(dims))\n\n        operators[\n            ChannelID(qubit_idx=qubit.qubit_idx, type=TermType.CONTROL).hash()\n        ] = HamiltonianTerm(\n            qubit_idx=qubit.qubit_idx,\n            type=TermType.CONTROL,\n            controlable=True,\n            operator=tensor(control_i, idx, num_qubits),\n        )\n\n    for coupling in coupling_constants:\n        # Add the coupling constant to the Hamiltonian\n        c_1 = tensor(a(dims), coupling.qubit_indices[0], num_qubits) @ tensor(\n            a_dag(dims), coupling.qubit_indices[1], num_qubits\n        )\n        c_2 = tensor(a_dag(dims), coupling.qubit_indices[0], num_qubits) @ tensor(\n            a(dims), coupling.qubit_indices[1], num_qubits\n        )\n        coupling_ij = 2 * jnp.pi * coupling.coupling_strength * (c_1 + c_2)\n\n        operators[\n            ChannelID(\n                qubit_idx=(coupling.qubit_indices[0], coupling.qubit_indices[1]),\n                type=TermType.COUPLING,\n            ).hash()\n        ] = HamiltonianTerm(\n            qubit_idx=(coupling.qubit_indices[0], coupling.qubit_indices[1]),\n            type=TermType.COUPLING,\n            controlable=False,\n            operator=coupling_ij,\n        )\n\n    return operators\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.hamiltonian_fn","title":"hamiltonian_fn","text":"<pre><code>hamiltonian_fn(args: dict[str, SignalParameters], t: ndarray, signals: dict[str, Callable[[SignalParameters, ndarray], ndarray]], hamiltonian_terms: dict[str, HamiltonianTerm], static_terms: list[str]) -&gt; ndarray\n</code></pre> <p>Hamiltonian function to be used whitebox. Expect to be used in partial form, i.e. making <code>signals</code>, <code>hamiltonian_terms</code>, and <code>static_terms</code> static arguments.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>dict[str, SignalParameters]</code> <p>Control parameter</p> required <code>t</code> <code>ndarray</code> <p>Time to evaluate</p> required <code>signals</code> <code>dict[str, Callable[[SignalParameters, ndarray], ndarray]]</code> <p>Signal function of the control</p> required <code>hamiltonian_terms</code> <code>dict[str, HamiltonianTerm]</code> <p>Dict of Hamiltonian terms, where key is channel</p> required <code>static_terms</code> <code>list[str]</code> <p>list of channel id specifing the static term.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: description</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def hamiltonian_fn(\n    args: dict[str, SignalParameters],\n    t: jnp.ndarray,\n    signals: dict[str, typing.Callable[[SignalParameters, jnp.ndarray], jnp.ndarray]],\n    hamiltonian_terms: dict[str, HamiltonianTerm],\n    static_terms: list[str],\n) -&gt; jnp.ndarray:\n    \"\"\"Hamiltonian function to be used whitebox.\n    Expect to be used in partial form, i.e. making `signals`, `hamiltonian_terms`, and `static_terms` static arguments.\n\n    Args:\n        args (dict[str, SignalParameters]): Control parameter\n        t (jnp.ndarray): Time to evaluate\n        signals (dict[str, typing.Callable[[SignalParameters, jnp.ndarray], jnp.ndarray]]): Signal function of the control\n        hamiltonian_terms (dict[str, HamiltonianTerm]): Dict of Hamiltonian terms, where key is channel\n        static_terms (list[str]): list of channel id specifing the static term.\n\n    Returns:\n        jnp.ndarray: _description_\n    \"\"\"\n    # Match the args with signal\n    drives = jnp.array(\n        [\n            signal(args[channel_id], t) * hamiltonian_terms[channel_id].operator\n            for channel_id, signal in signals.items()\n        ]\n    )\n\n    statics = jnp.array(\n        [hamiltonian_terms[static_term].operator for static_term in static_terms]\n    )\n\n    return jnp.sum(drives, axis=0) + jnp.sum(statics, axis=0)\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.signal_func_v3","title":"signal_func_v3","text":"<pre><code>signal_func_v3(get_envelope: Callable, drive_frequency: float, dt: float)\n</code></pre> <p>Make the envelope function into signal with drive frequency</p> <p>Parameters:</p> Name Type Description Default <code>get_envelope</code> <code>Callable</code> <p>The envelope function in unit of dt</p> required <code>drive_frequency</code> <code>float</code> <p>drive freuqency in unit of GHz</p> required <code>dt</code> <code>float</code> <p>The dt provived will be used to convert envelope unit to ns,         set to 1 if the envelope function is already in unit of ns</p> required Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def signal_func_v3(get_envelope: typing.Callable, drive_frequency: float, dt: float):\n    \"\"\"Make the envelope function into signal with drive frequency\n\n    Args:\n        get_envelope (Callable): The envelope function in unit of dt\n        drive_frequency (float): drive freuqency in unit of GHz\n        dt (float): The dt provived will be used to convert envelope unit to ns,\n                    set to 1 if the envelope function is already in unit of ns\n    \"\"\"\n\n    def signal(pulse_parameters: SignalParameters, t: jnp.ndarray):\n        return jnp.real(\n            get_envelope(pulse_parameters.pulse_params)(t / dt)\n            * jnp.exp(\n                1j * ((2 * jnp.pi * drive_frequency * t) + pulse_parameters.phase)\n            )\n        )\n\n    return signal\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.signal_func_v5","title":"signal_func_v5","text":"<pre><code>signal_func_v5(get_envelope: Callable[[ControlParam], Callable[[ndarray], ndarray]], drive_frequency: float, dt: float)\n</code></pre> <p>Make the envelope function into signal with drive frequency</p> <p>Parameters:</p> Name Type Description Default <code>get_envelope</code> <code>Callable</code> <p>The envelope function in unit of dt</p> required <code>drive_frequency</code> <code>float</code> <p>drive freuqency in unit of GHz</p> required <code>dt</code> <code>float</code> <p>The dt provived will be used to convert envelope unit to ns,         set to 1 if the envelope function is already in unit of ns</p> required Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def signal_func_v5(\n    get_envelope: typing.Callable[\n        [ControlParam], typing.Callable[[jnp.ndarray], jnp.ndarray]\n    ],\n    drive_frequency: float,\n    dt: float,\n):\n    \"\"\"Make the envelope function into signal with drive frequency\n\n    Args:\n        get_envelope (Callable): The envelope function in unit of dt\n        drive_frequency (float): drive freuqency in unit of GHz\n        dt (float): The dt provived will be used to convert envelope unit to ns,\n                    set to 1 if the envelope function is already in unit of ns\n    \"\"\"\n\n    def signal(control_parameters: ControlParam, t: jnp.ndarray):\n        return jnp.real(\n            get_envelope(control_parameters)(t / dt)\n            * jnp.exp(1j * (2 * jnp.pi * drive_frequency * t))\n        )\n\n    return signal\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.gate_fidelity","title":"gate_fidelity","text":"<pre><code>gate_fidelity(U: ndarray, V: ndarray) -&gt; ndarray\n</code></pre> <p>Calculate the gate fidelity between U and V</p> <p>Parameters:</p> Name Type Description Default <code>U</code> <code>ndarray</code> <p>Unitary operator to be targetted</p> required <code>V</code> <code>ndarray</code> <p>Unitary operator to be compared</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Gate fidelity</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def gate_fidelity(U: jnp.ndarray, V: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Calculate the gate fidelity between U and V\n\n    Args:\n        U (jnp.ndarray): Unitary operator to be targetted\n        V (jnp.ndarray): Unitary operator to be compared\n\n    Returns:\n        jnp.ndarray: Gate fidelity\n    \"\"\"\n    up = jnp.trace(U.conj().T @ V)\n    down = jnp.sqrt(jnp.trace(U.conj().T @ U) * jnp.trace(V.conj().T @ V))\n\n    return jnp.abs(up / down) ** 2\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.check_valid_density_matrix","title":"check_valid_density_matrix","text":"<pre><code>check_valid_density_matrix(rho: ndarray)\n</code></pre> <p>Check if the provided matrix is valid density matrix</p> <p>Parameters:</p> Name Type Description Default <code>rho</code> <code>ndarray</code> <p>description</p> required Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def check_valid_density_matrix(rho: jnp.ndarray):\n    \"\"\"Check if the provided matrix is valid density matrix\n\n    Args:\n        rho (jnp.ndarray): _description_\n    \"\"\"\n    # Check if the density matrix is valid\n    assert jnp.allclose(jnp.trace(rho), 1.0), \"Density matrix is not trace 1\"\n    assert jnp.allclose(rho, rho.conj().T), \"Density matrix is not Hermitian\"\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.check_hermitian","title":"check_hermitian","text":"<pre><code>check_hermitian(op: ndarray)\n</code></pre> <p>Check if the provided matrix is Hermitian</p> <p>Parameters:</p> Name Type Description Default <code>op</code> <code>ndarray</code> <p>Matrix to be assert</p> required Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def check_hermitian(op: jnp.ndarray):\n    \"\"\"Check if the provided matrix is Hermitian\n\n    Args:\n        op (jnp.ndarray): Matrix to be assert\n    \"\"\"\n    assert jnp.allclose(op, op.conj().T), \"Matrix is not Hermitian\"\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.direct_AFG_estimation","title":"direct_AFG_estimation","text":"<pre><code>direct_AFG_estimation(coefficients: ndarray, expectation_values: ndarray) -&gt; ndarray\n</code></pre> <p>Calculate single qubit average gate fidelity from expectation value This function should be used with <code>direct_AFG_estimation_coefficients</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; coefficients = direct_AFG_estimation_coefficients(unitary)\n... agf = direct_AFG_estimation(coefficients, expectation_value)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>coefficients</code> <code>ndarray</code> <p>The coefficients return from <code>direct_AFG_estimation_coefficients</code></p> required <code>expectation_values</code> <code>ndarray</code> <p>The expectation values assume to be shape of (..., 18) with order of <code>sq.constant.default_expectation_values_order</code></p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Average Gate Fidelity</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def direct_AFG_estimation(\n    coefficients: jnp.ndarray,\n    expectation_values: jnp.ndarray,\n) -&gt; jnp.ndarray:\n    \"\"\"Calculate single qubit average gate fidelity from expectation value\n    This function should be used with `direct_AFG_estimation_coefficients`\n\n    Examples:\n        &gt;&gt;&gt; coefficients = direct_AFG_estimation_coefficients(unitary)\n        ... agf = direct_AFG_estimation(coefficients, expectation_value)\n\n    Args:\n        coefficients (jnp.ndarray): The coefficients return from `direct_AFG_estimation_coefficients`\n        expectation_values (jnp.ndarray): The expectation values assume to be shape of (..., 18) with order of `sq.constant.default_expectation_values_order`\n\n    Returns:\n        jnp.ndarray: Average Gate Fidelity\n    \"\"\"\n    return (1 / 2) + ((1 / 12) * jnp.dot(coefficients, expectation_values))\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.direct_AFG_estimation_coefficients","title":"direct_AFG_estimation_coefficients","text":"<pre><code>direct_AFG_estimation_coefficients(target_unitary: ndarray) -&gt; ndarray\n</code></pre> <p>Compute the expected coefficients to be used for AGF calculation using <code>direct_AFG_estimation</code>. The order of coefficients is the same as <code>sq.constant.default_expectation_values_order</code></p> <p>Parameters:</p> Name Type Description Default <code>target_unitary</code> <code>ndarray</code> <p>Target unitary to be computed for coefficient</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Coefficients for AGF calculation.</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def direct_AFG_estimation_coefficients(target_unitary: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Compute the expected coefficients to be used for AGF calculation using `direct_AFG_estimation`.\n    The order of coefficients is the same as `sq.constant.default_expectation_values_order`\n\n    Args:\n        target_unitary (jnp.ndarray): Target unitary to be computed for coefficient\n\n    Returns:\n        jnp.ndarray: Coefficients for AGF calculation.\n    \"\"\"\n    coefficients = []\n    for pauli_i in [X, Y, Z]:\n        for pauli_j in [X, Y, Z]:\n            pauli_coeff = (1 / 2) * jnp.trace(\n                pauli_i @ target_unitary @ pauli_j @ target_unitary.conj().T\n            )\n            for state_coeff in [1, -1]:\n                coeff = state_coeff * pauli_coeff\n                coefficients.append(coeff)\n\n    return jnp.real(jnp.array(coefficients))\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.calculate_exp","title":"calculate_exp","text":"<pre><code>calculate_exp(unitary: ndarray, operator: ndarray, density_matrix: ndarray) -&gt; ndarray\n</code></pre> <p>Calculate the expectation value for given unitary, observable (operator), initial state (density_matrix). Shape of all arguments must be boardcastable.</p> <p>Parameters:</p> Name Type Description Default <code>unitary</code> <code>ndarray</code> <p>Unitary operator</p> required <code>operator</code> <code>ndarray</code> <p>Quantum Observable</p> required <code>density_matrix</code> <code>ndarray</code> <p>Intial state in form of density matrix.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Expectation value of quantum observable.</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def calculate_exp(\n    unitary: jnp.ndarray, operator: jnp.ndarray, density_matrix: jnp.ndarray\n) -&gt; jnp.ndarray:\n    \"\"\"Calculate the expectation value for given unitary, observable (operator), initial state (density_matrix).\n    Shape of all arguments must be boardcastable.\n\n    Args:\n        unitary (jnp.ndarray): Unitary operator\n        operator (jnp.ndarray): Quantum Observable\n        density_matrix (jnp.ndarray): Intial state in form of density matrix.\n\n    Returns:\n        jnp.ndarray: Expectation value of quantum observable.\n    \"\"\"\n    rho = jnp.matmul(\n        unitary, jnp.matmul(density_matrix, unitary.conj().swapaxes(-2, -1))\n    )\n    temp = jnp.matmul(rho, operator)\n    return jnp.real(jnp.sum(jnp.diagonal(temp, axis1=-2, axis2=-1), axis=-1))\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.unitaries_prod","title":"unitaries_prod","text":"<pre><code>unitaries_prod(prev_unitary: ndarray, curr_unitary: ndarray) -&gt; tuple[ndarray, ndarray]\n</code></pre> <p>Function to be used for trotterization Whitebox</p> <p>Parameters:</p> Name Type Description Default <code>prev_unitary</code> <code>ndarray</code> <p>Product of cummulate Unitary operator.</p> required <code>curr_unitary</code> <code>ndarray</code> <p>The next Unitary operator to be multiply.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[jnp.ndarray, jnp.ndarray]: Product of previous unitart and current unitary.</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def unitaries_prod(\n    prev_unitary: jnp.ndarray, curr_unitary: jnp.ndarray\n) -&gt; tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"Function to be used for trotterization Whitebox\n\n    Args:\n        prev_unitary (jnp.ndarray): Product of cummulate Unitary operator.\n        curr_unitary (jnp.ndarray): The next Unitary operator to be multiply.\n\n    Returns:\n        tuple[jnp.ndarray, jnp.ndarray]: Product of previous unitart and current unitary.\n    \"\"\"\n    prod_unitary = prev_unitary @ curr_unitary\n    return prod_unitary, prod_unitary\n</code></pre>"},{"location":"api/physics/#src.inspeqtor.experimental.physics.make_trotterization_solver","title":"make_trotterization_solver","text":"<pre><code>make_trotterization_solver(hamiltonian: Callable[..., ndarray], control_sequence: ControlSequence, dt: float, trotter_steps: int, y0: ndarray)\n</code></pre> <p>Retutn whitebox function compute using Trotterization strategy.</p> <p>Parameters:</p> Name Type Description Default <code>hamiltonian</code> <code>Callable[..., ndarray]</code> <p>The Hamiltonian function of the system</p> required <code>control_sequence</code> <code>ControlSequence</code> <p>The pulse sequence instance</p> required <code>dt</code> <code>float</code> <p>The duration of time step in nanosecond.</p> required <code>trotter_steps</code> <code>int</code> <p>The number of trotterization step.</p> required <code>y0</code> <code>ndarray</code> <p>The initial unitary state. Defaults to jnp.eye(2, dtype=jnp.complex128)</p> required <p>Returns:</p> Type Description <p>typing.Callable[..., jnp.ndarray]: Trotterization Whitebox function</p> Source code in <code>src/inspeqtor/experimental/physics.py</code> <pre><code>def make_trotterization_solver(\n    hamiltonian: typing.Callable[..., jnp.ndarray],\n    control_sequence: ControlSequence,\n    dt: float,\n    trotter_steps: int,\n    y0: jnp.ndarray,\n):\n    \"\"\"Retutn whitebox function compute using Trotterization strategy.\n\n    Args:\n        hamiltonian (typing.Callable[..., jnp.ndarray]): The Hamiltonian function of the system\n        control_sequence (ControlSequence): The pulse sequence instance\n        dt (float, optional): The duration of time step in nanosecond.\n        trotter_steps (int, optional): The number of trotterization step.\n        y0 (jnp.ndarray): The initial unitary state. Defaults to jnp.eye(2, dtype=jnp.complex128)\n\n    Returns:\n        typing.Callable[..., jnp.ndarray]: Trotterization Whitebox function\n    \"\"\"\n    hamiltonian = jax.jit(hamiltonian)\n    time_step = jnp.linspace(0, control_sequence.total_dt * dt, trotter_steps)\n\n    def whitebox(control_parameters: jnp.ndarray):\n        hamiltonians = jax.vmap(hamiltonian, in_axes=(None, 0))(\n            control_parameters, time_step\n        )\n        unitaries = jax.scipy.linalg.expm(\n            -1j * (time_step[1] - time_step[0]) * hamiltonians\n        )\n        # * Nice explanation of scan\n        # * https://www.nelsontang.com/blog/a-friendly-introduction-to-scan-with-jax\n        _, unitaries = jax.lax.scan(unitaries_prod, y0, unitaries)\n        return unitaries\n\n    return whitebox\n</code></pre>"},{"location":"api/predefined/","title":"predefined","text":""},{"location":"api/predefined/#src.inspeqtor.experimental.predefined","title":"src.inspeqtor.experimental.predefined","text":""},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.HamiltonianSpec","title":"HamiltonianSpec  <code>dataclass</code>","text":"Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>@dataclass\nclass HamiltonianSpec:\n    method: WhiteboxStrategy\n    hamiltonian_enum: HamiltonianEnum = HamiltonianEnum.rotating_transmon_hamiltonian\n    # For Trotterization\n    trotter_steps: int = 1000\n    # For ODE sovler\n    max_steps = int(2**16)\n\n    def get_hamiltonian_fn(self):\n        if self.hamiltonian_enum == HamiltonianEnum.rotating_transmon_hamiltonian:\n            return rotating_transmon_hamiltonian\n        elif self.hamiltonian_enum == HamiltonianEnum.transmon_hamiltonian:\n            return transmon_hamiltonian\n        else:\n            raise ValueError(f\"Unsupport Hamiltonian: {self.hamiltonian_enum}\")\n\n    def get_solver(\n        self,\n        control_sequence: ControlSequence,\n        qubit_info: QubitInformation,\n        dt: float,\n    ):\n        \"\"\"Return Unitary solver from the given specification of the Hamiltonian and solver\n\n        Args:\n            control_sequence (ControlSequence): The control sequence object\n            qubit_info (QubitInformation): The qubit information object\n            dt (float): The time step size of the device\n\n        Raises:\n            ValueError: Unsupport Solver method\n\n        Returns:\n            typing.Any: The unitary solver\n        \"\"\"\n        if self.method == WhiteboxStrategy.TROTTER:\n            hamiltonian = partial(\n                self.get_hamiltonian_fn(),\n                qubit_info=qubit_info,\n                signal=signal_func_v5(\n                    get_envelope=get_envelope_transformer(\n                        control_sequence=control_sequence\n                    ),\n                    drive_frequency=qubit_info.frequency,\n                    dt=dt,\n                ),\n            )\n\n            whitebox = make_trotterization_solver(\n                hamiltonian=hamiltonian,\n                control_sequence=control_sequence,\n                dt=dt,\n                trotter_steps=self.trotter_steps,\n                y0=jnp.eye(2, dtype=jnp.complex128),\n            )\n            return whitebox\n        elif self.method == WhiteboxStrategy.ODE:\n            return get_single_qubit_whitebox(\n                self.get_hamiltonian_fn(),\n                control_sequence,\n                qubit_info,\n                dt,\n                self.max_steps,\n            )\n        else:\n            raise ValueError(\"Unsupport method\")\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.HamiltonianSpec.get_solver","title":"get_solver","text":"<pre><code>get_solver(control_sequence: ControlSequence, qubit_info: QubitInformation, dt: float)\n</code></pre> <p>Return Unitary solver from the given specification of the Hamiltonian and solver</p> <p>Parameters:</p> Name Type Description Default <code>control_sequence</code> <code>ControlSequence</code> <p>The control sequence object</p> required <code>qubit_info</code> <code>QubitInformation</code> <p>The qubit information object</p> required <code>dt</code> <code>float</code> <p>The time step size of the device</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Unsupport Solver method</p> <p>Returns:</p> Type Description <p>typing.Any: The unitary solver</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def get_solver(\n    self,\n    control_sequence: ControlSequence,\n    qubit_info: QubitInformation,\n    dt: float,\n):\n    \"\"\"Return Unitary solver from the given specification of the Hamiltonian and solver\n\n    Args:\n        control_sequence (ControlSequence): The control sequence object\n        qubit_info (QubitInformation): The qubit information object\n        dt (float): The time step size of the device\n\n    Raises:\n        ValueError: Unsupport Solver method\n\n    Returns:\n        typing.Any: The unitary solver\n    \"\"\"\n    if self.method == WhiteboxStrategy.TROTTER:\n        hamiltonian = partial(\n            self.get_hamiltonian_fn(),\n            qubit_info=qubit_info,\n            signal=signal_func_v5(\n                get_envelope=get_envelope_transformer(\n                    control_sequence=control_sequence\n                ),\n                drive_frequency=qubit_info.frequency,\n                dt=dt,\n            ),\n        )\n\n        whitebox = make_trotterization_solver(\n            hamiltonian=hamiltonian,\n            control_sequence=control_sequence,\n            dt=dt,\n            trotter_steps=self.trotter_steps,\n            y0=jnp.eye(2, dtype=jnp.complex128),\n        )\n        return whitebox\n    elif self.method == WhiteboxStrategy.ODE:\n        return get_single_qubit_whitebox(\n            self.get_hamiltonian_fn(),\n            control_sequence,\n            qubit_info,\n            dt,\n            self.max_steps,\n        )\n    else:\n        raise ValueError(\"Unsupport method\")\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.rotating_transmon_hamiltonian","title":"rotating_transmon_hamiltonian","text":"<pre><code>rotating_transmon_hamiltonian(params: HamiltonianArgs, t: ndarray, qubit_info: QubitInformation, signal: Callable[[HamiltonianArgs, ndarray], ndarray]) -&gt; ndarray\n</code></pre> <p>Rotating frame hamiltonian of the transmon model</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>HamiltonianParameters</code> <p>The parameter of the pulse for hamiltonian</p> required <code>t</code> <code>ndarray</code> <p>The time to evaluate the Hamiltonian</p> required <code>qubit_info</code> <code>QubitInformation</code> <p>The information of qubit</p> required <code>signal</code> <code>Callable[..., ndarray]</code> <p>The pulse signal</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The Hamiltonian</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def rotating_transmon_hamiltonian(\n    params: HamiltonianArgs,\n    t: jnp.ndarray,\n    qubit_info: QubitInformation,\n    signal: typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray],\n) -&gt; jnp.ndarray:\n    \"\"\"Rotating frame hamiltonian of the transmon model\n\n    Args:\n        params (HamiltonianParameters): The parameter of the pulse for hamiltonian\n        t (jnp.ndarray): The time to evaluate the Hamiltonian\n        qubit_info (QubitInformation): The information of qubit\n        signal (Callable[..., jnp.ndarray]): The pulse signal\n\n    Returns:\n        jnp.ndarray: The Hamiltonian\n    \"\"\"\n    a0 = 2 * jnp.pi * qubit_info.frequency\n    a1 = 2 * jnp.pi * qubit_info.drive_strength\n\n    def f3(params, t):\n        return a1 * signal(params, t)\n\n    def f_sigma_x(params, t):\n        return f3(params, t) * jnp.cos(a0 * t)\n\n    def f_sigma_y(params, t):\n        return f3(params, t) * jnp.sin(a0 * t)\n\n    return f_sigma_x(params, t) * X - f_sigma_y(params, t) * Y\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.transmon_hamiltonian","title":"transmon_hamiltonian","text":"<pre><code>transmon_hamiltonian(params: HamiltonianArgs, t: ndarray, qubit_info: QubitInformation, signal: Callable[[HamiltonianArgs, ndarray], ndarray]) -&gt; ndarray\n</code></pre> <p>Lab frame hamiltonian of the transmon model</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>HamiltonianParameters</code> <p>The parameter of the pulse for hamiltonian</p> required <code>t</code> <code>ndarray</code> <p>The time to evaluate the Hamiltonian</p> required <code>qubit_info</code> <code>QubitInformation</code> <p>The information of qubit</p> required <code>signal</code> <code>Callable[..., ndarray]</code> <p>The pulse signal</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The Hamiltonian</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def transmon_hamiltonian(\n    params: HamiltonianArgs,\n    t: jnp.ndarray,\n    qubit_info: QubitInformation,\n    signal: typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray],\n) -&gt; jnp.ndarray:\n    \"\"\"Lab frame hamiltonian of the transmon model\n\n    Args:\n        params (HamiltonianParameters): The parameter of the pulse for hamiltonian\n        t (jnp.ndarray): The time to evaluate the Hamiltonian\n        qubit_info (QubitInformation): The information of qubit\n        signal (Callable[..., jnp.ndarray]): The pulse signal\n\n    Returns:\n        jnp.ndarray: The Hamiltonian\n    \"\"\"\n\n    a0 = 2 * jnp.pi * qubit_info.frequency\n    a1 = 2 * jnp.pi * qubit_info.drive_strength\n\n    return ((a0 / 2) * Z) + (a1 * signal(params, t) * X)\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.get_gaussian_control_sequence","title":"get_gaussian_control_sequence","text":"<pre><code>get_gaussian_control_sequence(qubit_info: QubitInformation, max_amp: float = 0.5)\n</code></pre> <p>Get predefined Gaussian control sequence with single Gaussian pulse.</p> <p>Parameters:</p> Name Type Description Default <code>qubit_info</code> <code>QubitInformation</code> <p>Qubit information</p> required <code>max_amp</code> <code>float</code> <p>The maximum amplitude. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ControlSequence</code> <p>Control sequence instance</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def get_gaussian_control_sequence(\n    qubit_info: QubitInformation,\n    max_amp: float = 0.5,  # NOTE: Choice of maximum amplitude is arbitrary\n):\n    \"\"\"Get predefined Gaussian control sequence with single Gaussian pulse.\n\n    Args:\n        qubit_info (QubitInformation): Qubit information\n        max_amp (float, optional): The maximum amplitude. Defaults to 0.5.\n\n    Returns:\n        ControlSequence: Control sequence instance\n    \"\"\"\n    total_length = 320\n    dt = 2 / 9\n\n    control_sequence = ControlSequence(\n        controls=[\n            GaussianPulse(\n                duration=total_length,\n                qubit_drive_strength=qubit_info.drive_strength,\n                dt=dt,\n                max_amp=max_amp,\n                min_theta=0.0,\n                max_theta=2 * jnp.pi,\n            ),\n        ],\n        total_dt=total_length,\n    )\n\n    return control_sequence\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.get_two_axis_gaussian_control_sequence","title":"get_two_axis_gaussian_control_sequence","text":"<pre><code>get_two_axis_gaussian_control_sequence(qubit_info: QubitInformation, max_amp: float = 0.5)\n</code></pre> <p>Get predefined two-axis Gaussian control sequence.</p> <p>Parameters:</p> Name Type Description Default <code>qubit_info</code> <code>QubitInformation</code> <p>Qubit information</p> required <code>max_amp</code> <code>float</code> <p>The maximum amplitude. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ControlSequence</code> <p>Control sequence instance</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def get_two_axis_gaussian_control_sequence(\n    qubit_info: QubitInformation,\n    max_amp: float = 0.5,\n):\n    \"\"\"Get predefined two-axis Gaussian control sequence.\n\n    Args:\n        qubit_info (QubitInformation): Qubit information\n        max_amp (float, optional): The maximum amplitude. Defaults to 0.5.\n\n    Returns:\n        ControlSequence: Control sequence instance\n    \"\"\"\n    total_length = 320\n    dt = 2 / 9\n\n    control_sequence = ControlSequence(\n        controls=[\n            TwoAxisGaussianPulse(\n                duration=total_length,\n                qubit_drive_strength=qubit_info.drive_strength,\n                dt=dt,\n                max_amp=max_amp,\n                min_theta_x=-2 * jnp.pi,\n                max_theta_x=2 * jnp.pi,\n                min_theta_y=-2 * jnp.pi,\n                max_theta_y=2 * jnp.pi,\n            ),\n        ],\n        total_dt=total_length,\n    )\n\n    return control_sequence\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.get_drag_pulse_v2_sequence","title":"get_drag_pulse_v2_sequence","text":"<pre><code>get_drag_pulse_v2_sequence(qubit_info_drive_strength: float, max_amp: float = 0.5, min_theta=0.0, max_theta=2 * pi, min_beta=-2.0, max_beta=2.0, dt=2 / 9)\n</code></pre> <p>Get predefined DRAG control sequence with single DRAG pulse.</p> <p>Parameters:</p> Name Type Description Default <code>qubit_info</code> <code>QubitInformation</code> <p>Qubit information</p> required <code>max_amp</code> <code>float</code> <p>The maximum amplitude. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>ControlSequence</code> <p>Control sequence instance</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def get_drag_pulse_v2_sequence(\n    qubit_info_drive_strength: float,\n    max_amp: float = 0.5,  # NOTE: Choice of maximum amplitude is arbitrary\n    min_theta=0.0,\n    max_theta=2 * jnp.pi,\n    min_beta=-2.0,\n    max_beta=2.0,\n    dt=2 / 9,\n):\n    \"\"\"Get predefined DRAG control sequence with single DRAG pulse.\n\n    Args:\n        qubit_info (QubitInformation): Qubit information\n        max_amp (float, optional): The maximum amplitude. Defaults to 0.5.\n\n    Returns:\n        ControlSequence: Control sequence instance\n    \"\"\"\n    total_length = 320\n    control_sequence = ControlSequence(\n        controls=[\n            DragPulseV2(\n                duration=total_length,\n                qubit_drive_strength=qubit_info_drive_strength,\n                dt=dt,\n                max_amp=max_amp,\n                min_theta=min_theta,\n                max_theta=max_theta,\n                min_beta=min_beta,\n                max_beta=max_beta,\n            ),\n        ],\n        total_dt=total_length,\n    )\n\n    return control_sequence\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.generate_experimental_data","title":"generate_experimental_data","text":"<pre><code>generate_experimental_data(key: ndarray, hamiltonian: Callable[..., ndarray], sample_size: int = 10, shots: int = 1000, strategy: SimulationStrategy = RANDOM, get_qubit_information_fn: Callable[[], QubitInformation] = get_mock_qubit_information, get_control_sequence_fn: Callable[[], ControlSequence] = get_multi_drag_control_sequence_v3, max_steps: int = int(2 ** 16), method: WhiteboxStrategy = ODE, trotter_steps: int = 1000, expectation_value_receipt: list[ExpectationValue] = default_expectation_values_order) -&gt; tuple[ExperimentData, ControlSequence, ndarray, Callable[[ndarray], ndarray]]\n</code></pre> <p>Generate simulated dataset</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>Random key</p> required <code>hamiltonian</code> <code>Callable[..., ndarray]</code> <p>Total Hamiltonian of the device</p> required <code>sample_size</code> <code>int</code> <p>Sample size of the control parameters. Defaults to 10.</p> <code>10</code> <code>shots</code> <code>int</code> <p>Number of shots used to estimate expectation value, will be used if <code>SimulationStrategy</code> is <code>SHOT</code>, otherwise ignored. Defaults to 1000.</p> <code>1000</code> <code>strategy</code> <code>SimulationStrategy</code> <p>Simulation strategy. Defaults to SimulationStrategy.RANDOM.</p> <code>RANDOM</code> <code>get_qubit_information_fn</code> <code>Callable[[], QubitInformation]</code> <p>Function that return qubit information. Defaults to get_mock_qubit_information.</p> <code>get_mock_qubit_information</code> <code>get_control_sequence_fn</code> <code>Callable[[], ControlSequence]</code> <p>Function that return control sequence. Defaults to get_multi_drag_control_sequence_v3.</p> <code>get_multi_drag_control_sequence_v3</code> <code>max_steps</code> <code>int</code> <p>Maximum step of solver. Defaults to int(2**16).</p> <code>int(2 ** 16)</code> <code>method</code> <code>WhiteboxStrategy</code> <p>Unitary solver method. Defaults to WhiteboxStrategy.ODE.</p> <code>ODE</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Not support strategy</p> <p>Returns:</p> Type Description <code>tuple[ExperimentData, ControlSequence, ndarray, Callable[[ndarray], ndarray]]</code> <p>tuple[ExperimentData, ControlSequence, jnp.ndarray, typing.Callable[[jnp.ndarray], jnp.ndarray]]: tuple of (1) Experiment data, (2) Pulse sequence, (3) Noisy unitary, (4) Noisy solver</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def generate_experimental_data(\n    key: jnp.ndarray,\n    hamiltonian: typing.Callable[..., jnp.ndarray],\n    sample_size: int = 10,\n    shots: int = 1000,\n    strategy: SimulationStrategy = SimulationStrategy.RANDOM,\n    get_qubit_information_fn: typing.Callable[\n        [], QubitInformation\n    ] = get_mock_qubit_information,\n    get_control_sequence_fn: typing.Callable[\n        [], ControlSequence\n    ] = get_multi_drag_control_sequence_v3,\n    max_steps: int = int(2**16),\n    method: WhiteboxStrategy = WhiteboxStrategy.ODE,\n    trotter_steps: int = 1000,\n    expectation_value_receipt: list[\n        ExpectationValue\n    ] = default_expectation_values_order,\n) -&gt; tuple[\n    ExperimentData,\n    ControlSequence,\n    jnp.ndarray,\n    typing.Callable[[jnp.ndarray], jnp.ndarray],\n]:\n    \"\"\"Generate simulated dataset\n\n    Args:\n        key (jnp.ndarray): Random key\n        hamiltonian (typing.Callable[..., jnp.ndarray]): Total Hamiltonian of the device\n        sample_size (int, optional): Sample size of the control parameters. Defaults to 10.\n        shots (int, optional): Number of shots used to estimate expectation value, will be used if `SimulationStrategy` is `SHOT`, otherwise ignored. Defaults to 1000.\n        strategy (SimulationStrategy, optional): Simulation strategy. Defaults to SimulationStrategy.RANDOM.\n        get_qubit_information_fn (typing.Callable[ [], QubitInformation ], optional): Function that return qubit information. Defaults to get_mock_qubit_information.\n        get_control_sequence_fn (typing.Callable[ [], ControlSequence ], optional): Function that return control sequence. Defaults to get_multi_drag_control_sequence_v3.\n        max_steps (int, optional): Maximum step of solver. Defaults to int(2**16).\n        method (WhiteboxStrategy, optional): Unitary solver method. Defaults to WhiteboxStrategy.ODE.\n\n    Raises:\n        NotImplementedError: Not support strategy\n\n    Returns:\n        tuple[ExperimentData, ControlSequence, jnp.ndarray, typing.Callable[[jnp.ndarray], jnp.ndarray]]: tuple of (1) Experiment data, (2) Pulse sequence, (3) Noisy unitary, (4) Noisy solver\n    \"\"\"\n    qubit_info, control_sequence, config = get_mock_prefined_exp_v1(\n        sample_size=sample_size,\n        shots=shots,\n        get_control_sequence_fn=get_control_sequence_fn,\n        get_qubit_information_fn=get_qubit_information_fn,\n    )\n\n    # Generate mock expectation value\n    key, exp_key = jax.random.split(key)\n\n    dt = config.device_cycle_time_ns\n\n    if method == WhiteboxStrategy.TROTTER:\n        noisy_simulator = jax.jit(\n            make_trotterization_solver(\n                hamiltonian=hamiltonian,\n                control_sequence=control_sequence,\n                dt=dt,\n                trotter_steps=trotter_steps,\n                y0=jnp.eye(2, dtype=jnp.complex128),\n            )\n        )\n    else:\n        t_eval = jnp.linspace(\n            0, control_sequence.total_dt * dt, control_sequence.total_dt\n        )\n        noisy_simulator = jax.jit(\n            partial(\n                solver,\n                t_eval=t_eval,\n                hamiltonian=hamiltonian,\n                y0=jnp.eye(2, dtype=jnp.complex64),\n                t0=0,\n                t1=control_sequence.total_dt * dt,\n                max_steps=max_steps,\n            )\n        )\n\n    control_params_list = []\n    parameter_structure = control_sequence.get_parameter_names()\n    num_parameters = len(list(itertools.chain.from_iterable(parameter_structure)))\n    # control_params: list[jnp.ndarray] = []\n    control_params = jnp.empty(shape=(sample_size, num_parameters))\n    for control_idx in range(config.sample_size):\n        key, subkey = jax.random.split(key)\n        pulse_params = control_sequence.sample_params(subkey)\n        control_params_list.append(pulse_params)\n\n        control_params = control_params.at[control_idx].set(\n            list_of_params_to_array(pulse_params, parameter_structure)\n        )\n\n    unitaries = jax.vmap(noisy_simulator)(control_params)\n    SHOTS = config.shots\n\n    # Calculate the expectation values depending on the strategy\n    unitaries_f = jnp.asarray(unitaries)[:, -1, :, :]\n\n    assert unitaries_f.shape == (\n        sample_size,\n        2,\n        2,\n    ), f\"Final unitaries shape is {unitaries_f.shape}\"\n\n    if strategy == SimulationStrategy.RANDOM:\n        # Just random expectation values with key\n        expectation_values = 2 * (\n            jax.random.uniform(exp_key, shape=(config.sample_size, 18)) - (1 / 2)\n        )\n    elif strategy == SimulationStrategy.IDEAL:\n        expectation_values = calculate_expectation_values(unitaries_f)\n\n    elif strategy == SimulationStrategy.SHOT:\n        key, sample_key = jax.random.split(key)\n        # The `shot_quantum_device` function will re-calculate the unitary\n        expectation_values = shot_quantum_device(\n            sample_key,\n            control_params,\n            noisy_simulator,\n            SHOTS,\n            expectation_value_receipt,\n        )\n    else:\n        raise NotImplementedError\n\n    assert expectation_values.shape == (\n        sample_size,\n        18,\n    ), f\"Expectation values shape is {expectation_values.shape}\"\n\n    rows = []\n    for sample_idx in range(config.sample_size):\n        for exp_idx, exp in enumerate(expectation_value_receipt):\n            row = make_row(\n                expectation_value=float(expectation_values[sample_idx, exp_idx]),\n                initial_state=exp.initial_state,\n                observable=exp.observable,\n                parameters_list=control_params_list[sample_idx],\n                parameters_id=sample_idx,\n            )\n\n            rows.append(row)\n\n    df = pd.DataFrame(rows)\n\n    exp_data = ExperimentData(experiment_config=config, preprocess_data=df)\n\n    return (\n        exp_data,\n        control_sequence,\n        jnp.array(unitaries),\n        noisy_simulator,\n    )\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.get_single_qubit_whitebox","title":"get_single_qubit_whitebox","text":"<pre><code>get_single_qubit_whitebox(hamiltonian: Callable[..., ndarray], control_sequence: ControlSequence, qubit_info: QubitInformation, dt: float, max_steps: int = int(2 ** 16)) -&gt; Callable[[ndarray], ndarray]\n</code></pre> <p>Generate single qubit whitebox</p> <p>Parameters:</p> Name Type Description Default <code>hamiltonian</code> <code>Callable[..., ndarray]</code> <p>Hamiltonian</p> required <code>control_sequence</code> <code>ControlSequence</code> <p>Control sequence instance</p> required <code>qubit_info</code> <code>QubitInformation</code> <p>Qubit information</p> required <code>dt</code> <code>float</code> <p>Duration of 1 timestep in nanosecond</p> required <code>max_steps</code> <code>int</code> <p>Maximum steps of solver. Defaults to int(2**16).</p> <code>int(2 ** 16)</code> <p>Returns:</p> Type Description <code>Callable[[ndarray], ndarray]</code> <p>typing.Callable[[jnp.ndarray], jnp.ndarray]: Whitebox with ODE solver</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def get_single_qubit_whitebox(\n    hamiltonian: typing.Callable[..., jnp.ndarray],\n    control_sequence: ControlSequence,\n    qubit_info: QubitInformation,\n    dt: float,\n    max_steps: int = int(2**16),\n) -&gt; typing.Callable[[jnp.ndarray], jnp.ndarray]:\n    \"\"\"Generate single qubit whitebox\n\n    Args:\n        hamiltonian (typing.Callable[..., jnp.ndarray]): Hamiltonian\n        control_sequence (ControlSequence): Control sequence instance\n        qubit_info (QubitInformation): Qubit information\n        dt (float): Duration of 1 timestep in nanosecond\n        max_steps (int, optional): Maximum steps of solver. Defaults to int(2**16).\n\n    Returns:\n        typing.Callable[[jnp.ndarray], jnp.ndarray]: Whitebox with ODE solver\n    \"\"\"\n    t_eval = jnp.linspace(0, control_sequence.total_dt * dt, control_sequence.total_dt)\n\n    hamiltonian = partial(\n        hamiltonian,\n        qubit_info=qubit_info,\n        signal=signal_func_v5(\n            get_envelope_transformer(control_sequence),\n            qubit_info.frequency,\n            dt,\n        ),\n    )\n\n    whitebox = partial(\n        solver,\n        t_eval=t_eval,\n        hamiltonian=hamiltonian,\n        y0=jnp.eye(2, dtype=jnp.complex64),\n        t0=0,\n        t1=control_sequence.total_dt * dt,\n        max_steps=max_steps,\n    )\n\n    return whitebox\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.get_single_qubit_rotating_frame_whitebox","title":"get_single_qubit_rotating_frame_whitebox","text":"<pre><code>get_single_qubit_rotating_frame_whitebox(control_sequence: ControlSequence, qubit_info: QubitInformation, dt: float) -&gt; Callable[[ndarray], ndarray]\n</code></pre> <p>Generate single qubit whitebox with rotating transmon hamiltonian</p> <p>Parameters:</p> Name Type Description Default <code>control_sequence</code> <code>ControlSequence</code> <p>Control sequence</p> required <code>qubit_info</code> <code>QubitInformation</code> <p>Qubit information</p> required <code>dt</code> <code>float</code> <p>Duration of 1 timestep in nanosecond</p> required <p>Returns:</p> Type Description <code>Callable[[ndarray], ndarray]</code> <p>typing.Callable[[jnp.ndarray], jnp.ndarray]: Whitebox with ODE solver and rotating transmon hamiltonian</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def get_single_qubit_rotating_frame_whitebox(\n    control_sequence: ControlSequence,\n    qubit_info: QubitInformation,\n    dt: float,\n) -&gt; typing.Callable[[jnp.ndarray], jnp.ndarray]:\n    \"\"\"Generate single qubit whitebox with rotating transmon hamiltonian\n\n    Args:\n        control_sequence (ControlSequence): Control sequence\n        qubit_info (QubitInformation): Qubit information\n        dt (float): Duration of 1 timestep in nanosecond\n\n    Returns:\n        typing.Callable[[jnp.ndarray], jnp.ndarray]: Whitebox with ODE solver and rotating transmon hamiltonian\n    \"\"\"\n    whitebox = get_single_qubit_whitebox(\n        rotating_transmon_hamiltonian,\n        control_sequence,\n        qubit_info,\n        dt,\n    )\n\n    return whitebox\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.load_data_from_path","title":"load_data_from_path","text":"<pre><code>load_data_from_path(path: str | Path, hamiltonian_spec: HamiltonianSpec, pulse_reader=default_pulse_reader) -&gt; LoadedData\n</code></pre> <p>Load and prepare the experimental data from given path and hamiltonian spec.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>The path to the folder that contain experimental data.</p> required <code>hamiltonian_spec</code> <code>HamiltonianSpec</code> <p>The specification of the Hamiltonian</p> required <code>pulse_reader</code> <code>Any</code> <p>description. Defaults to default_pulse_reader.</p> <code>default_pulse_reader</code> <p>Returns:</p> Name Type Description <code>LoadedData</code> <code>LoadedData</code> <p>The object contatin necessary information for device characterization.</p> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def load_data_from_path(\n    path: str | pathlib.Path,\n    hamiltonian_spec: HamiltonianSpec,\n    pulse_reader=default_pulse_reader,\n) -&gt; LoadedData:\n    \"\"\"Load and prepare the experimental data from given path and hamiltonian spec.\n\n    Args:\n        path (str | pathlib.Path): The path to the folder that contain experimental data.\n        hamiltonian_spec (HamiltonianSpec): The specification of the Hamiltonian\n        pulse_reader (typing.Any, optional): _description_. Defaults to default_pulse_reader.\n\n    Returns:\n        LoadedData: The object contatin necessary information for device characterization.\n    \"\"\"\n    exp_data = ExperimentData.from_folder(path)\n    control_sequence = pulse_reader(path)\n\n    qubit_info = exp_data.experiment_config.qubits[0]\n    dt = exp_data.experiment_config.device_cycle_time_ns\n\n    whitebox = hamiltonian_spec.get_solver(control_sequence, qubit_info, dt)\n\n    return prepare_data(exp_data, control_sequence, whitebox)\n</code></pre>"},{"location":"api/predefined/#src.inspeqtor.experimental.predefined.save_data_to_path","title":"save_data_to_path","text":"<pre><code>save_data_to_path(path: str | Path, experiment_data: ExperimentData, control_sequence: ControlSequence)\n</code></pre> <p>Save the experimental data to the path</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>The path to folder to save the experimental data</p> required <code>experiment_data</code> <code>ExperimentData</code> <p>The experimental data object</p> required <code>control_sequence</code> <code>ControlSequence</code> <p>The control sequence that used to create the experimental data.</p> required <p>Returns:</p> Name Type Description <code>None</code> Source code in <code>src/inspeqtor/experimental/predefined.py</code> <pre><code>def save_data_to_path(\n    path: str | pathlib.Path,\n    experiment_data: ExperimentData,\n    control_sequence: ControlSequence,\n):\n    \"\"\"Save the experimental data to the path\n\n    Args:\n        path (str | pathlib.Path): The path to folder to save the experimental data\n        experiment_data (ExperimentData): The experimental data object\n        control_sequence (ControlSequence): The control sequence that used to create the experimental data.\n\n    Returns:\n        None:\n    \"\"\"\n    path = pathlib.Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n    experiment_data.save_to_folder(path)\n    control_sequence.to_file(path)\n\n    return None\n</code></pre>"},{"location":"api/probabilistic/","title":"probabilistic","text":""},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic","title":"src.inspeqtor.experimental.probabilistic","text":""},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.LearningModel","title":"LearningModel","text":"<p>The learning model.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>class LearningModel(StrEnum):\n    \"\"\"The learning model.\"\"\"\n\n    TruncatedNormal = auto()\n    BernoulliProbs = auto()\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_probabilistic_model","title":"make_probabilistic_model","text":"<pre><code>make_probabilistic_model(predictive_model: Callable[..., ndarray], shots: int = 1, block_graybox: bool = False, separate_observables: bool = False, log_expectation_values: bool = False)\n</code></pre> <p>Make probabilistic model from the Statistical model with priors</p> <p>Parameters:</p> Name Type Description Default <code>base_model</code> <code>Module</code> <p>The statistical based model, currently only support flax.linen module</p> required <code>model_prediction_to_expvals_fn</code> <code>Callable[..., ndarray]</code> <p>Function to convert output from model to expectation values array</p> required <code>bnn_prior</code> <code>dict[str, Distribution] | Distribution</code> <p>The priors of BNN. Defaults to dist.Normal(0.0, 1.0).</p> required <code>shots</code> <code>int</code> <p>The number of shots forcing PGM to sample. Defaults to 1.</p> <code>1</code> <code>block_graybox</code> <code>bool</code> <p>If true, the latent variables in Graybox model will be hidden, i.e. not traced by <code>numpyro</code>. Defaults to False.</p> <code>False</code> <code>enable_bnn</code> <code>bool</code> <p>If true, the statistical model will be convert to probabilistic model. Defaults to True.</p> required <code>separate_observables</code> <code>bool</code> <p>If true, the observable will be separate into dict form. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>typing.Callable: Probabilistic Graybox Model</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_probabilistic_model(\n    predictive_model: typing.Callable[..., jnp.ndarray],\n    shots: int = 1,\n    block_graybox: bool = False,\n    separate_observables: bool = False,\n    log_expectation_values: bool = False,\n):\n    \"\"\"Make probabilistic model from the Statistical model with priors\n\n    Args:\n        base_model (nn.Module): The statistical based model, currently only support flax.linen module\n        model_prediction_to_expvals_fn (typing.Callable[..., jnp.ndarray]): Function to convert output from model to expectation values array\n        bnn_prior (dict[str, dist.Distribution] | dist.Distribution, optional): The priors of BNN. Defaults to dist.Normal(0.0, 1.0).\n        shots (int, optional): The number of shots forcing PGM to sample. Defaults to 1.\n        block_graybox (bool, optional): If true, the latent variables in Graybox model will be hidden, i.e. not traced by `numpyro`. Defaults to False.\n        enable_bnn (bool, optional): If true, the statistical model will be convert to probabilistic model. Defaults to True.\n        separate_observables (bool, optional): If true, the observable will be separate into dict form. Defaults to False.\n\n    Returns:\n        typing.Callable: Probabilistic Graybox Model\n    \"\"\"\n\n    def block_graybox_fn(\n        control_parameters: jnp.ndarray,\n        unitaries: jnp.ndarray,\n    ):\n        key = numpyro.prng_key()\n        with handlers.block(), handlers.seed(rng_seed=key):\n            expvals = predictive_model(control_parameters, unitaries)\n\n        return expvals\n\n    graybox_fn = block_graybox_fn if block_graybox else predictive_model\n\n    def bernoulli_model(\n        control_parameters: jnp.ndarray,\n        unitaries: jnp.ndarray,\n        observables: jnp.ndarray | None = None,\n    ):\n        expvals = graybox_fn(control_parameters, unitaries)\n\n        if log_expectation_values:\n            numpyro.deterministic(\"expectation_values\", expvals)\n\n        if observables is None:\n            sizes = control_parameters.shape[:-1] + (18,)\n            if shots &gt; 1:\n                sizes = (shots,) + sizes\n        else:\n            sizes = observables.shape\n\n        # The plate is for the shots prediction to work properly\n        with numpyro.util.optional(\n            shots &gt; 1, numpyro.plate_stack(\"plate\", sizes=list(sizes)[:-1])\n        ):\n            if separate_observables:\n                expvals_samples = {}\n\n                for idx, exp in enumerate(default_expectation_values_order):\n                    s = numpyro.sample(\n                        f\"obs/{exp.initial_state}/{exp.observable}\",\n                        dist.BernoulliProbs(\n                            probs=expectation_value_to_prob_minus(\n                                jnp.expand_dims(expvals[..., idx], axis=-1)\n                            )\n                        ).to_event(1),\n                        obs=(\n                            observables[..., idx] if observables is not None else None\n                        ),\n                    )\n\n                    expvals_samples[f\"obs/{exp.initial_state}/{exp.observable}\"] = s\n\n            else:\n                expvals_samples = numpyro.sample(\n                    \"obs\",\n                    dist.BernoulliProbs(\n                        probs=expectation_value_to_prob_minus(expvals)\n                    ).to_event(1),\n                    obs=observables,\n                    infer={\"enumerate\": \"parallel\"},\n                )\n\n        return expvals_samples\n\n    return bernoulli_model\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.get_args_of_distribution","title":"get_args_of_distribution","text":"<pre><code>get_args_of_distribution(x)\n</code></pre> <p>Get the arguments used to construct Distribution, if the provided parameter is not Distribution, return it. So that the function can be used with <code>jax.tree.map</code>.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>Maybe Distribution</p> required <p>Returns:</p> Type Description <p>typing.Any: Argument of Distribution if Distribution is provided.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>@deprecated\ndef get_args_of_distribution(x):\n    \"\"\"Get the arguments used to construct Distribution, if the provided parameter is not Distribution, return it.\n    So that the function can be used with `jax.tree.map`.\n\n    Args:\n        x (typing.Any): Maybe Distribution\n\n    Returns:\n        typing.Any: Argument of Distribution if Distribution is provided.\n    \"\"\"\n    if isinstance(x, dist.Distribution):\n        return x.get_args()\n    else:\n        return x\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.construct_normal_priors","title":"construct_normal_priors","text":"<pre><code>construct_normal_priors(posterior)\n</code></pre> <p>Construct a dict of Normal Distributions with posterior</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Any</code> <p>Dict of Normal distribution arguments</p> required <p>Returns:</p> Type Description <p>typing.Any: dict of Normal distributions</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>@deprecated\ndef construct_normal_priors(posterior):\n    \"\"\"Construct a dict of Normal Distributions with posterior\n\n    Args:\n        posterior (typing.Any): Dict of Normal distribution arguments\n\n    Returns:\n        typing.Any: dict of Normal distributions\n    \"\"\"\n    posterior_distributions = {}\n    assert isinstance(posterior, dict)\n    for name, value in posterior.items():\n        assert isinstance(name, str)\n        assert isinstance(value, dict)\n        posterior_distributions[name] = dist.Normal(value[\"loc\"], value[\"scale\"])  # type: ignore\n    return posterior_distributions\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.construct_normal_prior_from_samples","title":"construct_normal_prior_from_samples","text":"<pre><code>construct_normal_prior_from_samples(posterior_samples: dict[str, ndarray]) -&gt; dict[str, Distribution]\n</code></pre> <p>Construct a dict of Normal Distributions with posterior sample</p> <p>Parameters:</p> Name Type Description Default <code>posterior_samples</code> <code>dict[str, ndarray]</code> <p>Posterior sample</p> required <p>Returns:</p> Type Description <code>dict[str, Distribution]</code> <p>dict[str, dist.Distribution]: dict of Normal Distributions</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def construct_normal_prior_from_samples(\n    posterior_samples: dict[str, jnp.ndarray],\n) -&gt; dict[str, dist.Distribution]:\n    \"\"\"Construct a dict of Normal Distributions with posterior sample\n\n    Args:\n        posterior_samples (dict[str, jnp.ndarray]): Posterior sample\n\n    Returns:\n        dict[str, dist.Distribution]: dict of Normal Distributions\n    \"\"\"\n\n    posterior_mean = jax.tree.map(lambda x: jnp.mean(x, axis=0), posterior_samples)\n    posterior_std = jax.tree.map(lambda x: jnp.std(x, axis=0), posterior_samples)\n\n    prior = {}\n    for name, mean in posterior_mean.items():\n        prior[name] = dist.Normal(mean, posterior_std[name])\n\n    return prior\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_normal_posterior_dist_fn_from_svi_result","title":"make_normal_posterior_dist_fn_from_svi_result","text":"<pre><code>make_normal_posterior_dist_fn_from_svi_result(key: ndarray, guide: Callable, params: dict[str, ndarray], num_samples: int, prefix: str) -&gt; Callable[[str, tuple[int, ...]], Distribution]\n</code></pre> <p>This function create a get posterior function to be used with <code>numpyro.contrib.module</code>.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>The random key</p> required <code>guide</code> <code>Callable</code> <p>The guide (variational distribution)</p> required <code>params</code> <code>dict[str, ndarray]</code> <p>The variational parameters</p> required <code>num_samples</code> <code>int</code> <p>The number of sample for approxiatation the posterior distributions</p> required <p>Examples:</p> <pre><code>prefix = \"graybox\"\nprior_fn = make_normal_posterior_dist_fn_from_svi_result(\n    jax.random.key(0), guide, result.params, 10_000, prefix\n)\ngraybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(\n    name=prefix,\n    base_model=base_model,\n    adapter_fn=sq.probabilistic.observable_to_expvals,\n    prior=prior_fn,\n)\nposterior_model = sq.probabilistic.make_probabilistic_model(\n    predictive_model=graybox_model, log_expectation_values=True\n)\n</code></pre> <p>Returns:</p> Type Description <code>Callable[[str, tuple[int, ...]], Distribution]</code> <p>typing.Callable[[str, tuple[int, ...]], dist.Distribution]: The function that return posterior distribution</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_normal_posterior_dist_fn_from_svi_result(\n    key: jnp.ndarray,\n    guide: typing.Callable,\n    params: dict[str, jnp.ndarray],\n    num_samples: int,\n    prefix: str,\n) -&gt; typing.Callable[[str, tuple[int, ...]], dist.Distribution]:\n    \"\"\"This function create a get posterior function to be used with `numpyro.contrib.module`.\n\n    Args:\n        key (jnp.ndarray): The random key\n        guide (typing.Callable): The guide (variational distribution)\n        params (dict[str, jnp.ndarray]): The variational parameters\n        num_samples (int): The number of sample for approxiatation the posterior distributions\n\n    Examples:\n        ```python\n        prefix = \"graybox\"\n        prior_fn = make_normal_posterior_dist_fn_from_svi_result(\n            jax.random.key(0), guide, result.params, 10_000, prefix\n        )\n        graybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(\n            name=prefix,\n            base_model=base_model,\n            adapter_fn=sq.probabilistic.observable_to_expvals,\n            prior=prior_fn,\n        )\n        posterior_model = sq.probabilistic.make_probabilistic_model(\n            predictive_model=graybox_model, log_expectation_values=True\n        )\n        ```\n\n    Returns:\n        typing.Callable[[str, tuple[int, ...]], dist.Distribution]: The function that return posterior distribution\n    \"\"\"\n    posterior_samples = Predictive(model=guide, params=params, num_samples=num_samples)(\n        key\n    )\n\n    def posterior_dist_fn(name: str, shape: tuple[int, ...]) -&gt; dist.Distribution:\n        site_name = prefix + \"/\" + name\n        return construct_normal_prior_from_samples(posterior_samples)[site_name]\n\n    return posterior_dist_fn\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_predictive_fn","title":"make_predictive_fn","text":"<pre><code>make_predictive_fn(posterior_model, learning_model: LearningModel)\n</code></pre> <p>Construct predictive model from the probabilsitic model. This function does not relied on guide and the variational parameters</p> <p>Examples:</p> <pre><code>characterized_result = sq.probabilistic.SVIResult.from_file(\n    PGM_model_path / \"model.json\"\n)\n\nbase_model = sq.models.linen.WoModel(\n    hidden_sizes_1=characterized_result.config[\"model_config\"][\"hidden_sizes\"][0],\n    hidden_sizes_2=characterized_result.config[\"model_config\"][\"hidden_sizes\"][1],\n)\ngraybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(\n    name=\"graybox\",\n    base_model=base_model,\n    adapter_fn=sq.probabilistic.observable_to_expvals,\n    prior=dist.Normal(0, 1),\n)\nmodel = sq.probabilistic.make_probabilistic_model(\n    graybox_probabilistic_model=graybox_model,\n)\n# initialize guide\nguide = sq.probabilistic.auto_diagonal_normal_guide(\n    model,\n    ml.custom_feature_map(loaded_data.control_parameters),\n    loaded_data.unitaries,\n    jnp.zeros(shape=(shots, loaded_data.control_parameters.shape[0], 18)),\n)\npriors = {\n    k.strip(\"graybox/\"): v\n    for k, v in make_prior_from_params(guide, characterized_result.params).items()\n}\ngraybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(\n    name=\"graybox\",\n    base_model=base_model,\n    adapter_fn=sq.probabilistic.observable_to_expvals,\n    prior=priors,\n)\nposterior_model = sq.probabilistic.make_probabilistic_model(\n    graybox_probabilistic_model=graybox_model,\n    shots=shots,\n    block_graybox=True,\n    log_expectation_values=True,\n)\npredicetive_fn = sq.probabilistic.make_predictive_fn(\n    posterior_model, sq.probabilistic.LearningModel.BernoulliProbs\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>posterior_model</code> <code>Any</code> <p>probabilsitic model</p> required <code>learning_model</code> <code>LearningModel</code> <p>description</p> required Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_predictive_fn(\n    posterior_model,\n    learning_model: LearningModel,\n):\n    \"\"\"Construct predictive model from the probabilsitic model.\n    This function does not relied on guide and the variational parameters\n\n    Examples:\n        ```python\n        characterized_result = sq.probabilistic.SVIResult.from_file(\n            PGM_model_path / \"model.json\"\n        )\n\n        base_model = sq.models.linen.WoModel(\n            hidden_sizes_1=characterized_result.config[\"model_config\"][\"hidden_sizes\"][0],\n            hidden_sizes_2=characterized_result.config[\"model_config\"][\"hidden_sizes\"][1],\n        )\n        graybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(\n            name=\"graybox\",\n            base_model=base_model,\n            adapter_fn=sq.probabilistic.observable_to_expvals,\n            prior=dist.Normal(0, 1),\n        )\n        model = sq.probabilistic.make_probabilistic_model(\n            graybox_probabilistic_model=graybox_model,\n        )\n        # initialize guide\n        guide = sq.probabilistic.auto_diagonal_normal_guide(\n            model,\n            ml.custom_feature_map(loaded_data.control_parameters),\n            loaded_data.unitaries,\n            jnp.zeros(shape=(shots, loaded_data.control_parameters.shape[0], 18)),\n        )\n        priors = {\n            k.strip(\"graybox/\"): v\n            for k, v in make_prior_from_params(guide, characterized_result.params).items()\n        }\n        graybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(\n            name=\"graybox\",\n            base_model=base_model,\n            adapter_fn=sq.probabilistic.observable_to_expvals,\n            prior=priors,\n        )\n        posterior_model = sq.probabilistic.make_probabilistic_model(\n            graybox_probabilistic_model=graybox_model,\n            shots=shots,\n            block_graybox=True,\n            log_expectation_values=True,\n        )\n        predicetive_fn = sq.probabilistic.make_predictive_fn(\n            posterior_model, sq.probabilistic.LearningModel.BernoulliProbs\n        )\n        ```\n\n    Args:\n        posterior_model (typing.Any): probabilsitic model\n        learning_model (LearningModel): _description_\n    \"\"\"\n\n    def binary_predict_expectation_values(\n        key: jnp.ndarray,\n        control_params: jnp.ndarray,\n        unitary: jnp.ndarray,\n    ) -&gt; jnp.ndarray:\n        return jnp.mean(\n            binary_to_eigenvalue(\n                handlers.seed(posterior_model, key)(  # type: ignore\n                    control_params, unitary\n                )\n            ),\n            axis=0,\n        )\n\n    def normal_predict_expectation_values(\n        key: jnp.ndarray,\n        control_params: jnp.ndarray,\n        unitary: jnp.ndarray,\n    ) -&gt; jnp.ndarray:\n        return handlers.seed(posterior_model, key)(  # type: ignore\n            control_params, unitary\n        )\n\n    return (\n        binary_predict_expectation_values\n        if learning_model == LearningModel.BernoulliProbs\n        else normal_predict_expectation_values\n    )\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_pdf","title":"make_pdf","text":"<pre><code>make_pdf(sample: ndarray, bins: int, srange=(-1, 1))\n</code></pre> <p>Make the numberical PDF from given sample using histogram method</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>ndarray</code> <p>Sample to make PDF.</p> required <code>bins</code> <code>int</code> <p>The number of interval bin.</p> required <code>srange</code> <code>tuple</code> <p>The range of the pdf. Defaults to (-1, 1).</p> <code>(-1, 1)</code> <p>Returns:</p> Type Description <p>typing.Any: The approximated numerical PDF</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_pdf(sample: jnp.ndarray, bins: int, srange=(-1, 1)):\n    \"\"\"Make the numberical PDF from given sample using histogram method\n\n    Args:\n        sample (jnp.ndarray): Sample to make PDF.\n        bins (int): The number of interval bin.\n        srange (tuple, optional): The range of the pdf. Defaults to (-1, 1).\n\n    Returns:\n        typing.Any: The approximated numerical PDF\n    \"\"\"\n    density, bin_edges = jnp.histogram(sample, bins=bins, range=srange, density=True)\n    dx = jnp.diff(bin_edges)\n    p = density * dx\n    return p\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.safe_kl_divergence","title":"safe_kl_divergence","text":"<pre><code>safe_kl_divergence(p: ndarray, q: ndarray)\n</code></pre> <p>Calculate the KL divergence where the infinity is converted to zero.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The left PDF</p> required <code>q</code> <code>ndarray</code> <p>The right PDF</p> required <p>Returns:</p> Type Description <p>jnp.ndarray: The KL divergence</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def safe_kl_divergence(p: jnp.ndarray, q: jnp.ndarray):\n    \"\"\"Calculate the KL divergence where the infinity is converted to zero.\n\n    Args:\n        p (jnp.ndarray): The left PDF\n        q (jnp.ndarray): The right PDF\n\n    Returns:\n        jnp.ndarray: The KL divergence\n    \"\"\"\n    return jnp.sum(jnp.nan_to_num(jax.scipy.special.rel_entr(p, q), posinf=0.0))\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.kl_divergence","title":"kl_divergence","text":"<pre><code>kl_divergence(p: ndarray, q: ndarray)\n</code></pre> <p>Calculate the KL divergence</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The left PDF</p> required <code>q</code> <code>ndarray</code> <p>The right PDF</p> required <p>Returns:</p> Type Description <p>jnp.ndarray:  The KL divergence</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def kl_divergence(p: jnp.ndarray, q: jnp.ndarray):\n    \"\"\"Calculate the KL divergence\n\n    Args:\n        p (jnp.ndarray): The left PDF\n        q (jnp.ndarray): The right PDF\n\n    Returns:\n        jnp.ndarray:  The KL divergence\n    \"\"\"\n    return jnp.sum(jax.scipy.special.rel_entr(p, q))\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.safe_jensenshannon_divergence","title":"safe_jensenshannon_divergence","text":"<pre><code>safe_jensenshannon_divergence(p: ndarray, q: ndarray)\n</code></pre> <p>Calculate Jensen-Shannon Divergnece using KL divergence. Implement following: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The left PDF</p> required <code>q</code> <code>ndarray</code> <p>The right PDF</p> required <p>Returns:</p> Type Description <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def safe_jensenshannon_divergence(p: jnp.ndarray, q: jnp.ndarray):\n    \"\"\"Calculate Jensen-Shannon Divergnece using KL divergence.\n    Implement following: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n\n    Args:\n        p (jnp.ndarray): The left PDF\n        q (jnp.ndarray): The right PDF\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n    # Compute pointwise mean of p and q\n    m = (p + q) / 2\n    return (safe_kl_divergence(p, m) + safe_kl_divergence(q, m)) / 2\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.jensenshannon_divergence_from_pmf","title":"jensenshannon_divergence_from_pmf","text":"<pre><code>jensenshannon_divergence_from_pmf(p: ndarray, q: ndarray)\n</code></pre> <p>Calculate the Jensen-Shannon Divergence from PMF</p> <p>Example <pre><code>key = jax.random.key(0)\nkey_1, key_2 = jax.random.split(key)\nsample_1 = jax.random.normal(key_1, shape=(10000, ))\nsample_2 = jax.random.normal(key_2, shape=(10000, ))\n\n# Determine srange from sample\nmerged_sample = jnp.concat([sample_1, sample_2])\nsrange = jnp.min(merged_sample), jnp.max(merged_sample)\n\n# https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width\n# Recommend this book: https://catalog.lib.uchicago.edu/vufind/Record/6093380/TOC\nbins = int(2 * (sample_2.shape[0]) ** (1/3))\n# bins = 10\ndis_1 = sq.probabilistic.make_pdf(sample_1, bins=bins, srange=srange)\ndis_2 = sq.probabilistic.make_pdf(sample_2, bins=bins, srange=srange)\n\njsd = sq.probabilistic.jensenshannon_divergence_from_pdf(dis_1, dis_2)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>ndarray</code> <p>The 1st probability mass function</p> required <code>q</code> <code>ndarray</code> <p>The 1st probability mass function</p> required <p>Returns:</p> Type Description <p>jnp.ndarray: The Jensen-Shannon Divergence of p and q</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def jensenshannon_divergence_from_pmf(p: jnp.ndarray, q: jnp.ndarray):\n    \"\"\"Calculate the Jensen-Shannon Divergence from PMF\n\n    Example\n    ```python\n    key = jax.random.key(0)\n    key_1, key_2 = jax.random.split(key)\n    sample_1 = jax.random.normal(key_1, shape=(10000, ))\n    sample_2 = jax.random.normal(key_2, shape=(10000, ))\n\n    # Determine srange from sample\n    merged_sample = jnp.concat([sample_1, sample_2])\n    srange = jnp.min(merged_sample), jnp.max(merged_sample)\n\n    # https://stats.stackexchange.com/questions/510699/discrete-kl-divergence-with-decreasing-bin-width\n    # Recommend this book: https://catalog.lib.uchicago.edu/vufind/Record/6093380/TOC\n    bins = int(2 * (sample_2.shape[0]) ** (1/3))\n    # bins = 10\n    dis_1 = sq.probabilistic.make_pdf(sample_1, bins=bins, srange=srange)\n    dis_2 = sq.probabilistic.make_pdf(sample_2, bins=bins, srange=srange)\n\n    jsd = sq.probabilistic.jensenshannon_divergence_from_pdf(dis_1, dis_2)\n\n    ```\n\n    Args:\n        p (jnp.ndarray): The 1st probability mass function\n        q (jnp.ndarray): The 1st probability mass function\n\n    Returns:\n        jnp.ndarray: The Jensen-Shannon Divergence of p and q\n    \"\"\"\n    # Note for JSD: https://medium.com/data-science/how-to-understand-and-use-jensen-shannon-divergence-b10e11b03fd6\n    # Implement following: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n    # Compute pointwise mean of p and q\n    m = (p + q) / 2\n    return (kl_divergence(p, m) + kl_divergence(q, m)) / 2\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.jensenshannon_divergence_from_sample","title":"jensenshannon_divergence_from_sample","text":"<pre><code>jensenshannon_divergence_from_sample(sample_1: ndarray, sample_2: ndarray) -&gt; ndarray\n</code></pre> <p>Calculate the Jensen-Shannon Divergence from sample</p> <p>Parameters:</p> Name Type Description Default <code>sample_1</code> <code>ndarray</code> <p>The left PDF</p> required <code>sample_2</code> <code>ndarray</code> <p>The right PDF</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The Jensen-Shannon Divergence of p and q</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def jensenshannon_divergence_from_sample(\n    sample_1: jnp.ndarray, sample_2: jnp.ndarray\n) -&gt; jnp.ndarray:\n    \"\"\"Calculate the Jensen-Shannon Divergence from sample\n\n    Args:\n        sample_1 (jnp.ndarray): The left PDF\n        sample_2 (jnp.ndarray): The right PDF\n\n    Returns:\n        jnp.ndarray: The Jensen-Shannon Divergence of p and q\n    \"\"\"\n    merged_sample = jnp.concat([sample_1, sample_2])\n    bins = int(2 * (sample_2.shape[0]) ** (1 / 3))\n    srange = jnp.min(merged_sample), jnp.max(merged_sample)\n\n    dis_1 = make_pdf(sample_1, bins=bins, srange=srange)\n    dis_2 = make_pdf(sample_2, bins=bins, srange=srange)\n\n    return jensenshannon_divergence_from_pmf(dis_1, dis_2)\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.batched_matmul","title":"batched_matmul","text":"<pre><code>batched_matmul(x, w, b)\n</code></pre> <p>A specialized batched matrix multiplication of weight and input x, then add the bias. This function is intended to be used in <code>dense_layer</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The input x</p> required <code>w</code> <code>ndarray</code> <p>The weight to multiply to x</p> required <code>b</code> <code>ndarray</code> <p>The bias</p> required <p>Returns:</p> Type Description <p>jnp.ndarray: Output of the operations.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def batched_matmul(x, w, b):\n    \"\"\"A specialized batched matrix multiplication of weight and input x, then add the bias.\n    This function is intended to be used in `dense_layer`\n\n    Args:\n        x (jnp.ndarray): The input x\n        w (jnp.ndarray): The weight to multiply to x\n        b (jnp.ndarray): The bias\n\n    Returns:\n        jnp.ndarray: Output of the operations.\n    \"\"\"\n    return jnp.einsum(x, (..., 0), w, (..., 0, 1), (..., 1)) + b\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.get_trace","title":"get_trace","text":"<pre><code>get_trace(fn, key=key(0))\n</code></pre> <p>Convinent function to get a trace of the probabilistic model in numpyro.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>function</code> <p>The probabilistic model in numpyro.</p> required <code>key</code> <code>ndarray</code> <p>The random key. Defaults to jax.random.key(0).</p> <code>key(0)</code> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def get_trace(fn, key=jax.random.key(0)):\n    \"\"\"Convinent function to get a trace of the probabilistic model in numpyro.\n\n    Args:\n        fn (function): The probabilistic model in numpyro.\n        key (jnp.ndarray, optional): The random key. Defaults to jax.random.key(0).\n    \"\"\"\n\n    def inner(*args, **kwargs):\n        return handlers.trace(handlers.seed(fn, key)).get_trace(*args, **kwargs)\n\n    return inner\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.default_priors_fn","title":"default_priors_fn","text":"<pre><code>default_priors_fn(name: str, shape: tuple[int, ...]) -&gt; Distribution\n</code></pre> <p>This is a default prior function for the <code>dense_layer</code></p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The site name of the parameters, if end with <code>sigma</code> will return Log Normal distribution,               otherwise, return Normal distribution</p> required <p>Returns:</p> Type Description <code>Distribution</code> <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def default_priors_fn(name: str, shape: tuple[int, ...]) -&gt; dist.Distribution:\n    \"\"\"This is a default prior function for the `dense_layer`\n\n    Args:\n        name (str): The site name of the parameters, if end with `sigma` will return Log Normal distribution,\n                          otherwise, return Normal distribution\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n    if name.endswith(\"bias\"):\n        return dist.LogNormal(0, 1).expand(shape)\n\n    return dist.Normal(0, 1).expand(shape)\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.dense_layer","title":"dense_layer","text":"<pre><code>dense_layer(x: ndarray, name: str, in_features: int, out_features: int, priors_fn: Callable[[str, tuple[int, ...]], Distribution] = default_priors_fn)\n</code></pre> <p>A custom probabilistic dense layer for neural network model. This function intended to be used with <code>numpyro</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The input x</p> required <code>name</code> <code>str</code> <p>Site name of the layer</p> required <code>in_features</code> <code>int</code> <p>The size of the feature.</p> required <code>out_features</code> <code>int</code> <p>The desired size of the output feature.</p> required <code>priors_fn</code> <code>Callable[[str], Distribution]</code> <p>The prior function to be used for initializing prior distribution. Defaults to default_priors_fn.</p> <code>default_priors_fn</code> <p>Returns:</p> Type Description <p>typing.Any: Output of the layer given x.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def dense_layer(\n    x: jnp.ndarray,\n    name: str,\n    in_features: int,\n    out_features: int,\n    priors_fn: typing.Callable[\n        [str, tuple[int, ...]], dist.Distribution\n    ] = default_priors_fn,\n):\n    \"\"\"A custom probabilistic dense layer for neural network model.\n    This function intended to be used with `numpyro`\n\n    Args:\n        x (jnp.ndarray): The input x\n        name (str): Site name of the layer\n        in_features (int): The size of the feature.\n        out_features (int): The desired size of the output feature.\n        priors_fn (typing.Callable[[str], dist.Distribution], optional): The prior function to be used for initializing prior distribution. Defaults to default_priors_fn.\n\n    Returns:\n        typing.Any: Output of the layer given x.\n    \"\"\"\n    w_name = f\"{name}.kernel\"\n    w = numpyro.sample(\n        w_name,\n        priors_fn(w_name, (in_features, out_features)).to_event(2),  # type: ignore\n    )\n    b_name = f\"{name}.bias\"\n    b = numpyro.sample(\n        b_name,\n        priors_fn(b_name, (out_features,)).to_event(1),  # type: ignore\n    )\n    return batched_matmul(x, w, b)  # type: ignore\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.init_default","title":"init_default","text":"<pre><code>init_default(params_name: str)\n</code></pre> <p>The initialization function for deterministic dense layer</p> <p>Parameters:</p> Name Type Description Default <code>params_name</code> <code>str</code> <p>The site name</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Unsupport site name</p> <p>Returns:</p> Type Description <p>typing.Any: The function to be used for parameters init given site name.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def init_default(params_name: str):\n    \"\"\"The initialization function for deterministic dense layer\n\n    Args:\n        params_name (str): The site name\n\n    Raises:\n        ValueError: Unsupport site name\n\n    Returns:\n        typing.Any: The function to be used for parameters init given site name.\n    \"\"\"\n    if params_name.endswith(\"kernel\"):\n        return jnp.ones\n    elif params_name.endswith(\"bias\"):\n        return lambda x: 0.1 * jnp.ones(x)\n    else:\n        raise ValueError(\"Unsupport param name\")\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.dense_deterministic_layer","title":"dense_deterministic_layer","text":"<pre><code>dense_deterministic_layer(x, name: str, in_features: int, out_features: int, batch_shape: tuple[int, ...] = (), init_fn=init_default)\n</code></pre> <p>The deterministic dense layer, to be used with SVI optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Any</code> <p>The input feature</p> required <code>name</code> <code>str</code> <p>The site name</p> required <code>in_features</code> <code>int</code> <p>The size of the input features</p> required <code>out_features</code> <code>int</code> <p>The desired size of the output features.</p> required <code>batch_shape</code> <code>tuple[int, ...]</code> <p>The batch size of the x. Defaults to ().</p> <code>()</code> <code>init_fn</code> <code>Any</code> <p>Initilization function of the model parameters. Defaults to init_default.</p> <code>init_default</code> <p>Returns:</p> Type Description <p>typing.Any: The output of the layer given x.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def dense_deterministic_layer(\n    x,\n    name: str,\n    in_features: int,\n    out_features: int,\n    batch_shape: tuple[int, ...] = (),\n    init_fn=init_default,\n):\n    \"\"\"The deterministic dense layer, to be used with SVI optimizer.\n\n    Args:\n        x (typing.Any): The input feature\n        name (str): The site name\n        in_features (int): The size of the input features\n        out_features (int): The desired size of the output features.\n        batch_shape (tuple[int, ...], optional): The batch size of the x. Defaults to ().\n        init_fn (typing.Any, optional): Initilization function of the model parameters. Defaults to init_default.\n\n    Returns:\n        typing.Any: The output of the layer given x.\n    \"\"\"\n    # Sample weights - shape (in_features, out_features)\n    weight_shape = batch_shape + (in_features, out_features)\n    W_name = f\"{name}.kernel\"\n    W = numpyro.param(\n        W_name,\n        init_fn(W_name)(shape=weight_shape),  # type: ignore\n    )\n\n    # Sample bias - shape (out_features,)\n    bias_shape = batch_shape + (out_features,)\n    b_name = f\"{name}.bias\"\n    b = numpyro.param(b_name, init_fn(b_name)(shape=bias_shape))  # type: ignore\n\n    return batched_matmul(x, W, b)  # type: ignore\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_posteriors_fn","title":"make_posteriors_fn","text":"<pre><code>make_posteriors_fn(key: ndarray, guide, params, num_samples=10000)\n</code></pre> <p>Make the posterior distribution function that will return the posterior of parameter of the given name, from guide and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>guide</code> <code>Any</code> <p>The guide function</p> required <code>params</code> <code>Any</code> <p>The parameters of the guide</p> required <code>num_samples</code> <code>int</code> <p>The sample size. Defaults to 10000.</p> <code>10000</code> <p>Returns:</p> Type Description <p>typing.Any: A function of parameter name that return the sample from the posterior distribution of the parameters.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>@deprecated\ndef make_posteriors_fn(key: jnp.ndarray, guide, params, num_samples=10000):\n    \"\"\"Make the posterior distribution function that will\n    return the posterior of parameter of the given name, from guide and parameters.\n\n    Args:\n        guide (typing.Any): The guide function\n        params (typing.Any): The parameters of the guide\n        num_samples (int, optional): The sample size. Defaults to 10000.\n\n    Returns:\n        typing.Any: A function of parameter name that return the sample from the posterior distribution of the parameters.\n    \"\"\"\n    posterior_distribution = Predictive(\n        model=guide, params=params, num_samples=num_samples\n    )(key)\n\n    posterior_dict = construct_normal_prior_from_samples(posterior_distribution)\n\n    def posteriors_fn(param_name: str):\n        return posterior_dict[param_name]\n\n    return posteriors_fn\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.auto_diagonal_normal_guide","title":"auto_diagonal_normal_guide","text":"<pre><code>auto_diagonal_normal_guide(model, *args, block_sample: bool = False, init_loc_fn=zeros, key: ndarray = key(0))\n</code></pre> <p>Automatically generate guide from given model. Expected to be initialized with the example input of the model. The given input should also including the observed site. The blocking capability is intended to be used in the when the guide will be used with its corresponding model in anothe model. This is the avoid site name duplication, while allows for model to use newly sample from the guide.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The probabilistic model.</p> required <code>block_sample</code> <code>bool</code> <p>Flag to block the sample site. Defaults to False.</p> <code>False</code> <code>init_loc_fn</code> <code>Any</code> <p>Initialization of guide parameters function. Defaults to jnp.zeros.</p> <code>zeros</code> <p>Returns:</p> Type Description <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def auto_diagonal_normal_guide(\n    model,\n    *args,\n    block_sample: bool = False,\n    init_loc_fn=jnp.zeros,\n    key: jnp.ndarray = jax.random.key(0),\n):\n    \"\"\"Automatically generate guide from given model. Expected to be initialized with the example input of the model.\n    The given input should also including the observed site.\n    The blocking capability is intended to be used in the when the guide will be used with its corresponding model in anothe model.\n    This is the avoid site name duplication, while allows for model to use newly sample from the guide.\n\n    Args:\n        model (typing.Any): The probabilistic model.\n        block_sample (bool, optional): Flag to block the sample site. Defaults to False.\n        init_loc_fn (typing.Any, optional): Initialization of guide parameters function. Defaults to jnp.zeros.\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n    model_trace = handlers.trace(handlers.seed(model, key)).get_trace(*args)\n    # get the trace of the model\n    # Then get only the sample site with observed equal to false\n    sample_sites = [v for k, v in model_trace.items() if v[\"type\"] == \"sample\"]\n    non_observed_sites = [v for v in sample_sites if not v[\"is_observed\"]]\n    params_sites = [\n        {\"name\": v[\"name\"], \"shape\": v[\"value\"].shape} for v in non_observed_sites\n    ]\n\n    def guide(\n        *args,\n        **kwargs,\n    ):\n        params_loc = {\n            param[\"name\"]: numpyro.param(\n                f\"{param['name']}_loc\", init_loc_fn(param[\"shape\"])\n            )\n            for param in params_sites\n        }\n\n        params_scale = {\n            param[\"name\"]: numpyro.param(\n                f\"{param['name']}_scale\",\n                0.1 * jnp.ones(param[\"shape\"]),\n                constraint=dist.constraints.softplus_positive,\n            )\n            for param in params_sites\n        }\n\n        samples = {}\n\n        if block_sample:\n            with handlers.block():\n                # Sample from Normal distribution\n                for (k_loc, v_loc), (k_scale, v_scale) in zip(\n                    params_loc.items(), params_scale.items(), strict=True\n                ):\n                    s = numpyro.sample(\n                        k_loc,\n                        dist.Normal(v_loc, v_scale).to_event(),  # type: ignore\n                    )\n                    samples[k_loc] = s\n        else:\n            # Sample from Normal distribution\n            for (k_loc, v_loc), (k_scale, v_scale) in zip(\n                params_loc.items(), params_scale.items(), strict=True\n            ):\n                s = numpyro.sample(\n                    k_loc,\n                    dist.Normal(v_loc, v_scale).to_event(),  # type: ignore\n                )\n                samples[k_loc] = s\n\n        return samples\n\n    return guide\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.auto_diagonal_normal_guide_v2","title":"auto_diagonal_normal_guide_v2","text":"<pre><code>auto_diagonal_normal_guide_v2(model, *args, init_dist_fn=init_normal_dist_fn, init_params_fn=init_params_fn, block_sample: bool = False, key: ndarray = key(0))\n</code></pre> <p>Automatically generate guide from given model. Expected to be initialized with the example input of the model. The given input should also including the observed site. The blocking capability is intended to be used in the when the guide will be used with its corresponding model in anothe model. This is the avoid site name duplication, while allows for model to use newly sample from the guide.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The probabilistic model.</p> required <code>block_sample</code> <code>bool</code> <p>Flag to block the sample site. Defaults to False.</p> <code>False</code> <code>init_loc_fn</code> <code>Any</code> <p>Initialization of guide parameters function. Defaults to jnp.zeros.</p> required <p>Returns:</p> Type Description <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def auto_diagonal_normal_guide_v2(\n    model,\n    *args,\n    init_dist_fn=init_normal_dist_fn,\n    init_params_fn=init_params_fn,\n    block_sample: bool = False,\n    key: jnp.ndarray = jax.random.key(0),\n):\n    \"\"\"Automatically generate guide from given model. Expected to be initialized with the example input of the model.\n    The given input should also including the observed site.\n    The blocking capability is intended to be used in the when the guide will be used with its corresponding model in anothe model.\n    This is the avoid site name duplication, while allows for model to use newly sample from the guide.\n\n    Args:\n        model (typing.Any): The probabilistic model.\n        block_sample (bool, optional): Flag to block the sample site. Defaults to False.\n        init_loc_fn (typing.Any, optional): Initialization of guide parameters function. Defaults to jnp.zeros.\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n    # get the trace of the model\n    model_trace = handlers.trace(handlers.seed(model, key)).get_trace(*args)\n    # Then get only the sample site with observed equal to false\n    sample_sites = [v for k, v in model_trace.items() if v[\"type\"] == \"sample\"]\n    non_observed_sites = [v for v in sample_sites if not v[\"is_observed\"]]\n    params_sites = [\n        {\"name\": v[\"name\"], \"shape\": v[\"value\"].shape} for v in non_observed_sites\n    ]\n\n    def sample_fn(\n        params_loc: dict[str, typing.Any], params_scale: dict[str, typing.Any]\n    ):\n        samples = {}\n        # Sample from Normal distribution\n        for (k_loc, v_loc), (k_scale, v_scale) in zip(\n            params_loc.items(), params_scale.items(), strict=True\n        ):\n            s = numpyro.sample(\n                k_loc,\n                init_dist_fn(k_loc)(v_loc, v_scale).to_event(),  # type: ignore\n            )\n            samples[k_loc] = s\n\n        return samples\n\n    def guide(\n        *args,\n        **kwargs,\n    ):\n        params_loc = {\n            param[\"name\"]: init_params_fn(f\"{param['name']}_loc\", param[\"shape\"])\n            for param in params_sites\n        }\n\n        params_scale = {\n            param[\"name\"]: init_params_fn(f\"{param['name']}_scale\", param[\"shape\"])\n            for param in params_sites\n        }\n\n        if block_sample:\n            with handlers.block():\n                samples = sample_fn(params_loc, params_scale)\n        else:\n            samples = sample_fn(params_loc, params_scale)\n\n        return samples\n\n    return guide\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_predictive_fn_v2","title":"make_predictive_fn_v2","text":"<pre><code>make_predictive_fn_v2(model, guide, params, shots: int)\n</code></pre> <p>Make a postirior predictive model function from model, guide, SVI parameters, and the number of shots. This version relied explicitly on the guide and variational parameters. It might be slow than the first version.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>Probabilistic model.</p> required <code>guide</code> <code>Any</code> <p>Gudie corresponded to the model</p> required <code>params</code> <code>Any</code> <p>SVI parameters of the guide</p> required <code>shots</code> <code>int</code> <p>The number of shots</p> required <p>Returns:</p> Type Description <p>typing.Any: The posterior predictive model.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_predictive_fn_v2(\n    model,\n    guide,\n    params,\n    shots: int,\n):\n    \"\"\"Make a postirior predictive model function from model, guide, SVI parameters, and the number of shots.\n    This version relied explicitly on the guide and variational parameters. It might be slow than the first version.\n\n    Args:\n        model (typing.Any): Probabilistic model.\n        guide (typing.Any): Gudie corresponded to the model\n        params (typing.Any): SVI parameters of the guide\n        shots (int): The number of shots\n\n    Returns:\n        typing.Any: The posterior predictive model.\n    \"\"\"\n    predictive = Predictive(\n        model, guide=guide, params=params, num_samples=shots, return_sites=[\"obs\"]\n    )\n\n    def predictive_fn(*args, **kwargs):\n        return predictive(*args, **kwargs)[\"obs\"]\n\n    return predictive_fn\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_predictive_SGM_model","title":"make_predictive_SGM_model","text":"<pre><code>make_predictive_SGM_model(model: Module, model_params, output_to_expectation_values_fn, shots: int)\n</code></pre> <p>Make a predictive model from given SGM model, the model parameters, and number of shots.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Flax model</p> required <code>model_params</code> <code>Any</code> <p>The model parameters.</p> required <code>shots</code> <code>int</code> <p>The number of shots.</p> required Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>@deprecated\ndef make_predictive_SGM_model(\n    model: nn.Module, model_params, output_to_expectation_values_fn, shots: int\n):\n    \"\"\"Make a predictive model from given SGM model, the model parameters, and number of shots.\n\n    Args:\n        model (nn.Module): Flax model\n        model_params (typing.Any): The model parameters.\n        shots (int): The number of shots.\n    \"\"\"\n\n    def predictive_model(\n        key: jnp.ndarray, control_param: jnp.ndarray, unitaries: jnp.ndarray\n    ):\n        output = model.apply(model_params, control_param)\n        predicted_expvals = output_to_expectation_values_fn(output, unitaries)\n\n        return binary_to_eigenvalue(\n            jax.vmap(jax.random.bernoulli, in_axes=(0, None))(\n                jax.random.split(key, shots),\n                expectation_value_to_prob_minus(predicted_expvals),\n            ).astype(jnp.int_)\n        ).mean(axis=0)\n\n    return predictive_model\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_predictive_MCDGM_model","title":"make_predictive_MCDGM_model","text":"<pre><code>make_predictive_MCDGM_model(model: Module, model_params)\n</code></pre> <p>Make a predictive model from given Monte-Carlo Dropout Graybox model, and the model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Monte-Carlo Dropout Graybox model</p> required <code>model_params</code> <code>Any</code> <p>The model parameters</p> required Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_predictive_MCDGM_model(model: nn.Module, model_params):\n    \"\"\"Make a predictive model from given Monte-Carlo Dropout Graybox model, and the model parameters.\n\n    Args:\n        model (nn.Module): Monte-Carlo Dropout Graybox model\n        model_params (typing.Any): The model parameters\n    \"\"\"\n\n    def predictive_model(\n        key: jnp.ndarray, control_param: jnp.ndarray, unitaries: jnp.ndarray\n    ):\n        wo_params = model.apply(\n            model_params,\n            control_param,\n            rngs={\"dropout\": key},\n        )\n\n        predicted_expvals = get_predict_expectation_value(\n            wo_params,  # type: ignore\n            unitaries,\n            default_expectation_values_order,\n        )\n\n        return predicted_expvals\n\n    return predictive_model\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_predictive_resampling_model","title":"make_predictive_resampling_model","text":"<pre><code>make_predictive_resampling_model(predictive_fn: Callable[[ndarray, ndarray], ndarray], shots: int) -&gt; Callable[[ndarray, ndarray, ndarray], ndarray]\n</code></pre> <p>Make a binary predictive model from given SGM model, the model parameters, and number of shots.</p> <p>Parameters:</p> Name Type Description Default <code>predictive_fn</code> <code>Callable[[ndarray, ndarray], ndarray]</code> <p>The predictive_fn embeded with the SGM model.</p> required <code>shots</code> <code>int</code> <p>The number of shots.</p> required <p>Returns:</p> Type Description <code>Callable[[ndarray, ndarray, ndarray], ndarray]</code> <p>typing.Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray], jnp.ndarray]: Binary predictive model.</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_predictive_resampling_model(\n    predictive_fn: typing.Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray], shots: int\n) -&gt; typing.Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray], jnp.ndarray]:\n    \"\"\"Make a binary predictive model from given SGM model, the model parameters, and number of shots.\n\n    Args:\n        predictive_fn (typing.Callable[[jnp.ndarray, jnp.ndarray], jnp.ndarray]): The predictive_fn embeded with the SGM model.\n        shots (int): The number of shots.\n\n    Returns:\n        typing.Callable[[jnp.ndarray, jnp.ndarray, jnp.ndarray], jnp.ndarray]: Binary predictive model.\n    \"\"\"\n\n    def predictive_model(\n        key: jnp.ndarray, control_parameters: jnp.ndarray, unitaries: jnp.ndarray\n    ):\n        predicted_expvals = predictive_fn(control_parameters, unitaries)\n\n        return binary_to_eigenvalue(\n            jax.vmap(jax.random.bernoulli, in_axes=(0, None))(\n                jax.random.split(key, shots),\n                expectation_value_to_prob_minus(predicted_expvals),\n            ).astype(jnp.int_)\n        ).mean(axis=0)\n\n    return predictive_model\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_probabilistic_graybox_model","title":"make_probabilistic_graybox_model","text":"<pre><code>make_probabilistic_graybox_model(model, adapter_fn)\n</code></pre> <p>This function make a probabilistic graybox model using custom numpyro BNN model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>_type_</code> <p>description</p> required <code>adapter_fn</code> <code>_type_</code> <p>description</p> required Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_probabilistic_graybox_model(model, adapter_fn):\n    \"\"\"This function make a probabilistic graybox model using custom numpyro BNN model\n\n    Args:\n        model (_type_): _description_\n        adapter_fn (_type_): _description_\n    \"\"\"\n\n    def probabilistic_graybox_model(control_parameters, unitaries):\n        samples_shape = control_parameters.shape[:-2]\n        unitaries = jnp.broadcast_to(unitaries, samples_shape + unitaries.shape[-3:])\n\n        # Predict from control parameters\n        output = model(control_parameters)\n\n        numpyro.deterministic(\"output\", output)\n\n        # With unitary and Wo, calculate expectation values\n        expvals = adapter_fn(output, unitaries)\n\n        return expvals\n\n    return probabilistic_graybox_model\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.auto_diagonal_normal_guide_v3","title":"auto_diagonal_normal_guide_v3","text":"<pre><code>auto_diagonal_normal_guide_v3(model, *args, init_dist_fn=bnn_init_dist_fn, init_params_fn=bnn_init_params_fn, dist_transform_fn=default_transform_dist_fn, block_sample: bool = False, key: ndarray = key(0))\n</code></pre> <p>Automatically generate guide from given model. Expected to be initialized with the example input of the model. The given input should also including the observed site. The blocking capability is intended to be used in the when the guide will be used with its corresponding model in anothe model. This is the avoid site name duplication, while allows for model to use newly sample from the guide.</p> Notes <p>This version enable even more flexible initialization strategy. This function intended to be able to be compatible with auto marginal guide.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Any</code> <p>The probabilistic model.</p> required <code>block_sample</code> <code>bool</code> <p>Flag to block the sample site. Defaults to False.</p> <code>False</code> <code>init_loc_fn</code> <code>Any</code> <p>Initialization of guide parameters function. Defaults to jnp.zeros.</p> required <p>Returns:</p> Type Description <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def auto_diagonal_normal_guide_v3(\n    model,\n    *args,\n    init_dist_fn=bnn_init_dist_fn,\n    init_params_fn=bnn_init_params_fn,\n    dist_transform_fn=default_transform_dist_fn,\n    block_sample: bool = False,\n    key: jnp.ndarray = jax.random.key(0),\n):\n    \"\"\"Automatically generate guide from given model. Expected to be initialized with the example input of the model.\n    The given input should also including the observed site.\n    The blocking capability is intended to be used in the when the guide will be used with its corresponding model in anothe model.\n    This is the avoid site name duplication, while allows for model to use newly sample from the guide.\n\n    Notes:\n        This version enable even more flexible initialization strategy.\n        This function intended to be able to be compatible with auto marginal guide.\n\n    Args:\n        model (typing.Any): The probabilistic model.\n        block_sample (bool, optional): Flag to block the sample site. Defaults to False.\n        init_loc_fn (typing.Any, optional): Initialization of guide parameters function. Defaults to jnp.zeros.\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n    # get the trace of the model\n    model_trace = handlers.trace(handlers.seed(model, key)).get_trace(*args)\n    # Then get only the sample site with observed equal to false\n    sample_sites = [v for k, v in model_trace.items() if v[\"type\"] == \"sample\"]\n    non_observed_sites = [v for v in sample_sites if not v[\"is_observed\"]]\n    params_sites = [\n        {\"name\": v[\"name\"], \"shape\": v[\"value\"].shape} for v in non_observed_sites\n    ]\n\n    def sample_fn(\n        params_loc: dict[str, typing.Any], params_scale: dict[str, typing.Any]\n    ):\n        samples = {}\n        # Sample from Normal distribution\n        for (k_loc, v_loc), (k_scale, v_scale) in zip(\n            params_loc.items(), params_scale.items(), strict=True\n        ):\n            s = numpyro.sample(\n                k_loc,\n                dist_transform_fn(k_loc, init_dist_fn(k_loc)(v_loc, v_scale)),  # type: ignore\n            )\n            samples[k_loc] = s\n\n        return samples\n\n    def guide(\n        *args,\n        **kwargs,\n    ):\n        params_loc = {\n            param[\"name\"]: init_params_fn(f\"{param['name']}_loc\", param[\"shape\"])\n            for param in params_sites\n        }\n\n        params_scale = {\n            param[\"name\"]: init_params_fn(f\"{param['name']}_scale\", param[\"shape\"])\n            for param in params_sites\n        }\n\n        with numpyro.util.optional(block_sample, handlers.block()):\n            samples = sample_fn(params_loc, params_scale)\n\n        return samples\n\n    return guide\n</code></pre>"},{"location":"api/probabilistic/#src.inspeqtor.experimental.probabilistic.make_posterior_fn","title":"make_posterior_fn","text":"<pre><code>make_posterior_fn(params, get_dist_fn: Callable[[str], Any])\n</code></pre> <p>This function create a posterior function to make a posterior predictive model</p> <p>Examples:</p> <pre><code>posterior_model = sq.probabilistic.make_probabilistic_model(\n    predictive_model=partial(\n            probabilistic_graybox_model,\n            priors_fn=make_posterior_fn(result.params, init_bnn_dist_fn),\n        ),\n    log_expectation_values=True,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>_type_</code> <p>The variational parameters from SVI</p> required <code>get_dist_fn</code> <code>Callable[[str], Distribution]</code> <p>The function that return function given name</p> required Source code in <code>src/inspeqtor/experimental/probabilistic.py</code> <pre><code>def make_posterior_fn(params, get_dist_fn: typing.Callable[[str], typing.Any]):\n    \"\"\"This function create a posterior function to make a posterior predictive model\n\n    Examples:\n        ```python\n        posterior_model = sq.probabilistic.make_probabilistic_model(\n            predictive_model=partial(\n                    probabilistic_graybox_model,\n                    priors_fn=make_posterior_fn(result.params, init_bnn_dist_fn),\n                ),\n            log_expectation_values=True,\n        )\n        ```\n\n    Args:\n        params (_type_): The variational parameters from SVI\n        get_dist_fn (typing.Callable[[str], dist.Distribution]): The function that return function given name\n    \"\"\"\n\n    def posterior_fn(name: str, shape: tuple[int, ...]):\n        return get_dist_fn(name)(params[name + \"_loc\"], params[name + \"_scale\"])\n\n    return posterior_fn\n</code></pre>"},{"location":"api/utils/","title":"utils","text":""},{"location":"api/utils/#src.inspeqtor.experimental.utils","title":"src.inspeqtor.experimental.utils","text":""},{"location":"api/utils/#src.inspeqtor.experimental.utils.center_location","title":"center_location","text":"<pre><code>center_location(num_of_pulse: int, total_time_dt: int | float) -&gt; ndarray\n</code></pre> <p>Create an array of location equally that centered each pulse.</p> <p>Parameters:</p> Name Type Description Default <code>num_of_pulse</code> <code>int</code> <p>The number of the pulse in the sequence to be equally center.</p> required <code>total_time_dt</code> <code>int | float</code> <p>The total bins of the sequence.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The array of location equally that centered each pulse.</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def center_location(num_of_pulse: int, total_time_dt: int | float) -&gt; jnp.ndarray:\n    \"\"\"Create an array of location equally that centered each pulse.\n\n    Args:\n        num_of_pulse (int): The number of the pulse in the sequence to be equally center.\n        total_time_dt (int | float): The total bins of the sequence.\n\n    Returns:\n        jnp.ndarray: The array of location equally that centered each pulse.\n    \"\"\"\n    center_locations = (\n        jnp.array([(k - 0.5) / num_of_pulse for k in range(1, num_of_pulse + 1)])\n        * total_time_dt\n    )\n    return center_locations\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.drag_envelope_v2","title":"drag_envelope_v2","text":"<pre><code>drag_envelope_v2(amp: float | ndarray, sigma: float | ndarray, beta: float | ndarray, center: float | ndarray, final_amp: float | ndarray = 1.0)\n</code></pre> <p>Drag pulse following: https://docs.quantum.ibm.com/api/qiskit/qiskit.pulse.library.Drag_class.rst#drag</p> <p>Parameters:</p> Name Type Description Default <code>amp</code> <code>float | ndarray</code> <p>The amplitude of the pulse</p> required <code>sigma</code> <code>float | ndarray</code> <p>The standard deviation of the pulse</p> required <code>beta</code> <code>float | ndarray</code> <p>DRAG coefficient.</p> required <code>center</code> <code>float | ndarray</code> <p>Center location of the pulse</p> required <code>final_amp</code> <code>float | ndarray</code> <p>Final amplitude of the control. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>typing.Callable: DRAG envelope function</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def drag_envelope_v2(\n    amp: float | jnp.ndarray,\n    sigma: float | jnp.ndarray,\n    beta: float | jnp.ndarray,\n    center: float | jnp.ndarray,\n    final_amp: float | jnp.ndarray = 1.0,\n):\n    \"\"\"Drag pulse following: https://docs.quantum.ibm.com/api/qiskit/qiskit.pulse.library.Drag_class.rst#drag\n\n    Args:\n        amp (float | jnp.ndarray): The amplitude of the pulse\n        sigma (float | jnp.ndarray): The standard deviation of the pulse\n        beta (float | jnp.ndarray): DRAG coefficient.\n        center (float | jnp.ndarray): Center location of the pulse\n        final_amp (float | jnp.ndarray, optional): Final amplitude of the control. Defaults to 1.0.\n\n    Returns:\n        typing.Callable: DRAG envelope function\n    \"\"\"\n\n    def g(t):\n        return jnp.exp(-((t - center) ** 2) / (2 * sigma**2))\n\n    def g_prime(t):\n        return amp * (g(t) - g(-1)) / (1 - g(-1))\n\n    def envelop(t):\n        return final_amp * g_prime(t) * (1 + 1j * beta * (t - center) / sigma**2)\n\n    return envelop\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.detune_hamiltonian","title":"detune_hamiltonian","text":"<pre><code>detune_hamiltonian(hamiltonian: Callable[[HamiltonianArgs, ndarray], ndarray], detune: float) -&gt; Callable[[HamiltonianArgs, ndarray], ndarray]\n</code></pre> <p>Detune the Hamiltonian in Z-axis with detuning coefficient</p> <p>Parameters:</p> Name Type Description Default <code>hamiltonian</code> <code>Callable[[HamiltonianArgs, ndarray], ndarray]</code> <p>Hamiltonian function to be detuned</p> required <code>detune</code> <code>float</code> <p>Detuning coefficient</p> required <p>Returns:</p> Type Description <code>Callable[[HamiltonianArgs, ndarray], ndarray]</code> <p>typing.Callable: Detuned Hamiltonian.</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>@warn_not_tested_function\ndef detune_hamiltonian(\n    hamiltonian: typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray],\n    detune: float,\n) -&gt; typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray]:\n    \"\"\"Detune the Hamiltonian in Z-axis with detuning coefficient\n\n    Args:\n        hamiltonian (typing.Callable[[HamiltonianArgs, jnp.ndarray], jnp.ndarray]): Hamiltonian function to be detuned\n        detune (float): Detuning coefficient\n\n    Returns:\n        typing.Callable: Detuned Hamiltonian.\n\n    \"\"\"\n\n    def detuned_hamiltonian(\n        params: HamiltonianArgs,\n        t: jnp.ndarray,\n        *args,\n        **kwargs,\n    ) -&gt; jnp.ndarray:\n        return hamiltonian(params, t, *args, **kwargs) + detune * Z\n\n    return detuned_hamiltonian\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.prepare_data","title":"prepare_data","text":"<pre><code>prepare_data(exp_data: ExperimentData, control_sequence: ControlSequence, whitebox: Callable) -&gt; LoadedData\n</code></pre> <p>Prepare the data for easy accessing from experiment data, control sequence, and Whitebox.</p> <p>Parameters:</p> Name Type Description Default <code>exp_data</code> <code>ExperimentData</code> <p><code>ExperimentData</code> instance</p> required <code>control_sequence</code> <code>ControlSequence</code> <p>Control sequence of the experiment</p> required <code>whitebox</code> <code>Callable</code> <p>Ideal unitary solver.</p> required <p>Returns:</p> Name Type Description <code>LoadedData</code> <code>LoadedData</code> <p><code>LoadedData</code> instance</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def prepare_data(\n    exp_data: ExperimentData,\n    control_sequence: ControlSequence,\n    whitebox: typing.Callable,\n) -&gt; LoadedData:\n    \"\"\"Prepare the data for easy accessing from experiment data, control sequence, and Whitebox.\n\n    Args:\n        exp_data (ExperimentData): `ExperimentData` instance\n        control_sequence (ControlSequence): Control sequence of the experiment\n        whitebox (typing.Callable): Ideal unitary solver.\n\n    Returns:\n        LoadedData: `LoadedData` instance\n    \"\"\"\n    logging.info(f\"Loaded data from {exp_data.experiment_config.EXPERIMENT_IDENTIFIER}\")\n\n    control_parameters = jnp.array(exp_data.parameters)\n    # * Attempt to reshape the control_parameters to (size, features)\n    if len(control_parameters.shape) == 3:\n        control_parameters = control_parameters.reshape(\n            control_parameters.shape[0],\n            control_parameters.shape[1] * control_parameters.shape[2],\n        )\n\n    expectation_values = jnp.array(exp_data.get_expectation_values())\n    unitaries = jax.vmap(whitebox)(control_parameters)\n\n    logging.info(\n        f\"Finished preparing the data for the experiment {exp_data.experiment_config.EXPERIMENT_IDENTIFIER}\"\n    )\n\n    return LoadedData(\n        experiment_data=exp_data,\n        control_parameters=control_parameters,\n        unitaries=unitaries[:, -1, :, :],\n        expectation_values=expectation_values,\n        control_sequence=control_sequence,\n        whitebox=whitebox,\n    )\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.random_split","title":"random_split","text":"<pre><code>random_split(key: ndarray, test_size: int, *data_arrays: ndarray)\n</code></pre> <p>The random_split function splits the data into training and testing sets.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; key = jax.random.key(0)\n&gt;&gt;&gt; x = jnp.arange(10)\n&gt;&gt;&gt; y = jnp.arange(10)\n&gt;&gt;&gt; x_train, y_train, x_test, y_test = random_split(key, 2, x, y)\n&gt;&gt;&gt; assert x_train.shape[0] == 8 and y_train.shape[0] == 8\n&gt;&gt;&gt; assert x_test.shape[0] == 2 and y_test.shape[0] == 2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>Random key.</p> required <code>test_size</code> <code>int</code> <p>The size of the test set. Must be less than the size of the data.</p> required <p>Returns:</p> Type Description <p>typing.Sequence[jnp.ndarray]: The training and testing sets in the same order.</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def random_split(key: jnp.ndarray, test_size: int, *data_arrays: jnp.ndarray):\n    \"\"\"The random_split function splits the data into training and testing sets.\n\n    Examples:\n        &gt;&gt;&gt; key = jax.random.key(0)\n        &gt;&gt;&gt; x = jnp.arange(10)\n        &gt;&gt;&gt; y = jnp.arange(10)\n        &gt;&gt;&gt; x_train, y_train, x_test, y_test = random_split(key, 2, x, y)\n        &gt;&gt;&gt; assert x_train.shape[0] == 8 and y_train.shape[0] == 8\n        &gt;&gt;&gt; assert x_test.shape[0] == 2 and y_test.shape[0] == 2\n\n    Args:\n        key (jnp.ndarray): Random key.\n        test_size (int): The size of the test set. Must be less than the size of the data.\n\n    Returns:\n        typing.Sequence[jnp.ndarray]: The training and testing sets in the same order.\n    \"\"\"\n    # * General random split\n    idx = jax.random.permutation(key, data_arrays[0].shape[0])\n    train_data = []\n    test_data = []\n\n    for data in data_arrays:\n        train_data.append(data[idx][test_size:])\n        test_data.append(data[idx][:test_size])\n\n    return (*train_data, *test_data)\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.dataloader","title":"dataloader","text":"<pre><code>dataloader(arrays: Sequence[ndarray], batch_size: int, num_epochs: int, *, key: ndarray)\n</code></pre> <p>The dataloader function creates a generator that yields batches of data.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>Sequence[ndarray]</code> <p>The list or tuple of arrays to be batched.</p> required <code>batch_size</code> <code>int</code> <p>The size of the batch.</p> required <code>num_epochs</code> <code>int</code> <p>The number of epochs. If set to -1, the generator will run indefinitely.</p> required <code>key</code> <code>ndarray</code> <p>The random key.</p> required <p>Returns:</p> Name Type Description <code>None</code> <p>stop the generator.</p> <p>Yields:</p> Type Description <p>typing.Any: (step, batch_idx, is_last_batch, epoch_idx), (array_batch, ...)</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def dataloader(\n    arrays: typing.Sequence[jnp.ndarray],\n    batch_size: int,\n    num_epochs: int,\n    *,\n    key: jnp.ndarray,\n):\n    \"\"\"The dataloader function creates a generator that yields batches of data.\n\n    Args:\n        arrays (typing.Sequence[jnp.ndarray]): The list or tuple of arrays to be batched.\n        batch_size (int): The size of the batch.\n        num_epochs (int): The number of epochs. If set to -1, the generator will run indefinitely.\n        key (jnp.ndarray): The random key.\n\n    Returns:\n        None: stop the generator.\n\n    Yields:\n        typing.Any: (step, batch_idx, is_last_batch, epoch_idx), (array_batch, ...)\n    \"\"\"\n    # * General dataloader\n    # Check that all arrays have the same size in the first dimension\n    dataset_size = arrays[0].shape[0]\n    # assert all(array.shape[0] == dataset_size for array in arrays)\n    # Generate random indices\n    indices = jnp.arange(dataset_size)\n    step = 0\n    epoch_idx = 0\n    while True:\n        if epoch_idx == num_epochs:\n            return None\n        perm = jax.random.permutation(key, indices)\n        (key,) = jax.random.split(key, 1)\n        batch_idx = 0\n        start = 0\n        end = batch_size\n        is_last_batch = False\n        while not is_last_batch:\n            batch_perm = perm[start:end]\n            # Check if this is the last batch\n            is_last_batch = end &gt;= dataset_size\n            yield (\n                (step, batch_idx, is_last_batch, epoch_idx),\n                tuple(array[batch_perm] for array in arrays),\n            )\n            start = end\n            end = start + batch_size\n            step += 1\n            batch_idx += 1\n\n        epoch_idx += 1\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.expectation_value_to_prob_plus","title":"expectation_value_to_prob_plus","text":"<pre><code>expectation_value_to_prob_plus(expectation_value: ndarray) -&gt; ndarray\n</code></pre> <p>Calculate the probability of -1 and 1 for the given expectation value E[O] = -1 * P[O = -1] + 1 * P[O = 1], where P[O = -1] + P[O = 1] = 1 Thus, E[O] = -1 * (1 - P[O = 1]) + 1 * P[O = 1] E[O] = 2 * P[O = 1] - 1 -&gt; P[O = 1] = (E[O] + 1) / 2 Args:     expectation_value (jnp.ndarray): Expectation value of quantum observable</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Probability of measuring plus eigenvector</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def expectation_value_to_prob_plus(expectation_value: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Calculate the probability of -1 and 1 for the given expectation value\n    E[O] = -1 * P[O = -1] + 1 * P[O = 1], where P[O = -1] + P[O = 1] = 1\n    Thus, E[O] = -1 * (1 - P[O = 1]) + 1 * P[O = 1]\n    E[O] = 2 * P[O = 1] - 1 -&gt; P[O = 1] = (E[O] + 1) / 2\n    Args:\n        expectation_value (jnp.ndarray): Expectation value of quantum observable\n\n    Returns:\n        jnp.ndarray: Probability of measuring plus eigenvector\n    \"\"\"\n\n    return (expectation_value + 1) / 2\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.expectation_value_to_prob_minus","title":"expectation_value_to_prob_minus","text":"<pre><code>expectation_value_to_prob_minus(expectation_value: ndarray) -&gt; ndarray\n</code></pre> <p>Convert quantum observable expectation value to probability of measuring -1.</p> <p>For a binary quantum observable \\(\\hat{O}\\) with eigenvalues \\(b = \\{-1, 1\\}\\), this function calculates the probability of measuring the eigenvalue -1 given its expectation value.</p> <p>Derivation: $$     \\langle \\hat{O} \\rangle = -1 \\cdot \\Pr(b=-1) + 1 \\cdot \\Pr(b = 1) $$     With the constraint \\(\\Pr(b = -1) + \\Pr(b = 1) = 1\\):</p> \\[     \\langle \\hat{O} \\rangle = -1 \\cdot \\Pr(b=-1) + 1 \\cdot (1 - \\Pr(b=-1)) \\     \\langle \\hat{O} \\rangle = -\\Pr(b=-1) + 1 - \\Pr(b=-1) \\     \\langle \\hat{O} \\rangle = 1 - 2\\Pr(b=-1) \\     \\Pr(b=-1) = \\frac{1 - \\langle \\hat{O} \\rangle}{2} \\] <p>Parameters:</p> Name Type Description Default <code>expectation_value</code> <code>ndarray</code> <p>Expectation value of the quantum observable, must be in range [-1, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Probability of measuring the -1 eigenvalue.</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def expectation_value_to_prob_minus(expectation_value: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Convert quantum observable expectation value to probability of measuring -1.\n\n    For a binary quantum observable $\\\\hat{O}$ with eigenvalues $b = \\\\{-1, 1\\\\}$, this function\n    calculates the probability of measuring the eigenvalue -1 given its expectation value.\n\n    Derivation:\n    $$\n        \\\\langle \\\\hat{O} \\\\rangle = -1 \\\\cdot \\\\Pr(b=-1) + 1 \\\\cdot \\\\Pr(b = 1)\n    $$\n        With the constraint $\\\\Pr(b = -1) + \\\\Pr(b = 1) = 1$:\n\n    $$\n        \\\\langle \\\\hat{O} \\\\rangle = -1 \\\\cdot \\\\Pr(b=-1) + 1 \\\\cdot (1 - \\\\Pr(b=-1)) \\\\\n        \\\\langle \\\\hat{O} \\\\rangle = -\\\\Pr(b=-1) + 1 - \\\\Pr(b=-1) \\\\\n        \\\\langle \\\\hat{O} \\\\rangle = 1 - 2\\\\Pr(b=-1) \\\\\n        \\\\Pr(b=-1) = \\\\frac{1 - \\\\langle \\\\hat{O} \\\\rangle}{2}\n    $$\n\n    Args:\n        expectation_value (jnp.ndarray): Expectation value of the quantum observable,\n            must be in range [-1, 1].\n\n    Returns:\n        jnp.ndarray: Probability of measuring the -1 eigenvalue.\n    \"\"\"\n    return (1 - expectation_value) / 2\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.expectation_value_to_eigenvalue","title":"expectation_value_to_eigenvalue","text":"<pre><code>expectation_value_to_eigenvalue(expectation_value: ndarray, SHOTS: int) -&gt; ndarray\n</code></pre> <p>Convert expectation value to eigenvalue</p> <p>Parameters:</p> Name Type Description Default <code>expectation_value</code> <code>ndarray</code> <p>Expectation value of quantum observable</p> required <code>SHOTS</code> <code>int</code> <p>The number of shots used to produce expectation value</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Array of eigenvalues</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def expectation_value_to_eigenvalue(\n    expectation_value: jnp.ndarray, SHOTS: int\n) -&gt; jnp.ndarray:\n    \"\"\"Convert expectation value to eigenvalue\n\n    Args:\n        expectation_value (jnp.ndarray): Expectation value of quantum observable\n        SHOTS (int): The number of shots used to produce expectation value\n\n    Returns:\n        jnp.ndarray: Array of eigenvalues\n    \"\"\"\n    return jnp.where(\n        jnp.broadcast_to(jnp.arange(SHOTS), expectation_value.shape + (SHOTS,))\n        &lt; jnp.around(\n            expectation_value_to_prob_plus(\n                jnp.reshape(expectation_value, expectation_value.shape + (1,))\n            )\n            * SHOTS\n        ).astype(jnp.int32),\n        1,\n        -1,\n    ).astype(jnp.int32)\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.eigenvalue_to_binary","title":"eigenvalue_to_binary","text":"<pre><code>eigenvalue_to_binary(eigenvalue: ndarray) -&gt; ndarray\n</code></pre> <p>Convert -1 to 1, and 0 to 1 This implementation should be differentiable</p> <p>Parameters:</p> Name Type Description Default <code>eigenvalue</code> <code>ndarray</code> <p>Eigenvalue to convert to bit value</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Binary array</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def eigenvalue_to_binary(eigenvalue: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Convert -1 to 1, and 0 to 1\n    This implementation should be differentiable\n\n    Args:\n        eigenvalue (jnp.ndarray): Eigenvalue to convert to bit value\n\n    Returns:\n        jnp.ndarray: Binary array\n    \"\"\"\n\n    return (-1 * eigenvalue + 1) / 2\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.binary_to_eigenvalue","title":"binary_to_eigenvalue","text":"<pre><code>binary_to_eigenvalue(binary: ndarray) -&gt; ndarray\n</code></pre> <p>Convert 1 to -1, and 0 to 1 This implementation should be differentiable</p> <p>Parameters:</p> Name Type Description Default <code>binary</code> <code>ndarray</code> <p>Bit value to convert to eigenvalue</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Eigenvalue array</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def binary_to_eigenvalue(binary: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"Convert 1 to -1, and 0 to 1\n    This implementation should be differentiable\n\n    Args:\n        binary (jnp.ndarray): Bit value to convert to eigenvalue\n\n    Returns:\n        jnp.ndarray: Eigenvalue array\n    \"\"\"\n\n    return -1 * (binary * 2 - 1)\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.recursive_vmap","title":"recursive_vmap","text":"<pre><code>recursive_vmap(func, in_axes)\n</code></pre> <p>Perform recursive vmap on the given axis</p> Note <pre><code>def func(x):\n    assert x.ndim == 1\n    return x ** 2\nx = jnp.arange(10)\nx_test = jnp.broadcast_to(x, (2, 3, 4,) + x.shape)\nx_test.shape, recursive_vmap(func, (0,) * (x_test.ndim - 1))(x_test).shape\n((2, 3, 4, 10), (2, 3, 4, 10))\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def func(x):\n...     assert x.ndim == 1\n...     return x ** 2\n&gt;&gt;&gt; x = jnp.arange(10)\n&gt;&gt;&gt; x_test = jnp.broadcast_to(x, (2, 3, 4,) + x.shape)\n&gt;&gt;&gt; x_test.shape, recursive_vmap(func, (0,) * (x_test.ndim - 1))(x_test).shape\n((2, 3, 4, 10), (2, 3, 4, 10))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Any</code> <p>The function for vmap</p> required <code>in_axes</code> <code>Any</code> <p>The axes for vmap</p> required <p>Returns:</p> Type Description <p>typing.Any: description</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def recursive_vmap(func, in_axes):\n    \"\"\"Perform recursive vmap on the given axis\n\n    Note:\n        ```python\n        def func(x):\n            assert x.ndim == 1\n            return x ** 2\n        x = jnp.arange(10)\n        x_test = jnp.broadcast_to(x, (2, 3, 4,) + x.shape)\n        x_test.shape, recursive_vmap(func, (0,) * (x_test.ndim - 1))(x_test).shape\n        ((2, 3, 4, 10), (2, 3, 4, 10))\n        ```\n\n    Examples:\n        &gt;&gt;&gt; def func(x):\n        ...     assert x.ndim == 1\n        ...     return x ** 2\n        &gt;&gt;&gt; x = jnp.arange(10)\n        &gt;&gt;&gt; x_test = jnp.broadcast_to(x, (2, 3, 4,) + x.shape)\n        &gt;&gt;&gt; x_test.shape, recursive_vmap(func, (0,) * (x_test.ndim - 1))(x_test).shape\n        ((2, 3, 4, 10), (2, 3, 4, 10))\n\n    Args:\n        func (typing.Any): The function for vmap\n        in_axes (typing.Any): The axes for vmap\n\n    Returns:\n        typing.Any: _description_\n    \"\"\"\n    if not in_axes:\n        # Base case: no more axes to vectorize over\n        return func\n\n    # Apply vmap over the first axis specified in in_axes\n    vmap_func = jax.vmap(func, in_axes=in_axes[0])\n\n    # Recursively apply vmap over the remaining axes\n    return recursive_vmap(vmap_func, in_axes[1:])\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.calculate_shots_expectation_value","title":"calculate_shots_expectation_value","text":"<pre><code>calculate_shots_expectation_value(key: ndarray, initial_state: ndarray, unitary: ndarray, operator: ndarray, shots: int) -&gt; ndarray\n</code></pre> <p>Calculate finite-shots estimate of expectation value</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>ndarray</code> <p>Random key</p> required <code>initial_state</code> <code>ndarray</code> <p>Inital state</p> required <code>unitary</code> <code>ndarray</code> <p>Unitary operator</p> required <code>plus_projector</code> <code>ndarray</code> <p>The eigenvector corresponded to +1 eigenvalue of Pauli observable.</p> required <code>shots</code> <code>int</code> <p>Number of shot to be used in estimation of expectation value</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: Finite-shot estimate expectation value</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def calculate_shots_expectation_value(\n    key: jnp.ndarray,\n    initial_state: jnp.ndarray,\n    unitary: jnp.ndarray,\n    operator: jnp.ndarray,\n    shots: int,\n) -&gt; jnp.ndarray:\n    \"\"\"Calculate finite-shots estimate of expectation value\n\n    Args:\n        key (jnp.ndarray): Random key\n        initial_state (jnp.ndarray): Inital state\n        unitary (jnp.ndarray): Unitary operator\n        plus_projector (jnp.ndarray): The eigenvector corresponded to +1 eigenvalue of Pauli observable.\n        shots (int): Number of shot to be used in estimation of expectation value\n\n    Returns:\n        jnp.ndarray: Finite-shot estimate expectation value\n    \"\"\"\n    expval = jnp.trace(unitary @ initial_state @ unitary.conj().T @ operator).real\n    prob = expectation_value_to_prob_plus(expval)\n\n    return jax.random.choice(\n        key, jnp.array([1, -1]), shape=(shots,), p=jnp.array([prob, 1 - prob])\n    ).mean()\n</code></pre>"},{"location":"api/utils/#src.inspeqtor.experimental.utils.shot_quantum_device","title":"shot_quantum_device","text":"<pre><code>shot_quantum_device(key: ndarray, control_parameters: ndarray, solver: Callable[[ndarray], ndarray], SHOTS: int, expectation_value_receipt: list[ExpectationValue] = default_expectation_values_order) -&gt; ndarray\n</code></pre> <p>This is the shot estimate expectation value quantum device</p> <p>Parameters:</p> Name Type Description Default <code>control_parameters</code> <code>ndarray</code> <p>The control parameter to be feed to simlulator</p> required <code>key</code> <code>ndarray</code> <p>Random key</p> required <code>solver</code> <code>Callable[[ndarray], ndarray]</code> <p>The ODE solver for propagator</p> required <code>SHOTS</code> <code>int</code> <p>The number of shots used to estimate expectation values</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>jnp.ndarray: The expectation value of shape (control_parameters.shape[0], 18)</p> Source code in <code>src/inspeqtor/experimental/utils.py</code> <pre><code>def shot_quantum_device(\n    key: jnp.ndarray,\n    control_parameters: jnp.ndarray,\n    solver: typing.Callable[[jnp.ndarray], jnp.ndarray],\n    SHOTS: int,\n    expectation_value_receipt: list[\n        ExpectationValue\n    ] = default_expectation_values_order,\n) -&gt; jnp.ndarray:\n    \"\"\"This is the shot estimate expectation value quantum device\n\n    Args:\n        control_parameters (jnp.ndarray): The control parameter to be feed to simlulator\n        key (jnp.ndarray): Random key\n        solver (typing.Callable[[jnp.ndarray], jnp.ndarray]): The ODE solver for propagator\n        SHOTS (int): The number of shots used to estimate expectation values\n\n    Returns:\n        jnp.ndarray: The expectation value of shape (control_parameters.shape[0], 18)\n    \"\"\"\n\n    expectation_values = jnp.zeros((control_parameters.shape[0], 18))\n    unitaries = jax.vmap(solver)(control_parameters)[:, -1, :, :]\n\n    for idx, exp in enumerate(expectation_value_receipt):\n        key, sample_key = jax.random.split(key)\n        sample_keys = jax.random.split(sample_key, num=unitaries.shape[0])\n\n        expectation_value = jax.vmap(\n            calculate_shots_expectation_value,\n            in_axes=(0, None, 0, None, None),\n        )(\n            sample_keys,\n            exp.initial_density_matrix,\n            unitaries,\n            exp.observable_matrix,\n            SHOTS,\n        )\n\n        expectation_values = expectation_values.at[..., idx].set(expectation_value)\n\n    return expectation_values\n</code></pre>"},{"location":"api/visualization/","title":"visualization","text":""},{"location":"api/visualization/#src.inspeqtor.experimental.visualization","title":"src.inspeqtor.experimental.visualization","text":""},{"location":"api/visualization/#src.inspeqtor.experimental.visualization.format_expectation_values","title":"format_expectation_values","text":"<pre><code>format_expectation_values(expvals: ndarray) -&gt; dict[str, dict[str, ndarray]]\n</code></pre> <p>This function formats expectation values of shape (18, N) to a dictionary with the initial state as outer key and the observable as inner key.</p> <p>Parameters:</p> Name Type Description Default <code>expvals</code> <code>ndarray</code> <p>Expectation values of shape (18, N). Assumes that order is as in default_expectation_values_order.</p> required <p>Returns:</p> Type Description <code>dict[str, dict[str, ndarray]]</code> <p>dict[str, dict[str, jnp.ndarray]]: A dictionary with the initial state as outer key and the observable as inner key.</p> Source code in <code>src/inspeqtor/experimental/visualization.py</code> <pre><code>def format_expectation_values(\n    expvals: jnp.ndarray,\n) -&gt; dict[str, dict[str, jnp.ndarray]]:\n    \"\"\"This function formats expectation values of shape (18, N) to a dictionary\n    with the initial state as outer key and the observable as inner key.\n\n    Args:\n        expvals (jnp.ndarray): Expectation values of shape (18, N). Assumes that order is as in default_expectation_values_order.\n\n    Returns:\n        dict[str, dict[str, jnp.ndarray]]: A dictionary with the initial state as outer key and the observable as inner key.\n    \"\"\"\n    expvals_dict: dict[str, dict[str, jnp.ndarray]] = {}\n    for idx, exp in enumerate(default_expectation_values_order):\n        if exp.initial_state not in expvals_dict:\n            expvals_dict[exp.initial_state] = {}\n\n        expvals_dict[exp.initial_state][exp.observable] = expvals[idx]\n\n    return expvals_dict\n</code></pre>"},{"location":"api/visualization/#src.inspeqtor.experimental.visualization.plot_loss_with_moving_average","title":"plot_loss_with_moving_average","text":"<pre><code>plot_loss_with_moving_average(x: ndarray | ndarray, y: ndarray | ndarray, ax: Axes, window: int = 50, annotate_at: list[float] = [0.2, 0.4, 0.6, 0.8, 1.0], **kwargs) -&gt; Axes\n</code></pre> <p>Plot the moving average of the given argument y</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray | ndarray</code> <p>The horizontal axis</p> required <code>y</code> <code>ndarray | ndarray</code> <p>The vertical axis</p> required <code>ax</code> <code>Axes</code> <p>Axes object</p> required <code>window</code> <code>int</code> <p>The moving average window. Defaults to 50.</p> <code>50</code> <code>annotate_at</code> <code>list[int]</code> <p>The list of x positions to annotate the y value. Defaults to [2000, 4000, 6000, 8000, 10000].</p> <code>[0.2, 0.4, 0.6, 0.8, 1.0]</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>Axes object.</p> Source code in <code>src/inspeqtor/experimental/visualization.py</code> <pre><code>def plot_loss_with_moving_average(\n    x: jnp.ndarray | np.ndarray,\n    y: jnp.ndarray | np.ndarray,\n    ax: Axes,\n    window: int = 50,\n    annotate_at: list[float] = [0.2, 0.4, 0.6, 0.8, 1.0],\n    **kwargs,\n) -&gt; Axes:\n    \"\"\"Plot the moving average of the given argument y\n\n    Args:\n        x (jnp.ndarray | np.ndarray): The horizontal axis\n        y (jnp.ndarray | np.ndarray): The vertical axis\n        ax (Axes): Axes object\n        window (int, optional): The moving average window. Defaults to 50.\n        annotate_at (list[int], optional): The list of x positions to annotate the y value. Defaults to [2000, 4000, 6000, 8000, 10000].\n\n    Returns:\n        Axes: Axes object.\n    \"\"\"\n    moving_average = pd.Series(np.asarray(y)).rolling(window=window).mean()\n\n    ax.plot(\n        x,\n        moving_average,\n        **kwargs,\n    )\n\n    for percentile in annotate_at:\n        # Calculate the data index that corresponds to the percentile\n        idx = int(percentile * (len(x) - 1))\n\n        loss_value = moving_average[idx]\n\n        # Skip annotation if the moving average value is not available (e.g., at the beginning)\n        if pd.isna(loss_value):\n            continue\n\n        ax.annotate(\n            f\"{loss_value:.3g}\",\n            xy=(x[idx].item(), loss_value),\n            xytext=(-10, 10),  # Offset the text for better readability\n            textcoords=\"offset points\",\n            ha=\"center\",\n            va=\"bottom\",\n        )\n\n    return ax\n</code></pre>"},{"location":"api/visualization/#src.inspeqtor.experimental.visualization.assert_list_of_axes","title":"assert_list_of_axes","text":"<pre><code>assert_list_of_axes(axes) -&gt; list[Axes]\n</code></pre> <p>Assert the provide object that they are a list of Axes</p> <p>Parameters:</p> Name Type Description Default <code>axes</code> <code>Any</code> <p>Expected to be numpy array of Axes</p> required <p>Returns:</p> Type Description <code>list[Axes]</code> <p>list[Axes]: The list of Axes</p> Source code in <code>src/inspeqtor/experimental/visualization.py</code> <pre><code>def assert_list_of_axes(axes) -&gt; list[Axes]:\n    \"\"\"Assert the provide object that they are a list of Axes\n\n    Args:\n        axes (typing.Any): Expected to be numpy array of Axes\n\n    Returns:\n        list[Axes]: The list of Axes\n    \"\"\"\n    assert isinstance(axes, np.ndarray)\n    axes = axes.flatten()\n\n    for ax in axes:\n        assert isinstance(ax, Axes)\n    return axes.tolist()\n</code></pre>"},{"location":"api/visualization/#src.inspeqtor.experimental.visualization.set_fontsize","title":"set_fontsize","text":"<pre><code>set_fontsize(ax: Axes, fontsize: float | int)\n</code></pre> <p>Set all fontsize of the Axes object</p> <p>Parameters:</p> Name Type Description Default <code>ax</code> <code>Axes</code> <p>The Axes object which fontsize to be changed.</p> required <code>fontsize</code> <code>float | int</code> <p>The fontsize.</p> required Source code in <code>src/inspeqtor/experimental/visualization.py</code> <pre><code>def set_fontsize(ax: Axes, fontsize: float | int):\n    \"\"\"Set all fontsize of the Axes object\n\n    Args:\n        ax (Axes): The Axes object which fontsize to be changed.\n        fontsize (float | int): The fontsize.\n    \"\"\"\n    for item in (\n        [ax.title, ax.xaxis.label, ax.yaxis.label]\n        + ax.get_xticklabels()\n        + ax.get_yticklabels()\n    ):\n        item.set_fontsize(fontsize)\n\n    legend, handles = ax.get_legend_handles_labels()\n\n    ax.legend(legend, handles, fontsize=fontsize)\n</code></pre>"},{"location":"guides/dnn_to_bnn/","title":"How to convert DNN into BNN","text":"<p>Let us start by importing libraries.</p> In\u00a0[\u00a0]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport inspeqtor.experimental as sq\n</pre> import jax import jax.numpy as jnp import inspeqtor.experimental as sq In\u00a0[2]: Copied! <pre>def get_data():\n    # This is the predefined noise model that we are going to work with.\n    data_model = sq.predefined.get_predefined_data_model_m1()\n\n    # Now, we use the noise model to performing the data using simulator.\n    exp_data, _, _, _ = sq.predefined.generate_experimental_data(\n        key=jax.random.key(0),\n        hamiltonian=data_model.total_hamiltonian,\n        sample_size=100,\n        strategy=sq.predefined.SimulationStrategy.SHOT,\n        get_qubit_information_fn=lambda: data_model.qubit_information,\n        get_control_sequence_fn=lambda: data_model.control_sequence,\n        method=sq.predefined.WhiteboxStrategy.TROTTER,\n        trotter_steps=10_000,\n    )\n\n    # Now we can prepare the dataset that ready to use.\n    whitebox = sq.physics.make_trotterization_solver(\n        data_model.ideal_hamiltonian,\n        data_model.control_sequence,\n        data_model.dt,\n        trotter_steps=10_000,\n        y0=jnp.eye(2, dtype=jnp.complex128)\n    )\n    loaded_data = sq.predefined.prepare_data(\n        exp_data, data_model.control_sequence, whitebox\n    )\n\n    # Here, we just bundling things up for convinience uses.\n    key = jax.random.key(0)\n    key, random_split_key = jax.random.split(key)\n    (\n        train_pulse_parameters,\n        train_unitaries,\n        train_expectation_values,\n        test_pulse_parameters,\n        test_unitaries,\n        test_expectation_values,\n    ) = sq.utils.random_split(\n        random_split_key,\n        int(loaded_data.control_parameters.shape[0] * 0.1),  # Test size\n        loaded_data.control_parameters,\n        loaded_data.unitaries,\n        loaded_data.expectation_values,\n    )\n\n    shots = loaded_data.experiment_data.experiment_config.shots\n\n    train_binaries = sq.utils.eigenvalue_to_binary(\n        sq.utils.expectation_value_to_eigenvalue(train_expectation_values, shots)\n    )\n    train_binaries = jnp.swapaxes(jnp.swapaxes(train_binaries, 1, 2), 0, 1)\n\n    test_binaries = sq.utils.eigenvalue_to_binary(\n        sq.utils.expectation_value_to_eigenvalue(test_expectation_values, shots)\n    )\n\n    test_binaries = jnp.swapaxes(jnp.swapaxes(test_binaries, 1, 2), 0, 1)\n\n    assert train_binaries.shape == (shots, train_pulse_parameters.shape[0], 18)\n    assert test_binaries.shape == (shots, test_pulse_parameters.shape[0], 18)\n\n    train_data = sq.optimize.DataBundled(\n        control_params=sq.predefined.drag_feature_map(train_pulse_parameters),\n        unitaries=train_unitaries,\n        observables=train_binaries,\n        aux=train_expectation_values,\n    )\n\n    test_data = sq.optimize.DataBundled(\n        control_params=sq.predefined.drag_feature_map(test_pulse_parameters),\n        unitaries=test_unitaries,\n        observables=test_binaries,\n        aux=test_expectation_values,\n    )\n    # Return data ready to use.\n    return data_model, loaded_data, train_data, test_data\n\ndata_model, loaded_data, train_data, test_data = get_data()\n</pre> def get_data():     # This is the predefined noise model that we are going to work with.     data_model = sq.predefined.get_predefined_data_model_m1()      # Now, we use the noise model to performing the data using simulator.     exp_data, _, _, _ = sq.predefined.generate_experimental_data(         key=jax.random.key(0),         hamiltonian=data_model.total_hamiltonian,         sample_size=100,         strategy=sq.predefined.SimulationStrategy.SHOT,         get_qubit_information_fn=lambda: data_model.qubit_information,         get_control_sequence_fn=lambda: data_model.control_sequence,         method=sq.predefined.WhiteboxStrategy.TROTTER,         trotter_steps=10_000,     )      # Now we can prepare the dataset that ready to use.     whitebox = sq.physics.make_trotterization_solver(         data_model.ideal_hamiltonian,         data_model.control_sequence,         data_model.dt,         trotter_steps=10_000,         y0=jnp.eye(2, dtype=jnp.complex128)     )     loaded_data = sq.predefined.prepare_data(         exp_data, data_model.control_sequence, whitebox     )      # Here, we just bundling things up for convinience uses.     key = jax.random.key(0)     key, random_split_key = jax.random.split(key)     (         train_pulse_parameters,         train_unitaries,         train_expectation_values,         test_pulse_parameters,         test_unitaries,         test_expectation_values,     ) = sq.utils.random_split(         random_split_key,         int(loaded_data.control_parameters.shape[0] * 0.1),  # Test size         loaded_data.control_parameters,         loaded_data.unitaries,         loaded_data.expectation_values,     )      shots = loaded_data.experiment_data.experiment_config.shots      train_binaries = sq.utils.eigenvalue_to_binary(         sq.utils.expectation_value_to_eigenvalue(train_expectation_values, shots)     )     train_binaries = jnp.swapaxes(jnp.swapaxes(train_binaries, 1, 2), 0, 1)      test_binaries = sq.utils.eigenvalue_to_binary(         sq.utils.expectation_value_to_eigenvalue(test_expectation_values, shots)     )      test_binaries = jnp.swapaxes(jnp.swapaxes(test_binaries, 1, 2), 0, 1)      assert train_binaries.shape == (shots, train_pulse_parameters.shape[0], 18)     assert test_binaries.shape == (shots, test_pulse_parameters.shape[0], 18)      train_data = sq.optimize.DataBundled(         control_params=sq.predefined.drag_feature_map(train_pulse_parameters),         unitaries=train_unitaries,         observables=train_binaries,         aux=train_expectation_values,     )      test_data = sq.optimize.DataBundled(         control_params=sq.predefined.drag_feature_map(test_pulse_parameters),         unitaries=test_unitaries,         observables=test_binaries,         aux=test_expectation_values,     )     # Return data ready to use.     return data_model, loaded_data, train_data, test_data  data_model, loaded_data, train_data, test_data = get_data() <p>For the <code>linen</code> models, user has to use <code>random_flax_module</code> for a <code>flax_module</code> argument to <code>make_flax_probabilistic_graybox_model</code> function. Similar to <code>models</code> version, the adapter function that transform model's output into expectation values has to be use appropiately.</p> In\u00a0[3]: Copied! <pre>from numpyro.contrib.module import random_flax_module\n\nbase_model, adapter_fn, flax_module = (\n    sq.models.linen.UnitaryModel([10, 10]),\n    sq.models.toggling_unitary_to_expvals,\n    random_flax_module,\n)\n</pre> from numpyro.contrib.module import random_flax_module  base_model, adapter_fn, flax_module = (     sq.models.linen.UnitaryModel([10, 10]),     sq.models.toggling_unitary_to_expvals,     random_flax_module, ) In\u00a0[4]: Copied! <pre>base_model, adapter_fn, flax_module = (\n    sq.models.linen.WoModel([5], [5]),\n    sq.models.observable_to_expvals,\n    random_flax_module,\n)\n</pre> base_model, adapter_fn, flax_module = (     sq.models.linen.WoModel([5], [5]),     sq.models.observable_to_expvals,     random_flax_module, ) In\u00a0[5]: Copied! <pre>from flax import nnx\nfrom numpyro.contrib.module import random_nnx_module\n\nbase_model, adapter_fn, flax_module = (\n    sq.models.nnx.UnitaryModel([8, 8], rngs=nnx.Rngs(0)),\n    sq.models.toggling_unitary_to_expvals,\n    random_nnx_module,\n)\n</pre> from flax import nnx from numpyro.contrib.module import random_nnx_module  base_model, adapter_fn, flax_module = (     sq.models.nnx.UnitaryModel([8, 8], rngs=nnx.Rngs(0)),     sq.models.toggling_unitary_to_expvals,     random_nnx_module, ) In\u00a0[6]: Copied! <pre>base_model, adapter_fn, flax_module = (\n    sq.models.nnx.WoModel([8, 4, 6], [6, 4, 5], rngs=nnx.Rngs(0)),\n    sq.models.observable_to_expvals,\n    random_nnx_module,\n)\n</pre> base_model, adapter_fn, flax_module = (     sq.models.nnx.WoModel([8, 4, 6], [6, 4, 5], rngs=nnx.Rngs(0)),     sq.models.observable_to_expvals,     random_nnx_module, ) <p>Finally, we can define <code>graybox</code> model with the choice of the DNN model. For the custom <code>linen</code> and <code>nnx</code> models, you have to define a corresponding <code>adapter_fn</code> as well.</p> In\u00a0[7]: Copied! <pre>graybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(\n    name=\"graybox\",\n    base_model=base_model,\n    adapter_fn=adapter_fn,\n    prior=sq.probabilistic.dist.Normal(0, 1),\n    flax_module=flax_module,\n)\n\nnnx.display(base_model)\n</pre> graybox_model = sq.probabilistic.make_flax_probabilistic_graybox_model(     name=\"graybox\",     base_model=base_model,     adapter_fn=adapter_fn,     prior=sq.probabilistic.dist.Normal(0, 1),     flax_module=flax_module, )  nnx.display(base_model) <p>In the case that you need a completely in control of the model behavior. You can define the probabilistic mdoel from scratch too. You can do this using our primitive bayesian neural network components that design to be compatible with <code>numpyro</code>.</p> <p>Below is $\\hat{W}_{O}$-based model defined from scratch to mirror the <code>flax</code> implementation. Note that we use <code>sq.probabilistic.dense_layer</code> for our mathematical operation. The mental model of defining probabilistic model using our primitive as example below is that you can define it as operating on point prediction, while <code>numpyro</code> will handle the distribution part for you.</p> In\u00a0[8]: Copied! <pre>base_model = sq.probabilistic.WoModel(\"graybox\", (5,), (5,))\nadapter_fn = sq.model.observable_to_expvals\n\ngraybox_model = sq.probabilistic.make_probabilistic_graybox_model(\n    base_model, adapter_fn\n)\n</pre> base_model = sq.probabilistic.WoModel(\"graybox\", (5,), (5,)) adapter_fn = sq.model.observable_to_expvals  graybox_model = sq.probabilistic.make_probabilistic_graybox_model(     base_model, adapter_fn ) <p>You can inspect the model using <code>sq.probabilistic.get_trace</code>. Below, we visualize the trace of model using <code>nnx.display</code>. Note that <code>get_trace</code> handle the random key for you under the hood. User can suppliment their own key if desire.</p> In\u00a0[9]: Copied! <pre>from flax import nnx\n\nnnx.display(sq.probabilistic.get_trace(graybox_model)(test_data.control_params, test_data.unitaries))\n</pre> from flax import nnx  nnx.display(sq.probabilistic.get_trace(graybox_model)(test_data.control_params, test_data.unitaries)) In\u00a0[11]: Copied! <pre>import numpyro\nfrom numpyro.infer import (\n    SVI,\n    TraceMeanField_ELBO,\n)\nfrom alive_progress import alive_it\n</pre> import numpyro from numpyro.infer import (     SVI,     TraceMeanField_ELBO, ) from alive_progress import alive_it <p>You will see below that probabilistic model (<code>model</code> variable) accept <code>graybox</code> which can be defined from multiple ways demonstrated previously. In the following code snippet, we also use custom guide and custom training loop to demonstrate the flexibility.</p> In\u00a0[\u00a0]: Copied! <pre>model = sq.probabilistic.make_probabilistic_model(\n    predictive_model=graybox_model,\n)\n\nguide = sq.probabilistic.auto_diagonal_normal_guide_v3(\n    model,\n    train_data.control_params,\n    train_data.unitaries,\n    train_data.observables,\n    init_dist_fn=sq.probabilistic.bnn_init_dist_fn,\n    init_params_fn=sq.probabilistic.bnn_init_params_fn\n)\n\nNUM_STEPS = 10_000\noptimizer = sq.optimize.get_default_optimizer(NUM_STEPS)\n\nsvi = SVI(\n    model=model,\n    guide=guide,\n    optim=numpyro.optim.optax_to_numpyro(optimizer),\n    loss=TraceMeanField_ELBO(),\n)\n\nsvi_state = svi.init(\n    rng_key=jax.random.key(0),\n    control_parameters=train_data.control_params,\n    unitaries=train_data.unitaries,\n    observables=train_data.observables,\n)\n\nupdate_fn = sq.probabilistic.make_update_fn(\n    svi,\n    control_parameters=train_data.control_params,\n    unitaries=train_data.unitaries,\n    observables=train_data.observables,\n)\n\neval_fn = sq.probabilistic.make_evaluate_fn(\n    svi,\n    control_parameters=test_data.control_params,\n    unitaries=test_data.unitaries,\n    observables=test_data.observables,\n)\n\neval_losses = []\nlosses = []\nfor i in alive_it(range(NUM_STEPS), force_tty=True):\n    svi_state, loss = jax.jit(update_fn)(svi_state)\n    eval_loss = jax.jit(eval_fn)(svi_state)\n    losses.append(loss)\n    eval_losses.append(eval_loss)\n\nsvi_result = sq.probabilistic.SVIRunResult(\n    svi.get_params(svi_state), svi_state, jnp.stack(losses), jnp.stack(eval_losses)\n)\n</pre> model = sq.probabilistic.make_probabilistic_model(     predictive_model=graybox_model, )  guide = sq.probabilistic.auto_diagonal_normal_guide_v3(     model,     train_data.control_params,     train_data.unitaries,     train_data.observables,     init_dist_fn=sq.probabilistic.bnn_init_dist_fn,     init_params_fn=sq.probabilistic.bnn_init_params_fn )  NUM_STEPS = 10_000 optimizer = sq.optimize.get_default_optimizer(NUM_STEPS)  svi = SVI(     model=model,     guide=guide,     optim=numpyro.optim.optax_to_numpyro(optimizer),     loss=TraceMeanField_ELBO(), )  svi_state = svi.init(     rng_key=jax.random.key(0),     control_parameters=train_data.control_params,     unitaries=train_data.unitaries,     observables=train_data.observables, )  update_fn = sq.probabilistic.make_update_fn(     svi,     control_parameters=train_data.control_params,     unitaries=train_data.unitaries,     observables=train_data.observables, )  eval_fn = sq.probabilistic.make_evaluate_fn(     svi,     control_parameters=test_data.control_params,     unitaries=test_data.unitaries,     observables=test_data.observables, )  eval_losses = [] losses = [] for i in alive_it(range(NUM_STEPS), force_tty=True):     svi_state, loss = jax.jit(update_fn)(svi_state)     eval_loss = jax.jit(eval_fn)(svi_state)     losses.append(loss)     eval_losses.append(eval_loss)  svi_result = sq.probabilistic.SVIRunResult(     svi.get_params(svi_state), svi_state, jnp.stack(losses), jnp.stack(eval_losses) ) <pre>|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 52.7s (189.60/s\n</pre> In\u00a0[13]: Copied! <pre>sq.probabilistic.get_trace(guide)()\n</pre> sq.probabilistic.get_trace(guide)() Out[13]: <pre>OrderedDict([('graybox/shared.dense_0.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/shared.dense_0.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/shared.dense_0.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/shared.dense_0.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0., 0., 0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0., 0., 0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_X.dense_0.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/pauli_X.dense_0.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_X.dense_0.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/pauli_X.dense_0.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0., 0., 0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0., 0., 0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_X.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/U_X.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_X.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/U_X.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_X.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/D_X.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0.],\n                       [0., 0.],\n                       [0., 0.],\n                       [0., 0.],\n                       [0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0.],\n                      [0., 0.],\n                      [0., 0.],\n                      [0., 0.],\n                      [0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_X.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/D_X.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Y.dense_0.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/pauli_Y.dense_0.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Y.dense_0.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/pauli_Y.dense_0.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0., 0., 0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0., 0., 0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Y.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/U_Y.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Y.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/U_Y.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Y.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/D_Y.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0.],\n                       [0., 0.],\n                       [0., 0.],\n                       [0., 0.],\n                       [0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0.],\n                      [0., 0.],\n                      [0., 0.],\n                      [0., 0.],\n                      [0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Y.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/D_Y.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Z.dense_0.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/pauli_Z.dense_0.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.],\n                       [0., 0., 0., 0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.],\n                      [0., 0., 0., 0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Z.dense_0.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/pauli_Z.dense_0.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0., 0., 0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0., 0., 0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Z.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/U_Z.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.],\n                       [0., 0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.],\n                      [0., 0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Z.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/U_Z.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Z.kernel_loc',\n              {'type': 'param',\n               'name': 'graybox/D_Z.kernel_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0., 0.],\n                       [0., 0.],\n                       [0., 0.],\n                       [0., 0.],\n                       [0., 0.]], dtype=float64),),\n               'kwargs': {},\n               'value': Array([[0., 0.],\n                      [0., 0.],\n                      [0., 0.],\n                      [0., 0.],\n                      [0., 0.]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Z.bias_loc',\n              {'type': 'param',\n               'name': 'graybox/D_Z.bias_loc',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0., 0.], dtype=float64),),\n               'kwargs': {},\n               'value': Array([0., 0.], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/shared.dense_0.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/shared.dense_0.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/shared.dense_0.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/shared.dense_0.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_X.dense_0.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/pauli_X.dense_0.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_X.dense_0.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/pauli_X.dense_0.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_X.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/U_X.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_X.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/U_X.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_X.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/D_X.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_X.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/D_X.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Y.dense_0.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/pauli_Y.dense_0.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Y.dense_0.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/pauli_Y.dense_0.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Y.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/U_Y.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Y.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/U_Y.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Y.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/D_Y.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Y.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/D_Y.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Z.dense_0.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/pauli_Z.dense_0.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1, 0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/pauli_Z.dense_0.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/pauli_Z.dense_0.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1, 0.1, 0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Z.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/U_Z.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1],\n                       [0.1, 0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1],\n                      [0.1, 0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/U_Z.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/U_Z.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Z.kernel_scale',\n              {'type': 'param',\n               'name': 'graybox/D_Z.kernel_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([[0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1],\n                       [0.1, 0.1]], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([[0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1],\n                      [0.1, 0.1]], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/D_Z.bias_scale',\n              {'type': 'param',\n               'name': 'graybox/D_Z.bias_scale',\n               'fn': &lt;function numpyro.util.identity(x, *args, **kwargs)&gt;,\n               'args': (Array([0.1, 0.1], dtype=float64),),\n               'kwargs': {'constraint': SoftplusPositive(lower_bound=0.0)},\n               'value': Array([0.1, 0.1], dtype=float64),\n               'scale': None,\n               'cond_indep_stack': []}),\n             ('graybox/shared.dense_0.kernel',\n              {'type': 'sample',\n               'name': 'graybox/shared.dense_0.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe580 with batch shape () and event shape (8, 5)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [ 928981903 3453687069],\n                'sample_shape': ()},\n               'value': Array([[-0.14008841,  0.1432145 ,  0.06248107,  0.02004873,  0.02471476],\n                      [ 0.05244771,  0.08618686,  0.12237145, -0.14305551,  0.11400058],\n                      [-0.07499601,  0.04027209, -0.0546837 ,  0.04114347,  0.03301723],\n                      [-0.00954418,  0.09128964,  0.0388787 ,  0.19317479,  0.15932732],\n                      [ 0.13931197,  0.01366918,  0.07825694,  0.03967606,  0.07148536],\n                      [ 0.22849079,  0.03318426,  0.13805099,  0.05436563,  0.09425814],\n                      [-0.02164755, -0.05206585, -0.15751706,  0.05851865, -0.08691522],\n                      [-0.10058468,  0.13832578,  0.01075193, -0.05132055, -0.05932651]],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/shared.dense_0.bias',\n              {'type': 'sample',\n               'name': 'graybox/shared.dense_0.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe6d0 with batch shape () and event shape (5,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [1353695780 2116000888],\n                'sample_shape': ()},\n               'value': Array([1.04531782, 1.07238262, 0.85906295, 1.14733469, 1.00264397],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/pauli_X.dense_0.kernel',\n              {'type': 'sample',\n               'name': 'graybox/pauli_X.dense_0.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe510 with batch shape () and event shape (5, 5)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3531307783  465290248],\n                'sample_shape': ()},\n               'value': Array([[-0.03808256, -0.02172061, -0.06575782, -0.03350542, -0.03510779],\n                      [ 0.0584175 ,  0.11405403, -0.10272196,  0.15822406,  0.11285611],\n                      [-0.08091083, -0.02301288, -0.03948298, -0.1102386 ,  0.03338434],\n                      [-0.03640981,  0.02810232, -0.10063834,  0.02922136,  0.04713823],\n                      [-0.10802706,  0.05611299, -0.10468815,  0.03382672, -0.19492614]],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/pauli_X.dense_0.bias',\n              {'type': 'sample',\n               'name': 'graybox/pauli_X.dense_0.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe4a0 with batch shape () and event shape (5,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [1539457558  118255239],\n                'sample_shape': ()},\n               'value': Array([0.90524273, 0.8990806 , 1.0311893 , 0.98192345, 1.03276893],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/U_X.kernel',\n              {'type': 'sample',\n               'name': 'graybox/U_X.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe350 with batch shape () and event shape (5, 3)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [4274742258 3380111416],\n                'sample_shape': ()},\n               'value': Array([[ 0.17632242, -0.11744192,  0.01794445],\n                      [-0.05177495,  0.16960584, -0.06622758],\n                      [ 0.17634344, -0.03464814,  0.16775395],\n                      [ 0.0823993 ,  0.01903537, -0.09046388],\n                      [ 0.15659127,  0.10918574, -0.0401434 ]], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/U_X.bias',\n              {'type': 'sample',\n               'name': 'graybox/U_X.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe270 with batch shape () and event shape (3,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [1093704277 2843913905],\n                'sample_shape': ()},\n               'value': Array([0.9271848 , 1.0555702 , 1.06170073], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/D_X.kernel',\n              {'type': 'sample',\n               'name': 'graybox/D_X.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe200 with batch shape () and event shape (5, 2)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3905300018 1047965080],\n                'sample_shape': ()},\n               'value': Array([[-0.05559002,  0.04963322],\n                      [-0.05268756,  0.16933363],\n                      [-0.0222924 , -0.18154015],\n                      [-0.03311026, -0.07193543],\n                      [ 0.07697746,  0.04136223]], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/D_X.bias',\n              {'type': 'sample',\n               'name': 'graybox/D_X.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbe0b0 with batch shape () and event shape (2,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3381965182 2262451415],\n                'sample_shape': ()},\n               'value': Array([0.89985102, 1.03960598], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/pauli_Y.dense_0.kernel',\n              {'type': 'sample',\n               'name': 'graybox/pauli_Y.dense_0.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbdfd0 with batch shape () and event shape (5, 5)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [2772760534 1150241264],\n                'sample_shape': ()},\n               'value': Array([[ 0.10520649, -0.09439651,  0.25773657, -0.19807482,  0.08300809],\n                      [ 0.03322761, -0.11226499,  0.13162118,  0.00421367,  0.11259789],\n                      [ 0.02646392,  0.13100174, -0.13760007,  0.07654653,  0.00921974],\n                      [-0.00175621,  0.19364408, -0.04745788,  0.04232322, -0.00253774],\n                      [ 0.00661635, -0.13051184, -0.0986543 , -0.07035516, -0.00834113]],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/pauli_Y.dense_0.bias',\n              {'type': 'sample',\n               'name': 'graybox/pauli_Y.dense_0.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbdef0 with batch shape () and event shape (5,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3949573971  553583122],\n                'sample_shape': ()},\n               'value': Array([0.89491221, 0.99243349, 0.99889162, 0.98189325, 1.04351397],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/U_Y.kernel',\n              {'type': 'sample',\n               'name': 'graybox/U_Y.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbde10 with batch shape () and event shape (5, 3)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3789552139 2399770011],\n                'sample_shape': ()},\n               'value': Array([[-0.07285353,  0.04304867, -0.03740092],\n                      [-0.22240879,  0.04847291, -0.16410529],\n                      [-0.01032646,  0.06037331, -0.00884798],\n                      [-0.05031299,  0.02473008, -0.05038715],\n                      [-0.02642271,  0.10363026, -0.0586004 ]], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/U_Y.bias',\n              {'type': 'sample',\n               'name': 'graybox/U_Y.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbdd30 with batch shape () and event shape (3,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3499959921 3652298783],\n                'sample_shape': ()},\n               'value': Array([1.13729885, 0.76343103, 0.95312259], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/D_Y.kernel',\n              {'type': 'sample',\n               'name': 'graybox/D_Y.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbdcc0 with batch shape () and event shape (5, 2)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [ 312422605 4273504752],\n                'sample_shape': ()},\n               'value': Array([[ 0.10310684,  0.00423635],\n                      [ 0.03049902, -0.00904775],\n                      [-0.07080258, -0.14189202],\n                      [-0.03537874, -0.03072079],\n                      [ 0.02076615,  0.14101123]], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/D_Y.bias',\n              {'type': 'sample',\n               'name': 'graybox/D_Y.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbdb70 with batch shape () and event shape (2,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [1301443351 1262990949],\n                'sample_shape': ()},\n               'value': Array([0.95188058, 1.00563022], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/pauli_Z.dense_0.kernel',\n              {'type': 'sample',\n               'name': 'graybox/pauli_Z.dense_0.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbda90 with batch shape () and event shape (5, 5)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [1115782097  492331180],\n                'sample_shape': ()},\n               'value': Array([[-0.1325124 , -0.03843222,  0.02943445, -0.16186975,  0.15639413],\n                      [ 0.09779884,  0.03734697, -0.01805349,  0.07809086, -0.04967616],\n                      [-0.33209741,  0.07484574,  0.08881937,  0.05133246,  0.05045518],\n                      [ 0.00230586,  0.0461714 , -0.13727346, -0.0351774 ,  0.05739598],\n                      [-0.01912264,  0.07885565,  0.0339848 , -0.07031864, -0.00646732]],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/pauli_Z.dense_0.bias',\n              {'type': 'sample',\n               'name': 'graybox/pauli_Z.dense_0.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbda20 with batch shape () and event shape (5,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3716532200  665832165],\n                'sample_shape': ()},\n               'value': Array([1.05892567, 0.90481273, 1.02666678, 0.93606215, 0.99313125],      dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/U_Z.kernel',\n              {'type': 'sample',\n               'name': 'graybox/U_Z.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbd8d0 with batch shape () and event shape (5, 3)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3468366468  328474123],\n                'sample_shape': ()},\n               'value': Array([[-0.02167346,  0.02430774, -0.06703646],\n                      [-0.15356107, -0.02620818, -0.11573705],\n                      [ 0.03559265,  0.04111457, -0.10566783],\n                      [-0.08103412, -0.05960112, -0.09647807],\n                      [ 0.15874484,  0.23166607,  0.0868952 ]], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/U_Z.bias',\n              {'type': 'sample',\n               'name': 'graybox/U_Z.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbd7f0 with batch shape () and event shape (3,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3568434798 2807364103],\n                'sample_shape': ()},\n               'value': Array([1.02646247, 0.98463201, 1.19315067], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/D_Z.kernel',\n              {'type': 'sample',\n               'name': 'graybox/D_Z.kernel',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbd780 with batch shape () and event shape (5, 2)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [3995525659 1921098443],\n                'sample_shape': ()},\n               'value': Array([[ 0.1199833 , -0.10544737],\n                      [ 0.1504808 , -0.00809291],\n                      [-0.20515682,  0.11901646],\n                      [-0.02348738, -0.0813381 ],\n                      [-0.03168612,  0.11155693]], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}}),\n             ('graybox/D_Z.bias',\n              {'type': 'sample',\n               'name': 'graybox/D_Z.bias',\n               'fn': &lt;numpyro.distributions.distribution.Independent object at 0x356bbd630 with batch shape () and event shape (2,)&gt;,\n               'args': (),\n               'kwargs': {'rng_key': Array((), dtype=key&lt;fry&gt;) overlaying:\n                [2938295869 4294411401],\n                'sample_shape': ()},\n               'value': Array([0.97315917, 1.04829688], dtype=float64),\n               'scale': None,\n               'is_observed': False,\n               'intermediates': [],\n               'cond_indep_stack': [],\n               'infer': {}})])</pre> In\u00a0[14]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[8]: Copied! <pre>rescaled_eval_losses = svi_result.eval_losses / test_data.control_params.shape[0]\nrescaled_train_losses = svi_result.losses / train_data.control_params.shape[0]\n\niterations = jnp.arange(len(rescaled_train_losses))\nfig, ax = plt.subplots(figsize=(5, 3))\nax = sq.visualization.plot_loss_with_moving_average(\n    iterations,\n    rescaled_eval_losses,\n    ax=ax,\n    color=\"#6366f1\",\n    label=\"moving average Test ELBO Loss\",\n)\nax = sq.visualization.plot_loss_with_moving_average(\n    iterations,\n    rescaled_train_losses,\n    ax,\n    window=1,\n    annotate_at=[],\n    color=\"gray\",\n    alpha=0.25,\n    label=\"Train ELBO Loss\",\n)\nax.set_yscale(\"log\")\nax.set_xlabel(\"Iterations\")\nax.set_ylabel(\"Rescaled ELBO Loss\")\nax.legend()\n</pre> rescaled_eval_losses = svi_result.eval_losses / test_data.control_params.shape[0] rescaled_train_losses = svi_result.losses / train_data.control_params.shape[0]  iterations = jnp.arange(len(rescaled_train_losses)) fig, ax = plt.subplots(figsize=(5, 3)) ax = sq.visualization.plot_loss_with_moving_average(     iterations,     rescaled_eval_losses,     ax=ax,     color=\"#6366f1\",     label=\"moving average Test ELBO Loss\", ) ax = sq.visualization.plot_loss_with_moving_average(     iterations,     rescaled_train_losses,     ax,     window=1,     annotate_at=[],     color=\"gray\",     alpha=0.25,     label=\"Train ELBO Loss\", ) ax.set_yscale(\"log\") ax.set_xlabel(\"Iterations\") ax.set_ylabel(\"Rescaled ELBO Loss\") ax.legend()  Out[8]: <pre>&lt;matplotlib.legend.Legend at 0x3569ac1a0&gt;</pre> In\u00a0[15]: Copied! <pre>import tempfile\nfrom pathlib import Path\n\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n\n    model_path = Path(tmpdir)\n\n    # Create the path with parents if not existed already\n    model_path.mkdir(parents=True, exist_ok=True)\n\n    model_state = sq.models.ModelData(\n        params=svi_result.params,\n        config={}\n    )\n\n    model_state.to_file(model_path /\"model.json\")\n\n    reloaded_model = sq.models.ModelData.from_file(model_path / \"model.json\")\n\nassert reloaded_model == model_state\n</pre> import tempfile from pathlib import Path   with tempfile.TemporaryDirectory() as tmpdir:      model_path = Path(tmpdir)      # Create the path with parents if not existed already     model_path.mkdir(parents=True, exist_ok=True)      model_state = sq.models.ModelData(         params=svi_result.params,         config={}     )      model_state.to_file(model_path /\"model.json\")      reloaded_model = sq.models.ModelData.from_file(model_path / \"model.json\")  assert reloaded_model == model_state In\u00a0[17]: Copied! <pre>from numpyro.infer import Predictive\n\nparams = test_data.control_params[0]\nunitary = test_data.unitaries[0]\n</pre> from numpyro.infer import Predictive  params = test_data.control_params[0] unitary = test_data.unitaries[0] In\u00a0[18]: Copied! <pre>def make_predictive_fn_v2(\n    model,\n    guide,\n    params,\n    shots: int,\n):\n    predictive = Predictive(model, guide=guide, params=params, num_samples=shots)\n\n    def predictive_fn(*args, **kwargs):\n        return predictive(*args, **kwargs)\n\n    return predictive_fn\n\n\npredictive_fn_from_guide = make_predictive_fn_v2(\n    sq.probabilistic.make_probabilistic_model(\n        predictive_model=graybox_model, log_expectation_values=True\n    ),\n    guide,\n    model_state.params,\n    shots=1000,\n)\n\nguide_expectation_values = predictive_fn_from_guide(jax.random.key(0), params, unitary)[\n    \"expectation_values\"\n]\n</pre> def make_predictive_fn_v2(     model,     guide,     params,     shots: int, ):     predictive = Predictive(model, guide=guide, params=params, num_samples=shots)      def predictive_fn(*args, **kwargs):         return predictive(*args, **kwargs)      return predictive_fn   predictive_fn_from_guide = make_predictive_fn_v2(     sq.probabilistic.make_probabilistic_model(         predictive_model=graybox_model, log_expectation_values=True     ),     guide,     model_state.params,     shots=1000, )  guide_expectation_values = predictive_fn_from_guide(jax.random.key(0), params, unitary)[     \"expectation_values\" ] In\u00a0[19]: Copied! <pre>posterior_fn = sq.probabilistic.make_posterior_fn(\n    model_state.params, sq.probabilistic.bnn_init_dist_fn\n)\n\nbase_model = sq.probabilistic.WoModel(\"graybox\", (5,), (5,), priors_fn=posterior_fn)\nadapter_fn = sq.model.observable_to_expvals\n\ngraybox_model = sq.probabilistic.make_probabilistic_graybox_model(\n    base_model, adapter_fn\n)\n\nposterior_model = sq.probabilistic.make_probabilistic_model(\n    predictive_model=graybox_model,\n    log_expectation_values=True,\n)\n</pre> posterior_fn = sq.probabilistic.make_posterior_fn(     model_state.params, sq.probabilistic.bnn_init_dist_fn )  base_model = sq.probabilistic.WoModel(\"graybox\", (5,), (5,), priors_fn=posterior_fn) adapter_fn = sq.model.observable_to_expvals  graybox_model = sq.probabilistic.make_probabilistic_graybox_model(     base_model, adapter_fn )  posterior_model = sq.probabilistic.make_probabilistic_model(     predictive_model=graybox_model,     log_expectation_values=True, ) In\u00a0[20]: Copied! <pre>posterior_expectation_values = Predictive(model=posterior_model, num_samples=1000)(\n    jax.random.key(0), params, unitary\n)[\"expectation_values\"]\n</pre> posterior_expectation_values = Predictive(model=posterior_model, num_samples=1000)(     jax.random.key(0), params, unitary )[\"expectation_values\"] In\u00a0[21]: Copied! <pre>import seaborn as sns\nsns.histplot({\n    \"posterior\": posterior_expectation_values[:, -1], \n    \"guide\": guide_expectation_values[:, -1]\n})\n</pre> import seaborn as sns sns.histplot({     \"posterior\": posterior_expectation_values[:, -1],      \"guide\": guide_expectation_values[:, -1] }) Out[21]: <pre>&lt;Axes: ylabel='Count'&gt;</pre>"},{"location":"guides/dnn_to_bnn/#how-to-convert-dnn-into-bnn","title":"How to convert DNN into BNN\u00b6","text":"<p>Goal</p> <p>         This notebook aims to showcase how can you convert a statistical deep neural network that predict a point into a bayesian neural network that predict the distribution instead.      </p>"},{"location":"guides/dnn_to_bnn/#get-synthetic-dataset-ready","title":"Get synthetic dataset ready \ud83d\ude80\u00b6","text":"<p>Here we are working with synthetic dataset. So, we need to define a simulator, perform an experiment, prepare the dataset for model training/inference, benchmarking. Luckily, <code>inspeqtor</code> provide a serveral helper functions and predefined noise model for user to quickly get stuff setting up.</p>"},{"location":"guides/dnn_to_bnn/#routes-to-convert-dnn-model-to-bnn-model","title":"Routes to convert DNN model to BNN model\u00b6","text":"<p><code>inspeqtor</code> provides serveral ways to convert DNN to BNN and user can also create the BNN from scratch too! For the predefined models or model defined using <code>flax</code>, <code>numpyro.contrib.module</code> implemented the function that transform the statistical model to probabilistic model with ease. Thus, <code>inspeqtor</code> provides a wrapper function <code>make_flax_probabilistic_graybox_model</code> that help convet user defined model into a proper probabilistic Graybox model. Here are examples of how to do it.</p>"},{"location":"guides/dnn_to_bnn/#stochastic-variational-inference-of-bnn","title":"Stochastic Variational Inference of BNN\u00b6","text":"<p>Let import the goodies to use.</p>"},{"location":"guides/dnn_to_bnn/#visualize-the-negative-elbo-loss","title":"Visualize the Negative ELBO loss\u00b6","text":"<p>Belows, we rescale the loss with the number of the sample and plot it with <code>matplotlib</code>.</p>"},{"location":"guides/dnn_to_bnn/#two-ways-of-making-posterior-predictive-model","title":"Two ways of making posterior predictive model\u00b6","text":"<p>First, let us prepare our testing point and import helper function.</p>"},{"location":"guides/dnn_to_bnn/#from-guide","title":"From guide\u00b6","text":"<p>This is the way with a guide from elsewhere. We have to completely rely on the <code>Predictive</code> to do the work for us. Because we did not assume that the guide is create with structural approach in mind.</p>"},{"location":"guides/dnn_to_bnn/#from-variational-parameters","title":"From variational parameters\u00b6","text":"<p>With our <code>auto_guide</code>, the variational parameters are keep in the sturture that is ready to use.</p>"},{"location":"guides/gen_data/","title":"Gen data","text":"In\u00a0[\u00a0]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport inspeqtor.experimental as sq\n</pre> import jax import jax.numpy as jnp import inspeqtor.experimental as sq In\u00a0[\u00a0]: Copied! <pre># --8&lt;-- [start:qubit-info]\nqubit_info = sq.data.QubitInformation(\n    unit=\"GHz\",\n    qubit_idx=0,\n    anharmonicity=-0.2,\n    frequency=5.0,\n    drive_strength=0.1,\n)\n# --8&lt;-- [end:qubit-info]\n</pre> # --8&lt;-- [start:qubit-info] qubit_info = sq.data.QubitInformation(     unit=\"GHz\",     qubit_idx=0,     anharmonicity=-0.2,     frequency=5.0,     drive_strength=0.1, ) # --8&lt;-- [end:qubit-info] In\u00a0[\u00a0]: Copied! <pre># --8&lt;-- [start:control]\ntotal_length = 320\npulse = sq.predefined.DragPulseV2(\n    duration=total_length,\n    qubit_drive_strength=qubit_info.drive_strength,\n    dt=2 / 9,\n    max_amp=0.5,\n    min_theta=0,\n    max_theta=2 * jnp.pi,\n    min_beta=-5.0,\n    max_beta=5.0,\n)\n</pre> # --8&lt;-- [start:control] total_length = 320 pulse = sq.predefined.DragPulseV2(     duration=total_length,     qubit_drive_strength=qubit_info.drive_strength,     dt=2 / 9,     max_amp=0.5,     min_theta=0,     max_theta=2 * jnp.pi,     min_beta=-5.0,     max_beta=5.0, ) In\u00a0[\u00a0]: Copied! <pre>control_sequence = sq.control.ControlSequence(\n    controls=[\n        pulse,\n    ],\n    total_dt=total_length,\n)\n# --8&lt;-- [end:control]\n</pre> control_sequence = sq.control.ControlSequence(     controls=[         pulse,     ],     total_dt=total_length, ) # --8&lt;-- [end:control] In\u00a0[\u00a0]: Copied! <pre>data_model = sq.predefined.get_predefined_data_model_m1()\n</pre> data_model = sq.predefined.get_predefined_data_model_m1() In\u00a0[\u00a0]: Copied! <pre># --8&lt;-- [start:gen-syn-dataset]\nexp_data, control_seq, _, _ = sq.predefined.generate_experimental_data(\n    key=jax.random.key(0),\n    hamiltonian=data_model.total_hamiltonian,\n    sample_size=1_000,\n    strategy=sq.predefined.SimulationStrategy.SHOT,\n    get_qubit_information_fn=lambda: data_model.qubit_information,\n    get_control_sequence_fn=lambda: data_model.control_sequence,\n    method=sq.predefined.WhiteboxStrategy.TROTTER,\n    trotter_steps=10_000,\n)\n# --8&lt;-- [end:gen-syn-dataset]\n</pre> # --8&lt;-- [start:gen-syn-dataset] exp_data, control_seq, _, _ = sq.predefined.generate_experimental_data(     key=jax.random.key(0),     hamiltonian=data_model.total_hamiltonian,     sample_size=1_000,     strategy=sq.predefined.SimulationStrategy.SHOT,     get_qubit_information_fn=lambda: data_model.qubit_information,     get_control_sequence_fn=lambda: data_model.control_sequence,     method=sq.predefined.WhiteboxStrategy.TROTTER,     trotter_steps=10_000, ) # --8&lt;-- [end:gen-syn-dataset] In\u00a0[\u00a0]: Copied! <pre># --8&lt;-- [start:save-dataset]\nfrom pathlib import Path  # noqa: E402\n</pre> # --8&lt;-- [start:save-dataset] from pathlib import Path  # noqa: E402 In\u00a0[\u00a0]: Copied! <pre>path = Path(\"./test_data_v1\")\n# Create the path with parents if not existed already\npath.mkdir(parents=True, exist_ok=True)\n# Save the experiment with a single liner \ud83d\ude09.\nsq.predefined.save_data_to_path(path, exp_data, data_model.control_sequence)\n# --8&lt;-- [end:save-dataset]\n</pre> path = Path(\"./test_data_v1\") # Create the path with parents if not existed already path.mkdir(parents=True, exist_ok=True) # Save the experiment with a single liner \ud83d\ude09. sq.predefined.save_data_to_path(path, exp_data, data_model.control_sequence) # --8&lt;-- [end:save-dataset] In\u00a0[\u00a0]: Copied! <pre># --8&lt;-- [start:load-dataset]\nloaded_data = sq.predefined.load_data_from_path(\n    path,\n    hamiltonian_spec=sq.predefined.HamiltonianSpec(\n        method=sq.predefined.WhiteboxStrategy.TROTTER,\n        trotter_steps=10_000,\n    ),\n)\n# --8&lt;-- [end:load-dataset]\n</pre> # --8&lt;-- [start:load-dataset] loaded_data = sq.predefined.load_data_from_path(     path,     hamiltonian_spec=sq.predefined.HamiltonianSpec(         method=sq.predefined.WhiteboxStrategy.TROTTER,         trotter_steps=10_000,     ), ) # --8&lt;-- [end:load-dataset]"},{"location":"guides/generate-dataset/","title":"Dataset Creating","text":"<p>TODO</p> <p>This guide should show a quick way to generate the dataset to work with using <code>sq.predefined.generate_experimental_data</code>.</p>"},{"location":"guides/generate-dataset/#prepare-experiments","title":"Prepare experiments","text":"<p>We break the experiment preparation phase into the following steps.</p> <ul> <li>Gathering \"prior\" information about the quantum device.</li> <li>Defining the control action.</li> </ul>"},{"location":"guides/generate-dataset/#quantum-device-specification","title":"Quantum device specification","text":"<p>In <code>inspeqtor</code>, we focus on characterizing quantum device. In the finest level, user most likely want to perform control calibration on individual qubit which is part of the full system. Thus, we provide a dedicated <code>dataclass</code> responsible for holding \"prior\" information about the qubit. The information is often necessary for constructing the subsystem Hamiltonian which is used for open-loop optimization. Below is the code snippet to initialize <code>QubitInformation</code> object.</p> <pre><code>qubit_info = sq.data.QubitInformation(\n    unit=\"GHz\",\n    qubit_idx=0,\n    anharmonicity=-0.2,\n    frequency=5.0,\n    drive_strength=0.1,\n)\n</code></pre>"},{"location":"guides/generate-dataset/#define-the-control","title":"Define the control","text":"<p>For composability of control action, we let user define an \"atomic\" control action by inheriting <code>Control</code> dataclass, then compose them together via <code>ControlSequence</code> class. Below is an example of defining the total control action with only single predefined <code>DragPulseV2</code>.</p> <pre><code>total_length = 320\npulse = sq.predefined.DragPulseV2(\n    duration=total_length,\n    qubit_drive_strength=qubit_info.drive_strength,\n    dt=2 / 9,\n    max_amp=0.5,\n    min_theta=0,\n    max_theta=2 * jnp.pi,\n    min_beta=-5.0,\n    max_beta=5.0,\n)\n\ncontrol_sequence = sq.control.ControlSequence(\n    controls=[\n        pulse,\n    ],\n    total_dt=total_length,\n)\n</code></pre>"},{"location":"guides/generate-dataset/#perform-experiment","title":"Perform experiment","text":""},{"location":"guides/generate-dataset/#generate-some-synthetic-dataset-to-work-with","title":"Generate some synthetic dataset to work with","text":"<pre><code>exp_data, control_seq, _, _ = sq.predefined.generate_experimental_data(\n    key=jax.random.key(0),\n    hamiltonian=data_model.total_hamiltonian,\n    sample_size=1_000,\n    strategy=sq.predefined.SimulationStrategy.SHOT,\n    get_qubit_information_fn=lambda: data_model.qubit_information,\n    get_control_sequence_fn=lambda: data_model.control_sequence,\n    method=sq.predefined.WhiteboxStrategy.TROTTER,\n    trotter_steps=10_000,\n)\n</code></pre>"},{"location":"guides/generate-dataset/#perform-experiment-on-actual-device","title":"Perform experiment on actual device","text":""},{"location":"guides/generate-dataset/#save-the-experiment","title":"Save the experiment","text":"<pre><code>from pathlib import Path  # noqa: E402\n\npath = Path(\"./test_data_v1\")\n# Create the path with parents if not existed already\npath.mkdir(parents=True, exist_ok=True)\n# Save the experiment with a single liner \ud83d\ude09.\nsq.predefined.save_data_to_path(path, exp_data, data_model.control_sequence)\n</code></pre>"},{"location":"guides/generate-dataset/#load-the-experiment","title":"Load the experiment","text":"<pre><code>loaded_data = sq.predefined.load_data_from_path(\n    path,\n    hamiltonian_spec=sq.predefined.HamiltonianSpec(\n        method=sq.predefined.WhiteboxStrategy.TROTTER,\n        trotter_steps=10_000,\n    ),\n)\n</code></pre>"},{"location":"guides/working_memory/","title":"Working memory example","text":"In\u00a0[40]: Copied! <pre>import inspeqtor.experimental as sq\nimport numpyro\nfrom numpyro import handlers\nfrom numpyro import distributions as dist\nimport jax\nimport jax.numpy as jnp\n\nfrom numpyro.infer import (\n    SVI,\n    TraceMeanField_ELBO,\n)\nimport optax\n</pre> import inspeqtor.experimental as sq import numpyro from numpyro import handlers from numpyro import distributions as dist import jax import jax.numpy as jnp  from numpyro.infer import (     SVI,     TraceMeanField_ELBO, ) import optax In\u00a0[\u00a0]: Copied! <pre>sensitivity = 1.0\nprior_mean = 7.0\nprior_sd = 2.0\n\n\ndef make_model(mean, sd):\n    def model(length):\n        # Dimension -1 of `l` represents the number of rounds\n        # Other dimensions are batch dimensions: we indicate this with a plate_stack\n        with numpyro.plate_stack(\"plate\", list(length.shape[:-1])):\n            # Share theta across the number of rounds of the experiment\n            # This represents repeatedly testing the same participant\n            theta = numpyro.sample(\"theta\", dist.Normal(mean, sd))  # type: ignore\n            theta = jnp.expand_dims(theta, -1)\n            # This define a *logistic regression* model for y\n            logit_p = sensitivity * (theta - length)\n            # The event shape represents responses from the same participant\n            y = numpyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))  # type: ignore\n            return y\n\n    return model\n\n\ndef guide(length: jnp.ndarray):\n    # The guide is initialised at the prior\n    posterior_mean = numpyro.param(\"posterior_mean\", jnp.array(prior_mean))\n    posterior_sd = numpyro.param(\n        \"posterior_sd\", jnp.array(prior_sd), constraint=dist.constraints.positive\n    )\n    numpyro.sample(\"theta\", dist.Normal(posterior_mean, posterior_sd))  # type: ignore\n</pre> sensitivity = 1.0 prior_mean = 7.0 prior_sd = 2.0   def make_model(mean, sd):     def model(length):         # Dimension -1 of `l` represents the number of rounds         # Other dimensions are batch dimensions: we indicate this with a plate_stack         with numpyro.plate_stack(\"plate\", list(length.shape[:-1])):             # Share theta across the number of rounds of the experiment             # This represents repeatedly testing the same participant             theta = numpyro.sample(\"theta\", dist.Normal(mean, sd))  # type: ignore             theta = jnp.expand_dims(theta, -1)             # This define a *logistic regression* model for y             logit_p = sensitivity * (theta - length)             # The event shape represents responses from the same participant             y = numpyro.sample(\"y\", dist.Bernoulli(logits=logit_p).to_event(1))  # type: ignore             return y      return model   def guide(length: jnp.ndarray):     # The guide is initialised at the prior     posterior_mean = numpyro.param(\"posterior_mean\", jnp.array(prior_mean))     posterior_sd = numpyro.param(         \"posterior_sd\", jnp.array(prior_sd), constraint=dist.constraints.positive     )     numpyro.sample(\"theta\", dist.Normal(posterior_mean, posterior_sd))  # type: ignore  In\u00a0[3]: Copied! <pre>l_data = jnp.array([5., 7., 9.])\ny_data = jnp.array([1., 1., 0.])\n\nmodel = make_model(prior_mean, prior_sd)\nconditioned_model = handlers.condition(model, {\"y\": y_data})\n\noptimizer = optax.adamw(learning_rate=1e-3)\n\nsvi = SVI(\n    model=conditioned_model,\n    guide=guide,\n    optim=numpyro.optim.optax_to_numpyro(optimizer),\n    loss=TraceMeanField_ELBO(num_particles=1),\n)\n\nsvi_results = svi.run(\n    jax.random.key(0),\n    10_000,\n    l_data,\n    progress_bar=True\n)\n</pre> l_data = jnp.array([5., 7., 9.]) y_data = jnp.array([1., 1., 0.])  model = make_model(prior_mean, prior_sd) conditioned_model = handlers.condition(model, {\"y\": y_data})  optimizer = optax.adamw(learning_rate=1e-3)  svi = SVI(     model=conditioned_model,     guide=guide,     optim=numpyro.optim.optax_to_numpyro(optimizer),     loss=TraceMeanField_ELBO(num_particles=1), )  svi_results = svi.run(     jax.random.key(0),     10_000,     l_data,     progress_bar=True ) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10382.97it/s, init loss: 2.2569, avg. loss [9501-10000]: 1.2946]\n</pre> In\u00a0[4]: Copied! <pre>svi_results.params\n</pre> svi_results.params Out[4]: <pre>{'posterior_mean': Array(7.75896117, dtype=float64),\n 'posterior_sd': Array(1.21964926, dtype=float64)}</pre> In\u00a0[5]: Copied! <pre>def marginal_guide(design, observation_labels, target_labels):\n    # This shape allows us to learn a different parameter for each candidate design l\n    q_logit = numpyro.param(\"theta\", jnp.zeros(design.shape[-2:]))\n    numpyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))  # type: ignore\n</pre> def marginal_guide(design, observation_labels, target_labels):     # This shape allows us to learn a different parameter for each candidate design l     q_logit = numpyro.param(\"theta\", jnp.zeros(design.shape[-2:]))     numpyro.sample(\"y\", dist.Bernoulli(logits=q_logit).to_event(1))  # type: ignore In\u00a0[\u00a0]: Copied! <pre>candidate_designs = jnp.arange(1, 15, dtype=jnp.float_).reshape(-1, 1)\n\nNUM_STEP = 10_000\noptimizer = sq.optimize.get_default_optimizer(NUM_STEP)\n\neig, aux = sq.boed.estimate_eig(\n    jax.random.key(0),\n    model,\n    marginal_guide,\ncandidate_designs,  # design, or in this case, tensor of possible designs\n    observation_labels=[\"y\"],  # site label of observations, could be a list\n    target_labels=[\n        \"theta\"\n    ],  # site label of 'targets' (latent variables), could also be list\n    num_particles=100,  # number of samples to draw per step in the expectation\n    num_optimization_steps=NUM_STEP,  # number of gradient steps\n    optimizer=optimizer,  # optimizer with learning rate decay\n    final_num_particles=10000,  # at the last step, we draw more samples\n)\n</pre> candidate_designs = jnp.arange(1, 15, dtype=jnp.float_).reshape(-1, 1)  NUM_STEP = 10_000 optimizer = sq.optimize.get_default_optimizer(NUM_STEP)  eig, aux = sq.boed.estimate_eig(     jax.random.key(0),     model,     marginal_guide, candidate_designs,  # design, or in this case, tensor of possible designs     observation_labels=[\"y\"],  # site label of observations, could be a list     target_labels=[         \"theta\"     ],  # site label of 'targets' (latent variables), could also be list     num_particles=100,  # number of samples to draw per step in the expectation     num_optimization_steps=NUM_STEP,  # number of gradient steps     optimizer=optimizer,  # optimizer with learning rate decay     final_num_particles=10000,  # at the last step, we draw more samples ) <pre>|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 12.8s (781.98/s\n</pre> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\nplt.plot(eig, marker='o')\nplt.xlabel(\"$l$\")\nplt.ylabel(\"EIG($l$)\")\n</pre> import matplotlib.pyplot as plt plt.plot(eig, marker='o') plt.xlabel(\"$l$\") plt.ylabel(\"EIG($l$)\") Out[7]: <pre>Text(0, 0.5, 'EIG($l$)')</pre> In\u00a0[8]: Copied! <pre>jnp.argmax(eig) + 1\n</pre> jnp.argmax(eig) + 1 Out[8]: <pre>Array(7, dtype=int64)</pre> In\u00a0[25]: Copied! <pre>from flax import linen as nn\n\nq_prob = nn.sigmoid(aux['params']['theta'])\nprint(\"   l | q(y = 1 | l)\")\nfor (length, q) in zip(candidate_designs, q_prob):\n    print(\"{:&gt;4} | {}\".format(int(length.item()), q.item()))\n</pre> from flax import linen as nn  q_prob = nn.sigmoid(aux['params']['theta']) print(\"   l | q(y = 1 | l)\") for (length, q) in zip(candidate_designs, q_prob):     print(\"{:&gt;4} | {}\".format(int(length.item()), q.item())) <pre>   l | q(y = 1 | l)\n   1 | 0.9854291196099898\n   2 | 0.9672746697025538\n   3 | 0.9327353610497232\n   4 | 0.8702188019632721\n   5 | 0.7730606545062126\n   6 | 0.650433044707223\n   7 | 0.49903727648040747\n   8 | 0.3516913489983096\n   9 | 0.22458600618735214\n  10 | 0.12840463561270873\n  11 | 0.06807842543372478\n  12 | 0.032421556131702306\n  13 | 0.01429967846413999\n  14 | 0.005834729169285767\n</pre> In\u00a0[11]: Copied! <pre>def synthetic_person(length):\n    # The synthetic person can remember any sequence shorter than 6\n    # They cannot remember any sequence of length 6 or above\n    # (There is no randomness in their responses)\n    y = (length &lt; 6).astype(jnp.float_)\n    return y\n</pre> def synthetic_person(length):     # The synthetic person can remember any sequence shorter than 6     # They cannot remember any sequence of length 6 or above     # (There is no randomness in their responses)     y = (length &lt; 6).astype(jnp.float_)     return y In\u00a0[26]: Copied! <pre>ys = jnp.empty(shape=(1,))\nls = jnp.empty(shape=(1,))\nhistory = [(prior_mean, prior_sd)]\n\ncurrent_model = make_model(prior_mean, prior_sd)\n\nfor experiment in range(10):\n    print(\"Round\", experiment + 1)\n\n    # Step 1: compute the optimal length\n    NUM_STEP = 10_000\n    optimizer = sq.optimize.get_default_optimizer(NUM_STEP)\n\n    eig, aux = sq.boed.estimate_eig(\n        jax.random.key(0),\n        current_model,\n        marginal_guide,\n        candidate_designs,  # design, or in this case, tensor of possible designs\n        observation_labels=[\"y\"],  # site label of observations, could be a list\n        target_labels=[\n            \"theta\"\n        ],  # site label of 'targets' (latent variables), could also be list\n        num_particles=1000,  # number of samples to draw per step in the expectation\n        num_optimization_steps=NUM_STEP,  # number of gradient steps\n        optimizer=optimizer,  # optimizer with learning rate decay\n        final_num_particles=10000,  # at the last step, we draw more samples\n    )\n    best_l = jnp.argmax(eig) + 1\n\n    # Step 2: run the experiment, here using the synthetic person\n    print(\"Asking the participant to remember a sequence of length\", int(best_l))\n    y = synthetic_person(best_l)\n    if y:\n        print(\"Participant remembered correctly\")\n    else:\n        print(\"Participant could not remember the sequence\")\n    # Store the sequence length and outcome\n    ls = jnp.concat([ls, jnp.expand_dims(best_l, axis=0)], axis=0)\n    ys = jnp.concat([ys, jnp.expand_dims(y, axis=0)])\n\n    # Step 3: learn the posterior using all data seen so far\n    conditioned_model = handlers.condition(model, {\"y\": ys})\n\n    optimizer = sq.optimize.get_default_optimizer(NUM_STEP)\n    svi = SVI(\n        model=conditioned_model,\n        guide=guide,\n        optim=numpyro.optim.optax_to_numpyro(optimizer),\n        loss=TraceMeanField_ELBO(num_particles=1),\n    )\n\n    svi_results = svi.run(jax.random.key(0), 10_000, ls, progress_bar=True)\n\n    posterior_mean = svi_results.params[\"posterior_mean\"]\n    posterior_sd = svi_results.params[\"posterior_sd\"]\n\n    history.append(\n        (\n            posterior_mean,\n            posterior_sd,\n        )\n    )\n    current_model = make_model(\n        posterior_mean,\n        posterior_sd,\n    )\n    print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1]))\n</pre> ys = jnp.empty(shape=(1,)) ls = jnp.empty(shape=(1,)) history = [(prior_mean, prior_sd)]  current_model = make_model(prior_mean, prior_sd)  for experiment in range(10):     print(\"Round\", experiment + 1)      # Step 1: compute the optimal length     NUM_STEP = 10_000     optimizer = sq.optimize.get_default_optimizer(NUM_STEP)      eig, aux = sq.boed.estimate_eig(         jax.random.key(0),         current_model,         marginal_guide,         candidate_designs,  # design, or in this case, tensor of possible designs         observation_labels=[\"y\"],  # site label of observations, could be a list         target_labels=[             \"theta\"         ],  # site label of 'targets' (latent variables), could also be list         num_particles=1000,  # number of samples to draw per step in the expectation         num_optimization_steps=NUM_STEP,  # number of gradient steps         optimizer=optimizer,  # optimizer with learning rate decay         final_num_particles=10000,  # at the last step, we draw more samples     )     best_l = jnp.argmax(eig) + 1      # Step 2: run the experiment, here using the synthetic person     print(\"Asking the participant to remember a sequence of length\", int(best_l))     y = synthetic_person(best_l)     if y:         print(\"Participant remembered correctly\")     else:         print(\"Participant could not remember the sequence\")     # Store the sequence length and outcome     ls = jnp.concat([ls, jnp.expand_dims(best_l, axis=0)], axis=0)     ys = jnp.concat([ys, jnp.expand_dims(y, axis=0)])      # Step 3: learn the posterior using all data seen so far     conditioned_model = handlers.condition(model, {\"y\": ys})      optimizer = sq.optimize.get_default_optimizer(NUM_STEP)     svi = SVI(         model=conditioned_model,         guide=guide,         optim=numpyro.optim.optax_to_numpyro(optimizer),         loss=TraceMeanField_ELBO(num_particles=1),     )      svi_results = svi.run(jax.random.key(0), 10_000, ls, progress_bar=True)      posterior_mean = svi_results.params[\"posterior_mean\"]     posterior_sd = svi_results.params[\"posterior_sd\"]      history.append(         (             posterior_mean,             posterior_sd,         )     )     current_model = make_model(         posterior_mean,         posterior_sd,     )     print(\"Estimate of \\u03b8: {:.3f} \\u00b1 {:.3f}\\n\".format(*history[-1])) <pre>Round 1\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.4s (543.98/s\nAsking the participant to remember a sequence of length 7\nParticipant could not remember the sequence\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:01&lt;00:00, 9758.19it/s, init loss: 5.6538, avg. loss [9501-10000]: 5.1166]\n</pre> <pre>Estimate of \u03b8: 3.150 \u00b1 1.680\n\nRound 2\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.2s (548.64/s\nAsking the participant to remember a sequence of length 3\nParticipant remembered correctly\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10969.01it/s, init loss: 5.7361, avg. loss [9501-10000]: 5.8018]\n</pre> <pre>Estimate of \u03b8: 4.047 \u00b1 1.412\n\nRound 3\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.3s (546.00/s\nAsking the participant to remember a sequence of length 4\nParticipant remembered correctly\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:01&lt;00:00, 9447.21it/s, init loss: 5.9457, avg. loss [9501-10000]: 6.5046]\n</pre> <pre>Estimate of \u03b8: 4.747 \u00b1 1.231\n\nRound 4\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.6s (538.17/s\nAsking the participant to remember a sequence of length 5\nParticipant remembered correctly\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 12438.26it/s, init loss: 6.4366, avg. loss [9501-10000]: 7.3214]\n</pre> <pre>Estimate of \u03b8: 5.376 \u00b1 1.107\n\nRound 5\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.5s (542.11/s\nAsking the participant to remember a sequence of length 6\nParticipant could not remember the sequence\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10699.34it/s, init loss: 6.8943, avg. loss [9501-10000]: 7.7920]\n</pre> <pre>Estimate of \u03b8: 5.013 \u00b1 0.990\n\nRound 6\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.9s (529.97/s\nAsking the participant to remember a sequence of length 5\nParticipant remembered correctly\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10993.54it/s, init loss: 7.3852, avg. loss [9501-10000]: 8.4891]\n</pre> <pre>Estimate of \u03b8: 5.404 \u00b1 0.908\n\nRound 7\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.3s (546.57/s\nAsking the participant to remember a sequence of length 6\nParticipant could not remember the sequence\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10759.40it/s, init loss: 7.8429, avg. loss [9501-10000]: 8.9584]\n</pre> <pre>Estimate of \u03b8: 5.149 \u00b1 0.839\n\nRound 8\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.3s (545.90/s\nAsking the participant to remember a sequence of length 6\nParticipant could not remember the sequence\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10689.39it/s, init loss: 8.3006, avg. loss [9501-10000]: 9.3473]\n</pre> <pre>Estimate of \u03b8: 4.957 \u00b1 0.792\n\nRound 9\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.7s (534.44/s\nAsking the participant to remember a sequence of length 5\nParticipant remembered correctly\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10414.53it/s, init loss: 8.7915, avg. loss [9501-10000]: 10.0637]\n</pre> <pre>Estimate of \u03b8: 5.227 \u00b1 0.737\n\nRound 10\n|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [100%] in 18.7s (534.10/s\nAsking the participant to remember a sequence of length 6\nParticipant could not remember the sequence\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00&lt;00:00, 10510.62it/s, init loss: 9.2492, avg. loss [9501-10000]: 10.4691]\n</pre> <pre>Estimate of \u03b8: 5.071 \u00b1 0.703\n\n</pre> In\u00a0[39]: Copied! <pre>from jax.scipy.stats import norm\nimport matplotlib.colors as colors\nimport matplotlib.cm as cmx\nimport numpy as np\n\ncmap = plt.get_cmap(\"winter\")\ncNorm = colors.Normalize(vmin=0, vmax=len(history) - 1)\nscalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap)\nplt.figure(figsize=(12, 6))\nx = jnp.linspace(0, 14, 1000)\nfor idx, (mean, sd) in enumerate(history):\n    color = scalarMap.to_rgba(np.array(idx))\n    y = norm.pdf(x, mean, sd)\n    plt.plot(x, y, color=color)\n    plt.xlabel(\"$\\\\theta$\")\n    plt.ylabel(\"p.d.f.\")\nplt.show()\n</pre> from jax.scipy.stats import norm import matplotlib.colors as colors import matplotlib.cm as cmx import numpy as np  cmap = plt.get_cmap(\"winter\") cNorm = colors.Normalize(vmin=0, vmax=len(history) - 1) scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cmap) plt.figure(figsize=(12, 6)) x = jnp.linspace(0, 14, 1000) for idx, (mean, sd) in enumerate(history):     color = scalarMap.to_rgba(np.array(idx))     y = norm.pdf(x, mean, sd)     plt.plot(x, y, color=color)     plt.xlabel(\"$\\\\theta$\")     plt.ylabel(\"p.d.f.\") plt.show() In\u00a0[\u00a0]: Copied! <pre>ls = jnp.arange(1, 11, dtype=jnp.float_)\nys = synthetic_person(ls)\n\nconditioned_model = handlers.condition(model, {\"y\": ys})\n\noptimizer = sq.optimize.get_default_optimizer(NUM_STEP)\nsvi = SVI(\n    model=conditioned_model,\n    guide=guide,\n    optim=numpyro.optim.optax_to_numpyro(optimizer),\n    loss=TraceMeanField_ELBO(num_particles=1),\n)\n\nsvi_results = svi.run(jax.random.key(0), 10_000, ls, progress_bar=True)\n</pre> ls = jnp.arange(1, 11, dtype=jnp.float_) ys = synthetic_person(ls)  conditioned_model = handlers.condition(model, {\"y\": ys})  optimizer = sq.optimize.get_default_optimizer(NUM_STEP) svi = SVI(     model=conditioned_model,     guide=guide,     optim=numpyro.optim.optax_to_numpyro(optimizer),     loss=TraceMeanField_ELBO(num_particles=1), )  svi_results = svi.run(jax.random.key(0), 10_000, ls, progress_bar=True) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:01&lt;00:00, 9577.91it/s, init loss: 1.5913, avg. loss [9501-10000]: 2.6066]\n</pre> In\u00a0[37]: Copied! <pre>svi_results.params\n</pre> svi_results.params Out[37]: <pre>{'posterior_mean': Array(5.80657591, dtype=float64),\n 'posterior_sd': Array(0.91837322, dtype=float64)}</pre> In\u00a0[38]: Copied! <pre>plt.figure(figsize=(12,6))\ny1 = norm.pdf(x, svi_results.params.get(\"posterior_mean\"),\n              svi_results.params.get(\"posterior_sd\"))\ny2 = norm.pdf(x, history[-1][0], history[-1][1])\nplt.plot(x, y1)\nplt.plot(x, y2)\nplt.legend([\"Simple design\", \"Optimal design\"])\nplt.xlabel(\"$\\\\theta$\")\nplt.ylabel(\"p.d.f.\")\nplt.show()\n</pre> plt.figure(figsize=(12,6)) y1 = norm.pdf(x, svi_results.params.get(\"posterior_mean\"),               svi_results.params.get(\"posterior_sd\")) y2 = norm.pdf(x, history[-1][0], history[-1][1]) plt.plot(x, y1) plt.plot(x, y2) plt.legend([\"Simple design\", \"Optimal design\"]) plt.xlabel(\"$\\\\theta$\") plt.ylabel(\"p.d.f.\") plt.show()"},{"location":"guides/working_memory/#working-memory-example","title":"Working memory example\u00b6","text":"<p>Goal</p> <p>         In this notebook, we reproduced the example in this tutorial. Since in the reference, they use <code>pyro</code>, while we use <code>numpyro</code> which do not have <code>contrib.oed</code> implemented, we implement our version and include it in the <code>probabilistic</code> module. The purpose of this notebook is to confirm that our implementation yield a similar result as the reference.      </p>"},{"location":"tutorials/overviews/","title":"Overviews","text":"<p>Goal</p> <p>Hi! This page is intended for user to understand the overall concept of <code>inspeqtor</code> in the high-level first. You might find this page useful when you reviews the interaction between modules and functions offered by <code>inspeqtor</code>.</p> <p>We catergorized characterization and calibration of the quantum device into multiple phase. You might not necessary needs to do or understand every phases and chose to work with specific phases.</p> <pre><code>sequenceDiagram\n    participant User\n    participant Model as Predictive Model\n    participant Device as Quantum Device\n    note over User, Device: Characterization\n    loop \n    User -&gt;&gt; Device: Perform experiments\n    Device -&gt;&gt; User: Data\n    User -&gt;&gt; Model: Characterization\n    opt Selection strategy\n        Model -&gt;&gt; User: Select new experiments\n    end\n    end\n    note over User, Device: Calibration\n    loop Optimization\n    User &lt;&lt;-&gt;&gt; Model: Find the control that &lt;br/&gt; maximize fidelity\n    end\n    User -&gt;&gt; Device: Deploy calibrated control\n    Note over User, Device: Operational\n    loop Operating and monitering\n    User -&gt;&gt; Device: Use device\n    User -&gt;&gt; Device: Check the quality\n    end</code></pre> <ul> <li>Experimental Phase is a preparation of the characterization of the quantum device. It might dictate the constraint of your control calibration too.</li> <li>Characterization Phase</li> <li>Control Calibration Phase</li> </ul> <p>Note</p> <p>We would like to remind user that <code>inspeqtor</code> is a framework. We provide some opinions of how to do things in the characterization and calibration task. Thus, <code>inspeqtor</code> provides user with a varities of utility functions which is designed to be easily replaced by custom function from the user. You don't have to use everything, just what you need \ud83d\ude09</p>"},{"location":"tutorials/overviews/#experimental-phase","title":"Experimental Phase","text":"<p>Diagram below shows the sequence of interaction between, the user, <code>inspeqtor</code>, quantum device, and file system. Please refer to tutorial of how to work with data and experiment using <code>inspeqtor</code> in this tutorial.</p> <pre><code>sequenceDiagram\n    participant User\n    participant Control as Control Sequence\n    participant Device as Quantum Device&lt;br/&gt;(Real or Simulator)\n    participant Data as ExperimentData\n    participant Storage as File System\n\n    User-&gt;&gt;Control: Define atomic control action\n    User-&gt;&gt;Control: Create ControlSequence\n    Control-&gt;&gt;User: Validate &amp; return sequence\n\n    alt Real Hardware\n        User-&gt;&gt;Device: Setup the device\n        Note over Device: Physical quantum device&lt;br/&gt;with real noise &amp; decoherence\n    else Simulation\n        User-&gt;&gt;Device: Setup Hamiltonian &amp; Solver\n        Note over Device: Local simulation&lt;br/&gt;with modeled noise\n    end\n\n    loop For each sample (e.g., 100x)\n        User-&gt;&gt;Control: Sample parameters\n        Control-&gt;&gt;User: Return control params\n        User-&gt;&gt;Device: Execute with params\n        Device-&gt;&gt;User: Return expectation values\n        User-&gt;&gt;Data: Store row with make_row()\n    end\n\n    User-&gt;&gt;Data: Create ExperimentData\n    Data-&gt;&gt;Storage: Save to disk\n    Storage-&gt;&gt;User: Load ExperimentData back\n\n    Note over User,Storage: Same data format regardless&lt;br/&gt;of real device or simulator</code></pre>"},{"location":"tutorials/overviews/#characterization-phase","title":"Characterization Phase","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Data\n    participant Optimizer\n    participant Models\n\n    Note over User,Data: Data Preparation\n    User-&gt;&gt;Data: Prepare &amp; Split Data\n    Data--&gt;&gt;User: Return Training &amp; Testing Data\n\n    Note over User,Models: Model Initialization\n    alt Statistical Model (DNN)\n        User-&gt;&gt;Optimizer: Define Model &amp; Loss Function\n        Optimizer-&gt;&gt;Models: Initialize Parameters\n    else Probabilistic Model (BNN)\n        User-&gt;&gt;Optimizer: Define Model, Prior, Guide, &amp; SVI\n        Optimizer-&gt;&gt;Models: Initialize SVI State\n    end\n\n    Note over Optimizer,Models: Model Training\n    User-&gt;&gt;Optimizer: Start Training Loop\n\n    loop For each epoch\n        Optimizer-&gt;&gt;Models: Update parameters\n        Note right of Optimizer: Validates against testing data\n    end\n\n    Optimizer--&gt;&gt;User: Return Trained Model\n</code></pre>"},{"location":"tutorials/overviews/#alternative-characterization-phase","title":"Alternative Characterization phase","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Strategy as Abstract Strategy\n    participant Model\n    participant Device\n\n    User-&gt;&gt;Strategy: Prepare Experiment\n\n    loop Characterization Loop\n        Strategy-&gt;&gt;Strategy: Select next experiment parameters\n        Note right of Strategy: This can be Random (Open-Loop) or &lt;br&gt; Model-Informed/Adaptive (Closed-Loop).\n\n        Strategy--&gt;&gt;User: Recommend experiment\n\n        User-&gt;&gt;Device: Perform experiment\n        Device--&gt;&gt;User: Measurement data\n\n        User-&gt;&gt;Model: Update/Characterize Model\n\n        Model--&gt;&gt;Strategy: Provide Posterior Model (if adaptive)\n\n        Strategy-&gt;&gt;Strategy: Check termination condition\n    end\n\n    Strategy--&gt;&gt;User: Return Final Characterized Model</code></pre>"},{"location":"tutorials/overviews/#control-calibration-phase","title":"Control Calibration Phase","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Optimizer\n    participant CostFunction as Cost Function &lt;br/&gt; (e.g., Avg. Gate Infidelity)\n    participant Model as PredictiveModel\n    participant Device as Quantum Device&lt;br/&gt;(Real or Simulator)\n\n    Note over User, Model: Starts with Trained Predictive Model from Characterization\n\n    User-&gt;&gt;CostFunction: Define(Target Gate, PredictiveModel)\n    User-&gt;&gt;Optimizer: Start Optimization(CostFunction, Initial Params)\n\n    loop Optimization Steps\n        Optimizer-&gt;&gt;CostFunction: Evaluate(current_params)\n        CostFunction-&gt;&gt;Model: Predict(current_params)\n        Model--&gt;&gt;CostFunction: Return Expectation Values\n        CostFunction--&gt;&gt;Optimizer: Return Loss (Infidelity)\n        Optimizer-&gt;&gt;Optimizer: Update Parameters\n    end\n\n    Optimizer--&gt;&gt;User: Return Optimized Control Parameters\n\n    %% alt Benchmarking\n    User-&gt;&gt;Device: Execute(Optimized Params)\n    Device--&gt;&gt;User: Return Measured Fidelity\n    User-&gt;&gt;Model: Predict(Optimized Params)\n    Model--&gt;&gt;User: Return Predicted Fidelity</code></pre>"},{"location":"tutorials/overviews/#physics","title":"Physics","text":""},{"location":"tutorials/tutorial_0001_dataset/","title":"Dataset Creating","text":"<p>First and foremost, let's import some package that necessary for us to use for our purpose. We also ignore the User warning raised by the ODE solver.</p> In\u00a0[1]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore', category=UserWarning)\n</pre> import warnings warnings.filterwarnings('ignore', category=UserWarning) In\u00a0[2]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport inspeqtor.experimental as sq\nimport pandas as pd\n\nkey = jax.random.key(0)\n</pre> import jax import jax.numpy as jnp import inspeqtor.experimental as sq import pandas as pd  key = jax.random.key(0) <p>If you do not familar with <code>jax.random.key</code>, we refer to their document. But for brevity, you can think of it as how <code>jax</code> generate the pesudo random number.</p> In\u00a0[3]: Copied! <pre>qubit_info = sq.data.QubitInformation(\n    unit=\"GHz\",\n    qubit_idx=0,\n    anharmonicity=-0.2,\n    frequency=5.0,\n    drive_strength=0.1,\n)\n\nqubit_info\n</pre> qubit_info = sq.data.QubitInformation(     unit=\"GHz\",     qubit_idx=0,     anharmonicity=-0.2,     frequency=5.0,     drive_strength=0.1, )  qubit_info Out[3]: <pre>QubitInformation(unit='GHz', qubit_idx=0, anharmonicity=-0.2, frequency=5.0, drive_strength=0.1, date='2025-08-24 14:03:48')</pre> In\u00a0[\u00a0]: Copied! <pre>total_length = 320\ndt = 2 / 9\npulse = sq.predefined.DragPulseV2(\n    duration=total_length,\n    qubit_drive_strength=qubit_info.drive_strength,\n    dt=2 / 9,\n    max_amp=0.5,\n    min_theta=0,\n    max_theta=2 * jnp.pi,\n    min_beta=-5.0,\n    max_beta=5.0,\n)\n\ncontrol_sequence = sq.control.ControlSequence(\n    controls=[\n        pulse,\n    ],\n    total_dt=total_length,\n)\n</pre> total_length = 320 dt = 2 / 9 pulse = sq.predefined.DragPulseV2(     duration=total_length,     qubit_drive_strength=qubit_info.drive_strength,     dt=2 / 9,     max_amp=0.5,     min_theta=0,     max_theta=2 * jnp.pi,     min_beta=-5.0,     max_beta=5.0, )  control_sequence = sq.control.ControlSequence(     controls=[         pulse,     ],     total_dt=total_length, ) <p>By default, <code>ControlSequence</code> performs validation during the object instantiation to detect early bugs. We can see our control in action by sample from it.</p> In\u00a0[5]: Copied! <pre>key, subkey = jax.random.split(key)\nparams = control_sequence.sample_params(subkey)\nparams\n</pre> key, subkey = jax.random.split(key) params = control_sequence.sample_params(subkey) params Out[5]: <pre>[{'theta': Array(4.3855351, dtype=float64),\n  'beta': Array(-4.46933642, dtype=float64)}]</pre> <p>We can also visualize the control sequence waveform by</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(6, 3))\n\nt_eval = jnp.linspace(0, total_length, total_length)\n\nsq.visualization.plot_control_envelope(\n    control_sequence.get_waveform(params), t_eval, ax\n)\n</pre> import matplotlib.pyplot as plt  fig, ax = plt.subplots(figsize=(6, 3))  t_eval = jnp.linspace(0, total_length, total_length)  sq.visualization.plot_control_envelope(     control_sequence.get_waveform(params), t_eval, ax ) In\u00a0[7]: Copied! <pre>from functools import partial\n\nsignal_fn = sq.physics.signal_func_v3(\n    get_envelope=control_sequence.get_envelope,\n    drive_frequency=qubit_info.frequency,\n    dt=dt,\n)\n\nhamiltonian = partial(\n    sq.predefined.transmon_hamiltonian, qubit_info=qubit_info, signal=signal_fn\n)\nframe = (jnp.pi * qubit_info.frequency) * sq.constant.Z\nhamiltonian = sq.physics.auto_rotating_frame_hamiltonian(hamiltonian, frame=frame)\n</pre> from functools import partial  signal_fn = sq.physics.signal_func_v3(     get_envelope=control_sequence.get_envelope,     drive_frequency=qubit_info.frequency,     dt=dt, )  hamiltonian = partial(     sq.predefined.transmon_hamiltonian, qubit_info=qubit_info, signal=signal_fn ) frame = (jnp.pi * qubit_info.frequency) * sq.constant.Z hamiltonian = sq.physics.auto_rotating_frame_hamiltonian(hamiltonian, frame=frame) <p>Then, we can solve the Schrodinger equation using <code>sq.physics.solver</code>. By itself, <code>sq.physics.solver</code> is a function of multiple arguments that will solve the system dynamics on the call. But most of the time, we just want to solve the dynamics of the system with different control parameters, and keep the rest fixed. So the common pattern of using <code>sq.physics.solver</code> is to used it with <code>partial</code> as follows.</p> In\u00a0[8]: Copied! <pre>solver = partial(\n    sq.physics.solver,\n    t_eval=t_eval * dt, # nanosecond.\n    hamiltonian=hamiltonian,\n    y0=jnp.eye(2, dtype=jnp.complex_),\n    t0=0.0, # nanosecond.\n    t1=total_length * dt, # nanosecond.\n)\n</pre> solver = partial(     sq.physics.solver,     t_eval=t_eval * dt, # nanosecond.     hamiltonian=hamiltonian,     y0=jnp.eye(2, dtype=jnp.complex_),     t0=0.0, # nanosecond.     t1=total_length * dt, # nanosecond. ) <p>As per <code>inspeqtor</code> specification, the <code>hamiltonian</code> function required by <code>solver</code> should be a function of two arguments. The first argument can be arbitary while the second argument should be the a scalar of time.</p> In\u00a0[9]: Copied! <pre>signal_params = sq.physics.SignalParameters(pulse_params=params, phase=0)\nhamiltonian(signal_params, jnp.array(0))\n</pre> signal_params = sq.physics.SignalParameters(pulse_params=params, phase=0) hamiltonian(signal_params, jnp.array(0)) Out[9]: <pre>Array([[0.00000000e+00+0.j, 1.06878303e-05+0.j],\n       [1.06878303e-05+0.j, 0.00000000e+00+0.j]], dtype=complex128)</pre> <p>Now, we can solve the system dynamics by using the <code>solver</code> as a single argument function.</p> In\u00a0[10]: Copied! <pre>unitary = solver(signal_params)\n</pre> unitary = solver(signal_params) <p>We can visualize the trajectory of the system using the <code>visualization</code> module for quick inspection.</p> In\u00a0[11]: Copied! <pre>fig, axes = sq.visualization.plot_expectation_values(\n    sq.visualization.format_expectation_values(\n        sq.predefined.calculate_expectation_values(unitary).T\n    ),\n    title=\"Sample trajectory\",\n)\n</pre> fig, axes = sq.visualization.plot_expectation_values(     sq.visualization.format_expectation_values(         sq.predefined.calculate_expectation_values(unitary).T     ),     title=\"Sample trajectory\", ) <p>We saw that by using <code>sq.physics.signal_func_v3</code> as a signal function wih the hamiltonian in the <code>predefined</code> module require using <code>sq.physics.SignalParameters</code> as the first argument for <code>solver</code>. The advantage of this approach is the explicitness of the control parameters. While it needs a lot of setup for it to working properly.</p> <p>Alternatively, we can use <code>sq.physics.signal_func_v5</code> which will results in an <code>jnp.array</code> as the first argument. This approach might be preferred for model training, vectorization, and compatability with other library.</p> In\u00a0[12]: Copied! <pre>signal_fn = sq.physics.signal_func_v5(\n    # We have to transform the usual envelope with helper function.\n    get_envelope=sq.control.get_envelope_transformer(control_sequence),\n    drive_frequency=qubit_info.frequency,\n    dt=dt,\n)\n\nhamiltonian = partial(\n    sq.predefined.transmon_hamiltonian, qubit_info=qubit_info, signal=signal_fn\n)\nhamiltonian = sq.physics.auto_rotating_frame_hamiltonian(hamiltonian, frame=frame)\n\nsolver = partial(\n    sq.physics.solver,\n    t_eval=t_eval * dt, # nanosecond.\n    hamiltonian=hamiltonian,\n    y0=jnp.eye(2, dtype=jnp.complex_),\n    t0=0.0, # nanosecond.\n    t1=total_length * dt, # nanosecond.\n)\n</pre> signal_fn = sq.physics.signal_func_v5(     # We have to transform the usual envelope with helper function.     get_envelope=sq.control.get_envelope_transformer(control_sequence),     drive_frequency=qubit_info.frequency,     dt=dt, )  hamiltonian = partial(     sq.predefined.transmon_hamiltonian, qubit_info=qubit_info, signal=signal_fn ) hamiltonian = sq.physics.auto_rotating_frame_hamiltonian(hamiltonian, frame=frame)  solver = partial(     sq.physics.solver,     t_eval=t_eval * dt, # nanosecond.     hamiltonian=hamiltonian,     y0=jnp.eye(2, dtype=jnp.complex_),     t0=0.0, # nanosecond.     t1=total_length * dt, # nanosecond. ) <p>We also provide a helper function to convert the <code>list[ParameterDictType]</code> returned from the <code>sample_params</code> method to <code>jnp.narray</code> and vice versa.</p> In\u00a0[13]: Copied! <pre>a2l_fn, l2a_fn = sq.control.get_param_array_converter(control_sequence)\narray_params = l2a_fn(params)\narray_params\n</pre> a2l_fn, l2a_fn = sq.control.get_param_array_converter(control_sequence) array_params = l2a_fn(params) array_params Out[13]: <pre>Array([ 4.3855351 , -4.46933642], dtype=float64)</pre> In\u00a0[14]: Copied! <pre>reverted_params = a2l_fn(array_params)\nreverted_params\n</pre> reverted_params = a2l_fn(array_params) reverted_params Out[14]: <pre>[{'theta': Array(4.3855351, dtype=float64),\n  'beta': Array(-4.46933642, dtype=float64)}]</pre> <p>Now, we can simply solve the system dynamics with <code>solver</code> and yield the same result.</p> In\u00a0[15]: Copied! <pre>unitary_a = solver(array_params)\n\n# Assert that they yield the same unitary \ud83d\ude0e\nassert jnp.allclose(unitary_a, unitary)\n\nfig, axes = sq.visualization.plot_expectation_values(\n    sq.visualization.format_expectation_values(\n        sq.predefined.calculate_expectation_values(unitary_a).T\n    ),\n    title=\"Sample trajectory\",\n)\n</pre> unitary_a = solver(array_params)  # Assert that they yield the same unitary \ud83d\ude0e assert jnp.allclose(unitary_a, unitary)  fig, axes = sq.visualization.plot_expectation_values(     sq.visualization.format_expectation_values(         sq.predefined.calculate_expectation_values(unitary_a).T     ),     title=\"Sample trajectory\", ) <p>What we can measure in the experiment is not the unitary, but the finite-shot expectation value. <code>inspeqtor</code> provides a helper function <code>sq.predefined.shot_quantum_device</code> to turn the solver into just that! Again, we use <code>partial</code> to fix the <code>solver</code> and the number <code>SHOTS</code>. Note that, it is up to user to decide the partial strategy to suit their use case.</p> In\u00a0[16]: Copied! <pre>SHOTS = 1000\nquantum_device = partial(\n    sq.predefined.shot_quantum_device,\n    solver=solver,\n    SHOTS=SHOTS,\n    expectation_value_receipt=sq.constant.default_expectation_values_order,\n)\n\n# Since, shot_quantum_device is a stochastic function, we have to provide the random key.\nkey, subkey = jax.random.split(key)\n# Here the function accept a batch of control parameters, so we have to reshape it to have a batch dimension.\nexpvals = quantum_device(subkey, array_params.reshape(1, -1))\nexpvals\n</pre> SHOTS = 1000 quantum_device = partial(     sq.predefined.shot_quantum_device,     solver=solver,     SHOTS=SHOTS,     expectation_value_receipt=sq.constant.default_expectation_values_order, )  # Since, shot_quantum_device is a stochastic function, we have to provide the random key. key, subkey = jax.random.split(key) # Here the function accept a batch of control parameters, so we have to reshape it to have a batch dimension. expvals = quantum_device(subkey, array_params.reshape(1, -1)) expvals Out[16]: <pre>Array([[ 0.924, -0.91 , -0.17 ,  0.242, -0.272,  0.272,  0.216, -0.228,\n        -0.368,  0.328,  0.896, -0.934, -0.26 ,  0.32 , -0.918,  0.922,\n        -0.27 ,  0.266]], dtype=float64)</pre> <p>We saw that the output is an array of shape <code>(batch, 18)</code> where the number 18 is the complete combinatation of initial states and Pauli observables. The order of the expectation values array is order as the same as provided <code>expectation_value_receipt</code> argument. in this case, the order is the default order used throughout <code>inspeqtor</code>.</p> <p>It is important to note that, <code>inspeqtor</code> provide its functionality as functions so that user can provide a custom behavior by simply define function that have the same interface with the functions provided by <code>inspeqtor</code>. For example, <code>solver</code> that <code>sq.predefined.shot_quantum_device</code> is just a function of control parameters that return unitary at each time step as a result. Thus user can switch it out to user-defined solver such as physics-informed neural network.</p> <p>With the quantum device ready to use, we proceed with experimental data collection. To handle performing experiment in at once, in batch, or a single experiment at a time, we model each control parameter as a single row of the <code>pd.DataFrame</code> table (you can think of it as a sql table as well). Here we provide <code>sq.data.make_row</code> to enforce the schema. Let us see it in action by consider the experiment with initial state $|+\\rangle$ and measure in $\\hat{X}$ basis.</p> In\u00a0[17]: Copied! <pre>exp = sq.constant.default_expectation_values_order[0]\nexp\n</pre> exp = sq.constant.default_expectation_values_order[0] exp Out[17]: <pre>ExpectationValue(initial_state=\"+\", observable=\"X\", expectation_value=None)</pre> <p>Don't worry about <code>expectation_value = None</code>, since we merely use <code>ExpectationValue</code> in <code>sq.constant.default_expectation_values_order</code> for the code completion.</p> In\u00a0[18]: Copied! <pre>sq.data.make_row(\n    expectation_value=expvals[0][0].item(),\n    initial_state=exp.initial_state,\n    observable=exp.observable,\n    parameters_id=0,\n    parameters_list=params,\n    custom_id='stardust'\n)\n</pre> sq.data.make_row(     expectation_value=expvals[0][0].item(),     initial_state=exp.initial_state,     observable=exp.observable,     parameters_id=0,     parameters_list=params,     custom_id='stardust' ) Out[18]: <pre>{'expectation_value': 0.924,\n 'initial_state': '+',\n 'observable': 'X',\n 'parameters_id': 0,\n 'parameter/0/theta': 4.385535096915008,\n 'parameter/0/beta': -4.469336421734063,\n 'custom_id': 'stardust'}</pre> <p>The returned value is a python dictionary with parsed keys required by <code>inspeqtor</code> specification for dataset schema. Note that, you can store additional custom information by providing <code>kwargs</code> to the function. Here we provided <code>custom_id = \"Auspicious Elephant\"</code> as an example.</p> <p>As a good practice, we will define experiment configuration first before performing the dataset as it is required later. However, user can do it after the experiment as appropiate.</p> In\u00a0[\u00a0]: Copied! <pre>config = sq.data.ExperimentConfiguration(\n    qubits=[qubit_info],\n    expectation_values_order=sq.constant.default_expectation_values_order,\n    parameter_names=control_sequence.get_parameter_names(),\n    backend_name=\"red_demon\",\n    shots=SHOTS,\n    EXPERIMENT_IDENTIFIER=\"0001\",\n    EXPERIMENT_TAGS=[\"tutorial\", \"for\", \"you\", \"\ud83d\ude09\"],\n    description=\"One impossible step at a time\",\n    device_cycle_time_ns=dt,\n    sequence_duration_dt=control_sequence.total_dt,\n    instance=\"black_rose\",\n    sample_size=100,\n)\n</pre> config = sq.data.ExperimentConfiguration(     qubits=[qubit_info],     expectation_values_order=sq.constant.default_expectation_values_order,     parameter_names=control_sequence.get_parameter_names(),     backend_name=\"red_demon\",     shots=SHOTS,     EXPERIMENT_IDENTIFIER=\"0001\",     EXPERIMENT_TAGS=[\"tutorial\", \"for\", \"you\", \"\ud83d\ude09\"],     description=\"One impossible step at a time\",     device_cycle_time_ns=dt,     sequence_duration_dt=control_sequence.total_dt,     instance=\"black_rose\",     sample_size=100, ) <p>Now, we proceed to perform a series of experiment and store the data in the <code>pd.DataFrame</code> instance as follows. First, we sample the control parameters and store them in a ready-to-use format.</p> In\u00a0[20]: Copied! <pre>control_params_list = []\ntemp_control_params: list[jnp.ndarray] = []\n\nkey = jax.random.key(0)\n\nfor control_idx in range(config.sample_size):\n    key, subkey = jax.random.split(key)\n    params = control_sequence.sample_params(subkey)\n    \n    control_params_list.append(params)\n    temp_control_params.append(l2a_fn(params))\n\ncontrol_params = jnp.array(temp_control_params)\ncontrol_params.shape\n</pre> control_params_list = [] temp_control_params: list[jnp.ndarray] = []  key = jax.random.key(0)  for control_idx in range(config.sample_size):     key, subkey = jax.random.split(key)     params = control_sequence.sample_params(subkey)          control_params_list.append(params)     temp_control_params.append(l2a_fn(params))  control_params = jnp.array(temp_control_params) control_params.shape Out[20]: <pre>(100, 2)</pre> <p>For simplicity, we perform experiments in a single batches.</p> In\u00a0[21]: Copied! <pre>key, subkey = jax.random.split(key)\nexpectation_values = quantum_device(subkey, control_params)\nexpectation_values.shape\n</pre> key, subkey = jax.random.split(key) expectation_values = quantum_device(subkey, control_params) expectation_values.shape Out[21]: <pre>(100, 18)</pre> <p>Next, we store the experimetal data using <code>sq.data.make_row</code> in the intermediate <code>pd.DataFrame</code> instance.</p> In\u00a0[22]: Copied! <pre>rows = []\nfor sample_idx in range(config.sample_size):\n\n    for exp_idx, exp in enumerate(sq.constant.default_expectation_values_order):\n        row = sq.data.make_row(\n            expectation_value=float(expectation_values[sample_idx, exp_idx]),\n            initial_state=exp.initial_state,\n            observable=exp.observable,\n            parameters_list=control_params_list[sample_idx],\n            parameters_id=sample_idx,\n            custom_id=f'{sample_idx}/{exp_idx}'\n        )\n\n        rows.append(row)\n\ndf = pd.DataFrame(rows)\ndf\n</pre> rows = [] for sample_idx in range(config.sample_size):      for exp_idx, exp in enumerate(sq.constant.default_expectation_values_order):         row = sq.data.make_row(             expectation_value=float(expectation_values[sample_idx, exp_idx]),             initial_state=exp.initial_state,             observable=exp.observable,             parameters_list=control_params_list[sample_idx],             parameters_id=sample_idx,             custom_id=f'{sample_idx}/{exp_idx}'         )          rows.append(row)  df = pd.DataFrame(rows) df Out[22]: expectation_value initial_state observable parameters_id parameter/0/theta parameter/0/beta custom_id 0 0.914 + X 0 4.385535 -4.469336 0/0 1 -0.932 - X 0 4.385535 -4.469336 0/1 2 -0.176 r X 0 4.385535 -4.469336 0/2 3 0.238 l X 0 4.385535 -4.469336 0/3 4 -0.324 0 X 0 4.385535 -4.469336 0/4 ... ... ... ... ... ... ... ... 1795 -0.040 - Z 99 6.077224 1.334615 99/13 1796 -0.206 r Z 99 6.077224 1.334615 99/14 1797 0.196 l Z 99 6.077224 1.334615 99/15 1798 0.978 0 Z 99 6.077224 1.334615 99/16 1799 -0.982 1 Z 99 6.077224 1.334615 99/17 <p>1800 rows \u00d7 7 columns</p> <p>Finally, we can store our data using <code>sq.data.ExperimentData</code>!</p> In\u00a0[23]: Copied! <pre>exp_data = sq.data.ExperimentData(\n    experiment_config=config,\n    preprocess_data=df\n)\n</pre> exp_data = sq.data.ExperimentData(     experiment_config=config,     preprocess_data=df ) In\u00a0[24]: Copied! <pre>exp_data.postprocessed_data\n</pre> exp_data.postprocessed_data Out[24]: parameters_id expectation_value/+/X expectation_value/-/X expectation_value/r/X expectation_value/l/X expectation_value/0/X expectation_value/1/X expectation_value/+/Y expectation_value/-/Y expectation_value/r/Y ... expectation_value/r/Z expectation_value/l/Z expectation_value/0/Z expectation_value/1/Z expectation_value initial_state observable parameter/0/theta parameter/0/beta custom_id 0 0 0.914 -0.932 -0.176 0.238 -0.324 0.280 0.244 -0.194 -0.336 ... -0.924 0.926 -0.254 0.232 0.914 + X 4.385535 -4.469336 0/0 1 1 1.000 -1.000 -0.042 -0.006 0.030 -0.006 -0.042 0.000 0.958 ... 0.254 -0.218 0.980 -0.958 1.000 + X 0.276427 3.608326 1/0 2 2 0.974 -0.972 0.214 -0.206 0.124 -0.088 -0.162 0.180 0.462 ... -0.848 0.888 0.426 -0.434 0.974 + X 5.154778 2.333456 2/0 3 3 0.966 -0.968 -0.098 0.080 -0.226 0.218 0.106 -0.148 -0.604 ... -0.838 0.792 -0.542 0.598 0.966 + X 4.094562 -2.803718 3/0 4 4 1.000 -0.994 0.034 0.002 0.080 -0.068 0.098 -0.032 -0.754 ... 0.590 -0.608 -0.770 0.772 1.000 + X 2.458046 2.477768 4/0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 95 95 1.000 -1.000 0.016 -0.004 0.034 0.010 -0.008 -0.008 1.000 ... 0.042 0.006 1.000 -1.000 1.000 + X 0.007379 0.258322 95/0 96 96 1.000 -1.000 0.054 -0.016 0.000 0.010 -0.106 0.018 0.456 ... 0.898 -0.890 0.424 -0.414 1.000 + X 1.112030 -4.666942 96/0 97 97 1.000 -1.000 0.080 -0.014 0.012 0.026 0.016 -0.012 0.992 ... 0.066 -0.104 0.994 -0.998 1.000 + X 0.119057 -3.652233 97/0 98 98 1.000 -1.000 -0.048 -0.004 0.000 0.040 0.004 0.016 0.902 ... 0.426 -0.450 0.898 -0.920 1.000 + X 0.430843 3.605063 98/0 99 99 0.984 -0.996 0.044 -0.108 -0.026 0.028 -0.106 0.102 0.974 ... -0.206 0.196 0.978 -0.982 0.984 + X 6.077224 1.334615 99/0 <p>100 rows \u00d7 25 columns</p> <p>Note that <code>exp_data.postprocessed_data</code> is a dataframe that created from the aggregation of the <code>exp_data.preprocess_data</code> by combinding rows with the same <code>parameters_id</code>. So the extra fields provided by the user will be just the first entry of the expectaion values combination.</p> In\u00a0[25]: Copied! <pre>import tempfile\nfrom pathlib import Path\n\n# path = Path(\"./test_data_v1\")\ntmpdir = tempfile.TemporaryDirectory()\npath = Path(tmpdir.name)\n\n# Create the path with parents if not existed already\npath.mkdir(parents=True, exist_ok=True)\n# Save the experiment with a single liner \ud83d\ude09.\nsq.predefined.save_data_to_path(path, exp_data, control_sequence)\n</pre> import tempfile from pathlib import Path  # path = Path(\"./test_data_v1\") tmpdir = tempfile.TemporaryDirectory() path = Path(tmpdir.name)  # Create the path with parents if not existed already path.mkdir(parents=True, exist_ok=True) # Save the experiment with a single liner \ud83d\ude09. sq.predefined.save_data_to_path(path, exp_data, control_sequence) In\u00a0[26]: Copied! <pre>loaded_data = sq.predefined.load_data_from_path(\n    path,\n    hamiltonian_spec=sq.predefined.HamiltonianSpec(\n        method=sq.predefined.WhiteboxStrategy.TROTTER,\n        trotter_steps=1_000,\n    ),\n)\n\ntmpdir.cleanup()\n</pre> loaded_data = sq.predefined.load_data_from_path(     path,     hamiltonian_spec=sq.predefined.HamiltonianSpec(         method=sq.predefined.WhiteboxStrategy.TROTTER,         trotter_steps=1_000,     ), )  tmpdir.cleanup() <p>Note that in the case of custom solver, user has to manually load the experimental dataset back by the primitive load method of <code>sq.data.ExperimentData</code> and solve for the unitary by the solver. User can also bundle the data by manually instantiate the <code>sq.utils.LoadedData</code> for further usage too.</p>"},{"location":"tutorials/tutorial_0001_dataset/#dataset-creating","title":"Dataset Creating\u00b6","text":"<p>Goal</p> <p>         Before any characterization and control calibration, we have to gather the data for us to extract information about the target quantum device first. The central object for holding data for us is <code>ExperimentData</code> instance. It is responsible for saving and loading dataset to and from disk. Furthermore, it allows us to organize the dataset better. Thus, the goal of this tutorial is to create the instance of <code>ExperimentData</code>.     </p>"},{"location":"tutorials/tutorial_0001_dataset/#prepare-experiments","title":"Prepare experiments\u00b6","text":"<p>We break the experiment preparation phase into the following steps.</p> <ul> <li>Gathering \"prior\" information about the quantum device.</li> <li>Defining the control action.</li> </ul>"},{"location":"tutorials/tutorial_0001_dataset/#quantum-device-specification","title":"Quantum device specification\u00b6","text":"<p>In <code>inspeqtor</code>, we focus on characterizing quantum device. In the finest level, user most likely want to perform control calibration on individual qubit which is part of the full system. Thus, we provide a dedicated <code>dataclass</code> responsible for holding \"prior\" information about the qubit. The information is often necessary for constructing the subsystem Hamiltonian which is used for open-loop optimization. Below is the code snippet to initialize <code>QubitInformation</code> object.</p>"},{"location":"tutorials/tutorial_0001_dataset/#define-the-control","title":"Define the control\u00b6","text":"<p>For composability of control action, we let user define an \"atomic\" control action by inheriting <code>Control</code> dataclass, then compose them together via <code>ControlSequence</code> class. Below is an example of defining the total control action with only single predefined <code>DragPulseV2</code>.</p>"},{"location":"tutorials/tutorial_0001_dataset/#perform-experiment","title":"Perform experiment\u00b6","text":"<p>Now, we are ready to perform an experiment on the quantum device. In this tutorial, we will use a local simulator to generate the data for us. In fact, most of the time, we might want to work on the local before performing experiment on the quantum device without confidence.</p> <p>Either ways, <code>inspeqtor</code> provides a unified and flexible pipeline for user to populate the data and store it with <code>ExperimentData</code> object. For now, let us shift our attention a bit to the quantum device simulator.</p> <p>As an example, we define our device as a single transmon qubit. We can also rotate the Hamiltonian with a frame as follows.</p>"},{"location":"tutorials/tutorial_0001_dataset/#save-the-experiment","title":"Save the experiment\u00b6","text":"<p>There are several ways to save <code>sq.data.ExperimentData</code>. For the most common usage pattern, we provides a pair of save and load function in the <code>predefined</code> module. The <code>sq.predefined.save_data_to_path</code> function will save the dataset in the format that can be easily load back using <code>sq.predefined.load_data_from_path</code>.</p>"},{"location":"tutorials/tutorial_0001_dataset/#load-the-experiment","title":"Load the experiment\u00b6","text":"<p>Loading just <code>sq.data.ExperimentData</code> back is often not enough. We load the data back into a bundle of <code>sq.utils.LoadedData</code> dataclass instance. The dataclass provides access to the <code>sq.data.ExperimentData</code>, <code>sq.control.ControlSequence</code>, <code>solver</code> with provided specification of Hamiltonian <code>sq.predefined.HamiltonianSpec</code> and the <code>jnp.ndarray</code> of control parameters, ideal unitary operators, and the corresponding expectation values.</p>"},{"location":"tutorials/tutorial_0002_control/","title":"Control Action","text":""},{"location":"tutorials/tutorial_0002_control/#control-action","title":"Control Action\u00b6","text":"<p>TODO</p> <p>         This tutorial will show how to define, use, and custom the <code>sq.control</code> module.         <ul> <li>How to define custom control</li> <li>How to save and load custom control using control reader</li> </ul> </p>"},{"location":"tutorials/tutorial_0003_cc/","title":"Characterization and Calibration using Graybox","text":"In\u00a0[1]: Copied! <pre>import jax\nimport jax.numpy as jnp\nimport inspeqtor.experimental as sq\n</pre> import jax import jax.numpy as jnp import inspeqtor.experimental as sq In\u00a0[2]: Copied! <pre>data_model = sq.predefined.get_predefined_data_model_m1()\n\n# Now, we use the noise model to performing the data using simulator.\nexp_data, _, _, _ = sq.predefined.generate_experimental_data(\n    key=jax.random.key(0),\n    hamiltonian=data_model.total_hamiltonian,\n    sample_size=100,\n    strategy=sq.predefined.SimulationStrategy.SHOT,\n    get_qubit_information_fn=lambda: data_model.qubit_information,\n    get_control_sequence_fn=lambda: data_model.control_sequence,\n    method=sq.predefined.WhiteboxStrategy.TROTTER,\n    trotter_steps=10_000,\n)\n\n# Now we can prepare the dataset that ready to use.\nwhitebox = sq.physics.make_trotterization_solver(\n    data_model.ideal_hamiltonian,\n    data_model.control_sequence,\n    data_model.dt,\n    trotter_steps=10_000,\n)\nloaded_data = sq.utils.prepare_data(\n    exp_data, data_model.control_sequence, whitebox\n)\n</pre> data_model = sq.predefined.get_predefined_data_model_m1()  # Now, we use the noise model to performing the data using simulator. exp_data, _, _, _ = sq.predefined.generate_experimental_data(     key=jax.random.key(0),     hamiltonian=data_model.total_hamiltonian,     sample_size=100,     strategy=sq.predefined.SimulationStrategy.SHOT,     get_qubit_information_fn=lambda: data_model.qubit_information,     get_control_sequence_fn=lambda: data_model.control_sequence,     method=sq.predefined.WhiteboxStrategy.TROTTER,     trotter_steps=10_000, )  # Now we can prepare the dataset that ready to use. whitebox = sq.physics.make_trotterization_solver(     data_model.ideal_hamiltonian,     data_model.control_sequence,     data_model.dt,     trotter_steps=10_000, ) loaded_data = sq.utils.prepare_data(     exp_data, data_model.control_sequence, whitebox ) <p>We can inspect the experiment configuration from predefined noise model using <code>display</code> function from <code>flax.nnx</code></p> In\u00a0[3]: Copied! <pre>from flax.nnx import display\n\ndisplay(loaded_data.experiment_data.experiment_config)\n</pre> from flax.nnx import display  display(loaded_data.experiment_data.experiment_config) <p>And the experimental data,</p> In\u00a0[4]: Copied! <pre>loaded_data.experiment_data.postprocessed_data\n</pre> loaded_data.experiment_data.postprocessed_data Out[4]: parameters_id expectation_value/+/X expectation_value/-/X expectation_value/r/X expectation_value/l/X expectation_value/0/X expectation_value/1/X expectation_value/+/Y expectation_value/-/Y expectation_value/r/Y ... expectation_value/-/Z expectation_value/r/Z expectation_value/l/Z expectation_value/0/Z expectation_value/1/Z expectation_value initial_state observable parameter/0/theta parameter/0/beta 0 0 0.982 -0.954 0.244 -0.188 -0.042 -0.010 -0.228 0.260 0.934 ... -0.010 0.264 -0.234 0.958 -0.962 0.982 + X 0.276427 3.608326 1 1 0.998 -0.994 -0.064 0.132 -0.020 0.026 0.084 -0.130 0.442 ... 0.054 -0.894 0.886 0.446 -0.486 0.998 + X 5.154778 2.333456 2 2 0.972 -0.972 0.096 -0.130 0.236 -0.272 -0.106 0.168 -0.580 ... -0.202 -0.774 0.784 -0.590 0.522 0.972 + X 4.094562 -2.803718 3 3 0.976 -0.978 0.058 -0.052 -0.152 0.190 -0.100 0.038 -0.784 ... 0.198 0.574 -0.614 -0.766 0.782 0.976 + X 2.458046 2.477768 4 4 0.838 -0.826 0.444 -0.414 0.312 -0.310 -0.484 0.438 0.392 ... -0.220 -0.766 0.764 0.540 -0.560 0.838 + X 5.240611 -4.604616 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 95 95 0.992 -0.986 0.148 -0.168 -0.086 0.082 -0.154 0.142 0.472 ... 0.076 0.892 -0.896 0.392 -0.474 0.992 + X 1.112030 -4.666942 96 96 0.972 -0.974 0.226 -0.200 0.010 0.022 -0.212 0.246 0.972 ... -0.070 0.180 -0.148 0.996 -0.992 0.972 + X 0.119057 -3.652233 97 97 0.976 -0.970 0.204 -0.224 -0.118 0.010 -0.228 0.208 0.900 ... 0.054 0.406 -0.420 0.898 -0.906 0.976 + X 0.430843 3.605063 98 98 1.000 -0.998 0.026 -0.070 0.006 -0.018 -0.028 0.036 0.992 ... -0.030 -0.150 0.158 0.972 -0.978 1.000 + X 6.077224 1.334615 99 99 0.952 -0.964 -0.232 0.248 -0.110 0.136 0.224 -0.212 0.482 ... 0.126 -0.862 0.870 0.514 -0.532 0.952 + X 5.171392 4.308521 <p>100 rows \u00d7 24 columns</p> In\u00a0[5]: Copied! <pre># Here, we just bundling things up for convinience uses.\nkey = jax.random.key(0)\nkey, random_split_key, training_key = jax.random.split(key, 3)\n(\n    train_control_parameters,\n    train_unitaries,\n    train_expectation_values,\n    test_control_paramaeters,\n    test_unitaries,\n    test_expectation_values,\n) = sq.utils.random_split(\n    random_split_key,\n    int(loaded_data.control_parameters.shape[0] * 0.1),  # Test size\n    loaded_data.control_parameters,\n    loaded_data.unitaries,\n    loaded_data.expectation_values,\n)\n</pre> # Here, we just bundling things up for convinience uses. key = jax.random.key(0) key, random_split_key, training_key = jax.random.split(key, 3) (     train_control_parameters,     train_unitaries,     train_expectation_values,     test_control_paramaeters,     test_unitaries,     test_expectation_values, ) = sq.utils.random_split(     random_split_key,     int(loaded_data.control_parameters.shape[0] * 0.1),  # Test size     loaded_data.control_parameters,     loaded_data.unitaries,     loaded_data.expectation_values, ) <p>We going to pass the data back and forth a lot, so we use a helper <code>DataBundled</code> to hold the dataset. The advantange of using this <code>dataclass</code> is that we have a code completion.</p> In\u00a0[6]: Copied! <pre>train_data = sq.optimize.DataBundled(\n    control_params=sq.predefined.drag_feature_map(train_control_parameters),\n    unitaries=train_unitaries,\n    observables=train_expectation_values,\n)\n\ntest_data = sq.optimize.DataBundled(\n    control_params=sq.predefined.drag_feature_map(test_control_paramaeters),\n    unitaries=test_unitaries,\n    observables=test_expectation_values,\n)\n</pre> train_data = sq.optimize.DataBundled(     control_params=sq.predefined.drag_feature_map(train_control_parameters),     unitaries=train_unitaries,     observables=train_expectation_values, )  test_data = sq.optimize.DataBundled(     control_params=sq.predefined.drag_feature_map(test_control_paramaeters),     unitaries=test_unitaries,     observables=test_expectation_values, ) <p>Now, we setup the optimizer with the number of epoches.</p> In\u00a0[7]: Copied! <pre>NUM_EPOCH = 5000\noptimizer = sq.optimize.get_default_optimizer(8 * NUM_EPOCH)\n</pre> NUM_EPOCH = 5000 optimizer = sq.optimize.get_default_optimizer(8 * NUM_EPOCH) <p>In this tutorial, we are going to use predefined $\\hat{W}_{O}$-based model using <code>flax.linen</code>.</p> In\u00a0[8]: Copied! <pre>model = sq.models.linen.WoModel(\n    hidden_sizes_1=[10],\n    hidden_sizes_2=[10],\n)\nmodel\n</pre> model = sq.models.linen.WoModel(     hidden_sizes_1=[10],     hidden_sizes_2=[10], ) model Out[8]: <pre>WoModel(\n    # attributes\n    hidden_sizes_1 = [10]\n    hidden_sizes_2 = [10]\n    pauli_operators = ('X', 'Y', 'Z')\n    NUM_UNITARY_PARAMS = 3\n    NUM_DIAGONAL_PARAMS = 2\n    unitary_activation_fn = &lt;lambda&gt;\n    diagonal_activation_fn = &lt;lambda&gt;\n)</pre> <p>Next, we also have to make a loss function using <code>make_loss_fn</code>. This part depends on the implementation of the model that you choose to use.</p> In\u00a0[\u00a0]: Copied! <pre>loss_fn = sq.models.linen.make_loss_fn(\n    adapter_fn=sq.model.observable_to_expvals,\n    model=model,\n    calculate_metric_fn=sq.model.calculate_metric,\n    loss_metric=sq.model.LossMetric.MSEE,\n)\n</pre> loss_fn = sq.models.linen.make_loss_fn(     adapter_fn=sq.model.observable_to_expvals,     model=model,     calculate_metric_fn=sq.model.calculate_metric,     loss_metric=sq.model.LossMetric.MSEE, ) In\u00a0[10]: Copied! <pre>import optax\nfrom alive_progress import alive_bar\n\nwith alive_bar(NUM_EPOCH, title='Training \ud83d\ude80', force_tty=True) as bar:\n\n    def callback(\n        model_params: sq.models.linen.VariableDict,\n        opt_state: optax.OptState,\n        histories: list[sq.optimize.HistoryEntryV3],\n    ):\n        bar()\n\n    model_params, opt_state, histories = sq.models.linen.train_model(\n        training_key,\n        train_data=train_data,\n        val_data=test_data,  # Here, we did not care about the validating dataset.\n        test_data=test_data,\n        model=model,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        callbacks=[lambda x, y, z: bar()],\n        NUM_EPOCH=NUM_EPOCH,\n    )\n    # Alternatively, you can use callback for a compact callback function definition.\n    # alt_callback_fn = lambda x, y, z: bar() # noqa: E731\n</pre> import optax from alive_progress import alive_bar  with alive_bar(NUM_EPOCH, title='Training \ud83d\ude80', force_tty=True) as bar:      def callback(         model_params: sq.models.linen.VariableDict,         opt_state: optax.OptState,         histories: list[sq.optimize.HistoryEntryV3],     ):         bar()      model_params, opt_state, histories = sq.models.linen.train_model(         training_key,         train_data=train_data,         val_data=test_data,  # Here, we did not care about the validating dataset.         test_data=test_data,         model=model,         optimizer=optimizer,         loss_fn=loss_fn,         callbacks=[lambda x, y, z: bar()],         NUM_EPOCH=NUM_EPOCH,     )     # Alternatively, you can use callback for a compact callback function definition.     # alt_callback_fn = lambda x, y, z: bar() # noqa: E731 <pre>Training \ud83d\ude80 |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [100%] in 43.5s\n</pre> In\u00a0[11]: Copied! <pre>import tempfile\nfrom pathlib import Path\n\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n\n    model_path = Path(tmpdir)\n\n    # Create the path with parents if not existed already\n    model_path.mkdir(parents=True, exist_ok=True)\n\n    model_state = sq.model.ModelData(\n        params=model_params,\n        config={\n            \"hidden_sizes_1\": model.hidden_sizes_1,\n            \"hidden_sizes_2\": model.hidden_sizes_1,\n        },\n    )\n    # Save with,\n    model_state.to_file(model_path / \"model.json\")\n    # and load it back using,\n    model_state_from_file = sq.model.ModelData.from_file(model_path / \"model.json\")\n</pre> import tempfile from pathlib import Path   with tempfile.TemporaryDirectory() as tmpdir:      model_path = Path(tmpdir)      # Create the path with parents if not existed already     model_path.mkdir(parents=True, exist_ok=True)      model_state = sq.model.ModelData(         params=model_params,         config={             \"hidden_sizes_1\": model.hidden_sizes_1,             \"hidden_sizes_2\": model.hidden_sizes_1,         },     )     # Save with,     model_state.to_file(model_path / \"model.json\")     # and load it back using,     model_state_from_file = sq.model.ModelData.from_file(model_path / \"model.json\")  <p>We can check that both are equal,</p> In\u00a0[12]: Copied! <pre>model_state == model_state_from_file\n</pre> model_state == model_state_from_file Out[12]: <pre>True</pre> In\u00a0[13]: Copied! <pre>import pandas as pd\nimport matplotlib.pyplot as plt\n\nhist_df = pd.DataFrame(\n    [\n        {\"step\": entry.step, \"loss\": float(entry.loss), \"loop\": entry.loop}\n        for entry in histories\n    ]\n)\nfig, ax = plt.subplots(1, 1, figsize=(10, 3))\nsq.visualization.plot_loss_with_moving_average(\n    x=hist_df.query(\"loop == 'train'\")[\"step\"].to_numpy(),\n    y=hist_df.query(\"loop == 'train'\")[\"loss\"].to_numpy(),\n    ax=ax,\n    annotate_at=[],\n    color=\"red\",\n    label=\"Training\",\n)\n\nsq.visualization.plot_loss_with_moving_average(\n    x=hist_df.query(\"loop == 'test'\")[\"step\"].to_numpy(),\n    y=hist_df.query(\"loop == 'test'\")[\"loss\"].to_numpy(),\n    ax=ax,\n    annotate_at=[],\n    color=\"blue\",\n    label=\"Testing\",\n)\n\nshots = loaded_data.experiment_data.experiment_config.shots\n\nax.set_yscale(\"log\")\nax.set_xscale(\"log\")\nax.axhline(y=2/(3 * shots), linestyle='dashed', color=\"gray\")\nax.legend()\n\nsq.visualization.set_fontsize(ax, 16)\n</pre> import pandas as pd import matplotlib.pyplot as plt  hist_df = pd.DataFrame(     [         {\"step\": entry.step, \"loss\": float(entry.loss), \"loop\": entry.loop}         for entry in histories     ] ) fig, ax = plt.subplots(1, 1, figsize=(10, 3)) sq.visualization.plot_loss_with_moving_average(     x=hist_df.query(\"loop == 'train'\")[\"step\"].to_numpy(),     y=hist_df.query(\"loop == 'train'\")[\"loss\"].to_numpy(),     ax=ax,     annotate_at=[],     color=\"red\",     label=\"Training\", )  sq.visualization.plot_loss_with_moving_average(     x=hist_df.query(\"loop == 'test'\")[\"step\"].to_numpy(),     y=hist_df.query(\"loop == 'test'\")[\"loss\"].to_numpy(),     ax=ax,     annotate_at=[],     color=\"blue\",     label=\"Testing\", )  shots = loaded_data.experiment_data.experiment_config.shots  ax.set_yscale(\"log\") ax.set_xscale(\"log\") ax.axhline(y=2/(3 * shots), linestyle='dashed', color=\"gray\") ax.legend()  sq.visualization.set_fontsize(ax, 16) In\u00a0[14]: Copied! <pre>model = sq.models.linen.WoModel(**model_state.config)\npredictive_fn = sq.models.linen.make_predictive_fn(sq.model.observable_to_expvals, model, model_state.params)\n</pre> model = sq.models.linen.WoModel(**model_state.config) predictive_fn = sq.models.linen.make_predictive_fn(sq.model.observable_to_expvals, model, model_state.params) <p>Here is how can the predictive model be used to predict the expectation values given new parameters.</p> In\u00a0[15]: Copied! <pre>key, params_key = jax.random.split(key)\n_, l2a_fn = sq.control.get_param_array_converter(loaded_data.control_sequence)\nsample_params = l2a_fn(loaded_data.control_sequence.sample_params(params_key))\n\nunitary_f = loaded_data.whitebox(sample_params)[-1]\npredictive_fn(sq.predefined.drag_feature_map(sample_params), unitary_f)\n</pre> key, params_key = jax.random.split(key) _, l2a_fn = sq.control.get_param_array_converter(loaded_data.control_sequence) sample_params = l2a_fn(loaded_data.control_sequence.sample_params(params_key))  unitary_f = loaded_data.whitebox(sample_params)[-1] predictive_fn(sq.predefined.drag_feature_map(sample_params), unitary_f) Out[15]: <pre>Array([ 0.97084716, -0.97084716,  0.23626797, -0.23626797, -0.04041181,\n        0.04041181, -0.22440206,  0.22440206,  0.9390122 , -0.9390122 ,\n       -0.26057569,  0.26057569, -0.04353546,  0.04353546,  0.24955875,\n       -0.24955875,  0.96738053, -0.96738053], dtype=float64)</pre> <p>Even more elegant, we can define a predictive model with the <code>whitebox</code> embeded as follows.</p> In\u00a0[16]: Copied! <pre>def embed_predictive_model(control_parameters: jnp.ndarray):\n    unitary_f = loaded_data.whitebox(control_parameters)[-1]\n    return predictive_fn(\n        sq.predefined.drag_feature_map(control_parameters), unitary_f\n    )\n\nembed_predictive_model(sample_params)\n</pre> def embed_predictive_model(control_parameters: jnp.ndarray):     unitary_f = loaded_data.whitebox(control_parameters)[-1]     return predictive_fn(         sq.predefined.drag_feature_map(control_parameters), unitary_f     )  embed_predictive_model(sample_params) Out[16]: <pre>Array([ 0.97084716, -0.97084716,  0.23626797, -0.23626797, -0.04041181,\n        0.04041181, -0.22440206,  0.22440206,  0.9390122 , -0.9390122 ,\n       -0.26057569,  0.26057569, -0.04353546,  0.04353546,  0.24955875,\n       -0.24955875,  0.96738053, -0.96738053], dtype=float64)</pre> In\u00a0[17]: Copied! <pre>calculate_agf_sx = sq.physics.direct_AGF_estimation_fn(sq.constant.SX)\n\n@jax.jit\ndef average_gate_infidelity(params: jnp.ndarray):\n    # Predict the expectation values\n    predicted_expvals = embed_predictive_model(params)\n    # Calculate the average gate fidelity with respected to SX gate.\n    AGF = calculate_agf_sx(predicted_expvals)\n    # return average gate infidelity squared and log the results.\n    return (1 - AGF) ** 2, {\"AGF\": AGF}\n\naverage_gate_infidelity(sample_params)\n</pre> calculate_agf_sx = sq.physics.direct_AGF_estimation_fn(sq.constant.SX)  @jax.jit def average_gate_infidelity(params: jnp.ndarray):     # Predict the expectation values     predicted_expvals = embed_predictive_model(params)     # Calculate the average gate fidelity with respected to SX gate.     AGF = calculate_agf_sx(predicted_expvals)     # return average gate infidelity squared and log the results.     return (1 - AGF) ** 2, {\"AGF\": AGF}  average_gate_infidelity(sample_params) Out[17]: <pre>(Array(0.06409491, dtype=float64), {'AGF': Array(0.74683027, dtype=float64)})</pre> <p>Now, we can optimize it using <code>sq.optimize.minimize</code>.</p> In\u00a0[18]: Copied! <pre>init_key = jax.random.key(73)\ninit_params = l2a_fn(loaded_data.control_sequence.sample_params(init_key))\n\nlower, upper = loaded_data.control_sequence.get_bounds()\n\nsteps = 400\noptimized_params, aux = sq.optimize.minimize(\n    init_params,\n    average_gate_infidelity,\n    sq.optimize.get_default_optimizer(steps),\n    l2a_fn(lower),\n    l2a_fn(upper),\n    maxiter=steps,\n)\n</pre> init_key = jax.random.key(73) init_params = l2a_fn(loaded_data.control_sequence.sample_params(init_key))  lower, upper = loaded_data.control_sequence.get_bounds()  steps = 400 optimized_params, aux = sq.optimize.minimize(     init_params,     average_gate_infidelity,     sq.optimize.get_default_optimizer(steps),     l2a_fn(lower),     l2a_fn(upper),     maxiter=steps, ) <pre>|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 400/400 [100%] in 50.2s (7.97/s)     \n</pre> In\u00a0[19]: Copied! <pre>optimized_params\n</pre> optimized_params Out[19]: <pre>Array([ 1.20804706, -0.15345464], dtype=float64)</pre> In\u00a0[20]: Copied! <pre>aux[-1]\n</pre> aux[-1] Out[20]: <pre>{'AGF': Array(0.97544916, dtype=float64),\n 'params': Array([ 1.20804706, -0.15345464], dtype=float64)}</pre> In\u00a0[21]: Copied! <pre>def quantum_device(params: jnp.ndarray):\n    return sq.model.get_predict_expectation_value(\n        {\"X\": sq.constant.X, \"Y\": sq.constant.Y, \"Z\": sq.constant.Z},\n        data_model.solver(params)[-1],\n        sq.constant.default_expectation_values_order,\n    )\n</pre> def quantum_device(params: jnp.ndarray):     return sq.model.get_predict_expectation_value(         {\"X\": sq.constant.X, \"Y\": sq.constant.Y, \"Z\": sq.constant.Z},         data_model.solver(params)[-1],         sq.constant.default_expectation_values_order,     ) In\u00a0[22]: Copied! <pre>assert isinstance(optimized_params, jnp.ndarray)\nreal_expvals = quantum_device(optimized_params)\n\ncalculate_agf_sx(real_expvals)\n</pre> assert isinstance(optimized_params, jnp.ndarray) real_expvals = quantum_device(optimized_params)  calculate_agf_sx(real_expvals) Out[22]: <pre>Array(0.97306995, dtype=float64)</pre> <p>As a final thought:</p> <ul> <li>This is the benchmark without taking the finite-shot effect into the account.</li> <li>This is characterization without a model selection process such as hyperparameter tuning.</li> </ul>"},{"location":"tutorials/tutorial_0003_cc/#characterization-and-calibration-using-graybox","title":"Characterization and Calibration using Graybox\u00b6","text":"<p>Goal</p> <p>         This tutorial aims to guide you thourgh a process of characterizing a predicitive model, and using the predictive model in calibrating for quantum gate using open-loop optimization (although not necessary).      </p>"},{"location":"tutorials/tutorial_0003_cc/#generate-some-synthetic-data","title":"Generate some synthetic data\u00b6","text":""},{"location":"tutorials/tutorial_0003_cc/#data-preprocessing","title":"Data preprocessing\u00b6","text":"<p>Since we are going to train Deep neural network, it is considered a good practice to split dataset into training and testing dataset. Here we use a <code>sq.utils.random_split</code> helper function for this task.</p>"},{"location":"tutorials/tutorial_0003_cc/#save-and-load-model","title":"Save and load model\u00b6","text":"<p>We can save the model using <code>ModelData</code> as follows,</p>"},{"location":"tutorials/tutorial_0003_cc/#predictive-model-construction","title":"Predictive model construction.\u00b6","text":"<p>We can use the adapter function that we used to make a loss function with <code>partial</code> to create the predictive model.</p>"},{"location":"tutorials/tutorial_0003_cc/#control-calibration","title":"Control Calibration\u00b6","text":"<p>As an example, we are going to use the predictive model to calibrate for the quantum gate. Specifically, we want to find a control parameters that maximize an average gate fidelity with respected to $\\sqrt{X}$ gate. First, we define a cost function that the optimizer should find the parameters that minimize its output.</p>"},{"location":"tutorials/tutorial_0003_cc/#benchmark","title":"Benchmark\u00b6","text":"<p>Let's check if the model can accurately characterized the hidden device.</p>"}]}